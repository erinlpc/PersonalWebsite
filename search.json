[
  {
    "objectID": "teaching/consumer-behavior/index.html",
    "href": "teaching/consumer-behavior/index.html",
    "title": "Consumer Behavior",
    "section": "",
    "text": "This course examines the psychological, social, and cultural factors that influence consumer decision-making. We explore how consumers perceive information, form attitudes, make choices, and experience consumption—and how marketers can apply these insights ethically and effectively."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#course-format",
    "href": "teaching/consumer-behavior/index.html#course-format",
    "title": "Consumer Behavior",
    "section": "Course Format",
    "text": "Course Format\nThis course alternates between in-person and asynchronous online formats depending on the semester."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#lecture-videos",
    "href": "teaching/consumer-behavior/index.html#lecture-videos",
    "title": "Consumer Behavior",
    "section": "Lecture Videos",
    "text": "Lecture Videos\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nLecture video links will be added here."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#readings",
    "href": "teaching/consumer-behavior/index.html#readings",
    "title": "Consumer Behavior",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse readings and supplementary materials will be added here."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#assignments",
    "href": "teaching/consumer-behavior/index.html#assignments",
    "title": "Consumer Behavior",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment descriptions and materials will be added here."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html",
    "title": "Week 1: File Management Practice",
    "section": "",
    "text": "Goals for this assignment:\n\nDownload and install R and RStudio\nOpen and run an R script\nFamiliarize yourself with file management on your personal computer\nPractice a clean folder setup\nSave a CSV to a /Datasets folder\nLoad it back using a relative path in R/RStudio\n\n\n\n\n\n\n\nNoteWhat’s a relative path?\n\n\n\nA relative path tells R where to find a file relative to your current working directory—like saying “go into the Datasets folder from here.” An absolute path spells out the entire route from the root of your computer, like /Users/erin.p.carter/Documents/Marketing_Research/Datasets/file.csv. Relative paths are shorter, portable, and won’t break when you move your project folder or switch computers."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#overview",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#overview",
    "title": "Week 1: File Management Practice",
    "section": "",
    "text": "Goals for this assignment:\n\nDownload and install R and RStudio\nOpen and run an R script\nFamiliarize yourself with file management on your personal computer\nPractice a clean folder setup\nSave a CSV to a /Datasets folder\nLoad it back using a relative path in R/RStudio\n\n\n\n\n\n\n\nNoteWhat’s a relative path?\n\n\n\nA relative path tells R where to find a file relative to your current working directory—like saying “go into the Datasets folder from here.” An absolute path spells out the entire route from the root of your computer, like /Users/erin.p.carter/Documents/Marketing_Research/Datasets/file.csv. Relative paths are shorter, portable, and won’t break when you move your project folder or switch computers."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-1-set-up-your-class-folders",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-1-set-up-your-class-folders",
    "title": "Week 1: File Management Practice",
    "section": "Part 1: Set Up Your Class Folders",
    "text": "Part 1: Set Up Your Class Folders\nBefore we touch R, let’s get your computer organized. Create a main class folder somewhere you control—your Desktop or Documents folder works well.\nName it something like:\nMarketing_Research_R_Fall2025\nThe exact names don’t matter for the code to work—you could call your folder dinofartbroccolini and save your scripts as various Kings of England if you want. You just need to know where things are stored so you can find them later.\nThat said, I’d recommend a nice, safe, boring file structure. Inside your main folder, create subfolders like:\nMarketing_Research_R_Fall2025/\n├── Code/\n├── Datasets/\n├── Homework/\n└── Resources/\n\n\n\n\n\n\nWarningAvoid these locations\n\n\n\nDo NOT work from your Downloads folder. Files there get buried and accidentally deleted.\nBe careful with cloud-only locations (Google Drive, iCloud, OneDrive) unless the folder is fully synced to your local machine. Cloud sync can cause weird errors when R tries to read or write files that are still uploading."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-2-install-r-and-rstudio",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-2-install-r-and-rstudio",
    "title": "Week 1: File Management Practice",
    "section": "Part 2: Install R and RStudio",
    "text": "Part 2: Install R and RStudio\nYou only need to do this once.\n\nStep 1: Install R\nGo to https://cran.r-project.org\n\nChoose your operating system (Mac, Windows, or Linux)\nDownload and run the installer\nAccept the defaults\n\n\n\nStep 2: Install RStudio\nGo to https://posit.co/download/rstudio-desktop/\n\nDownload the free RStudio Desktop version\nRun the installer\nAccept the defaults\n\n\n\nStep 3: Open RStudio\nOpen RStudio (not R directly—RStudio is the friendlier interface that runs R for you).\nTo create a new script file:\n\nGo to File → New File → R Script\nIf that’s grayed out, use the toolbar icon (white page with a green “+”) or press Ctrl+Shift+N (Windows) / Cmd+Shift+N (Mac)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-3-create-an-rstudio-project",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-3-create-an-rstudio-project",
    "title": "Week 1: File Management Practice",
    "section": "Part 3: Create an RStudio Project",
    "text": "Part 3: Create an RStudio Project\nAn RStudio Project ties your work to a specific folder. When you open the Project, R automatically knows where your files are. This is the key to making relative paths work.\n\nIn RStudio: File → New Project\nChoose Existing Directory\nNavigate to your Marketing_Research_R_Fall2025 folder and select it\nClick Create Project\n\nNow, whenever you open this Project, your working directory will automatically be set to your class folder.\n\n\n\n\n\n\nTipHow do I open a Project later?\n\n\n\nLook for a file called Marketing_Research_R_Fall2025.Rproj in your class folder. Double-click it, and RStudio will open with everything configured."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-4-create-and-save-your-first-script",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-4-create-and-save-your-first-script",
    "title": "Week 1: File Management Practice",
    "section": "Part 4: Create and Save Your First Script",
    "text": "Part 4: Create and Save Your First Script\nNow let’s create a script file to store our code.\n\nIn RStudio: File → New File → R Script (or click the white page icon with a green “+”)\nYou should see a blank script panel in the top-left of RStudio\nSave this file to your Code folder:\n\nFile → Save As\nNavigate to Marketing_Research_R_Fall2025/Code/\nName it something like HW1_file_management_practice.R\n\n\n\nA quick orientation to RStudio\nRStudio has four main panels:\n\nTop-left (Source/Script): Where you write and edit code\nBottom-left (Console): Where R actually runs your code and shows output\nTop-right (Environment): Shows what data and variables R currently has loaded\nBottom-right (Files/Plots/Help): File browser, visualizations, and documentation\n\n\n\nHow to run code\nTo run a single line: Put your cursor on the line and press Cmd+Return (Mac) or Ctrl+Enter (Windows).\nTo run multiple lines: Highlight the lines you want and use the same keyboard shortcut, or click the Run button in the top-right of the script panel.\nTo run the entire script: Click Source or press Ctrl+Shift+Enter (Windows) / Cmd+Shift+Enter (Mac)."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-5-quick-checkwhere-are-we",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-5-quick-checkwhere-are-we",
    "title": "Week 1: File Management Practice",
    "section": "Part 5: Quick Check—Where Are We?",
    "text": "Part 5: Quick Check—Where Are We?\nLet’s verify that R knows where we are. Run this command:\n\ngetwd()\n\nWhen you run this, your console (the pane below your script) should show something like:\n[1] \"/Users/yourname/Documents/Marketing_Research_R_Fall2025\"\nThis is your working directory—the folder R considers “home base” for finding and saving files.\n\n\n\n\n\n\nImportantWhat if it’s wrong?\n\n\n\nIf the path shown is NOT your class folder, you probably didn’t open the RStudio Project. Close RStudio, find the .Rproj file in your class folder, and double-click it to open.\nAlternatively, you can set the working directory manually with setwd(), but I recommend using Projects instead—it’s more reliable."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-6-create-a-dataset",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-6-create-a-dataset",
    "title": "Week 1: File Management Practice",
    "section": "Part 6: Create a Dataset",
    "text": "Part 6: Create a Dataset\nLet’s make a small practice dataset. Imagine we’re tracking pop-up tastings for a “Maine Blueberry Beverage” across several towns. We recorded the town name, local advertising spend, foot traffic, and average satisfaction scores.\nRun the following code:\n\n# Create variables\nTown         &lt;- c(\"Orono\", \"Bangor\", \"Portland\", \"Bar Harbor\", \"Lewiston\", \"Augusta\")\nAdSpend      &lt;- c(120, 180, 350, 220, 150, 160)       # local ad dollars\nFootTraffic  &lt;- c(85, 110, 240, 130, 95, 100)         # people who stopped by\nSatisfaction &lt;- c(78, 80, 88, 85, 76, 79)             # avg satisfaction (0-100)\n\n# Combine into a data frame\nmaine_marketing &lt;- data.frame(\n  Town, AdSpend, FootTraffic, Satisfaction,\n  stringsAsFactors = FALSE\n)\n\n# Display the data\nprint(maine_marketing)\n\nWhen you run this, you should see the data print in your console:\n       Town AdSpend FootTraffic Satisfaction\n1     Orono     120          85           78\n2    Bangor     180         110           80\n3  Portland     350         240           88\n4 Bar Harbor    220         130           85\n5  Lewiston     150          95           76\n6   Augusta     160         100           79\nYou should also see maine_marketing appear in your Environment panel (top-right) as a dataset with 6 observations and 4 variables."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-7-save-the-data-to-a-csv-file",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-7-save-the-data-to-a-csv-file",
    "title": "Week 1: File Management Practice",
    "section": "Part 7: Save the Data to a CSV File",
    "text": "Part 7: Save the Data to a CSV File\nNow we’ll save this dataset to your Datasets folder using a relative path.\nBecause your working directory is already set to your class folder, you don’t need to type the full path. You can just say “go into the Datasets folder and save this file there.”\n\nwrite.csv(maine_marketing, \"Datasets/maine_marketing_sample.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nWarningDid you get an error?\n\n\n\nIf you see something like cannot open the connection or No such file or directory, I’d bet a metaphorical $1,000,000 it’s because you don’t have a folder called Datasets in your working directory.\nFix: Either create a folder called Datasets in your class folder, or change the code to match whatever folder structure you actually have.\n\n\n\nVerify the file was created\nYou can check manually by opening your Datasets folder in Finder/File Explorer. Or you can ask R to check for you:\n\nfile.exists(\"Datasets/maine_marketing_sample.csv\")\n\nIf it returns TRUE, the file is there."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-8-load-the-csv-back-into-r",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-8-load-the-csv-back-into-r",
    "title": "Week 1: File Management Practice",
    "section": "Part 8: Load the CSV Back into R",
    "text": "Part 8: Load the CSV Back into R\nNow let’s practice loading data from a file—something you’ll do constantly in this class.\nRun this code:\n\nloaded &lt;- read.csv(\"Datasets/maine_marketing.csv\")\n\n\n\n\n\n\n\nCautionDid that work? (Click to expand)\n\n\n\n\n\nIf you’ve been following exactly as written above, that should NOT have worked. You should have received an error message in your console.\nTake a moment to figure out why before reading on.\n\n\n\n\n\n\n\n\n\nCautionHint (Click to expand)\n\n\n\n\n\nCheck the file name carefully. What did we name the file when we saved it?\n\n\n\n\n\n\n\n\n\nCautionThe answer (Click to expand)\n\n\n\n\n\nWe saved the file as maine_marketing_sample.csv, but we tried to load maine_marketing.csv. The names don’t match!\nI tricked you a little there—but only with the best of intentions. This is the kind of error you will make approximately 1,000 times in your coding career. Noticing small differences in file names is a skill you’ll develop.\nThe correct code is:\n\nloaded &lt;- read.csv(\"Datasets/maine_marketing_sample.csv\")\n\n\n\n\nNow run the corrected version and verify it worked by printing both datasets:\n\nprint(loaded)\nprint(maine_marketing)\n\nThey should look identical."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-9-verify-the-data-matches",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-9-verify-the-data-matches",
    "title": "Week 1: File Management Practice",
    "section": "Part 9: Verify the Data Matches",
    "text": "Part 9: Verify the Data Matches\nYou confirmed with your eyes that the datasets look the same. But eyeballing data is inefficient—especially when datasets have thousands of rows. Let’s ask R to compare them for us.\n\nsame &lt;- all.equal(maine_marketing, loaded)\n\nOK, the code ran without errors… but nothing happened. R just sits there. What did we miss?\n\n\n\n\n\n\nCautionThink about it… (Click to expand)\n\n\n\n\n\nWe told R to create something called same and store the result of all.equal() in it. But we never asked R to show us what same contains.\nR is very good at a literal, nerdy version of “Simon Says.” We have to explicitly ask it to display things.\n\n\n\nTry this:\n\nSame\n\n\n\n\n\n\n\nCautionStill not working? (Click to expand)\n\n\n\n\n\nYou should have gotten an error: object 'Same' not found.\nWell, this class is broken. The professor clearly doesn’t know what she’s doing. Here you are having to troubleshoot her broken code…\nActually, no. We defined same (lowercase), but we typed Same (capitalized). R is case-sensitive—same and Same are completely different things to R.\nTry:\n\nsame\n\nYou should see [1] TRUE, confirming the datasets are identical.\n\n\n\nIn this case, the verification saved us zero time—the data was tiny and obvious. But when your datasets have thousands of rows, being able to programmatically verify things is invaluable."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#troubleshooting-checklist",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#troubleshooting-checklist",
    "title": "Week 1: File Management Practice",
    "section": "Troubleshooting Checklist",
    "text": "Troubleshooting Checklist\nIf you see errors like cannot open the connection or no such file or directory, work through this checklist:\n\nWhat folder is R actually in? Run getwd() and verify it’s your class folder.\nDoes R see your file? Try:\n\nfile.exists(\"Datasets/maine_marketing_sample.csv\")\nlist.files(\"Datasets\")\n\nCheck for typos. File names must match exactly, including capitalization and underscores.\nAre you using forward slashes? In R, always use / in file paths, even on Windows. (I know this is hard for you PC folks. Hang in there.)\nDid you open the RStudio Project? If you just opened RStudio without opening the Project file, your working directory might be somewhere else entirely.\nIs the file open in another program? If you opened the CSV in Excel, close it and try again. Some programs lock files while they’re open."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#bonus-save-a-dated-backup",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#bonus-save-a-dated-backup",
    "title": "Week 1: File Management Practice",
    "section": "Bonus: Save a Dated Backup",
    "text": "Bonus: Save a Dated Backup\nHere’s a handy trick for keeping track of different versions of your data: automatically add today’s date to the filename.\n\n# Get today's date in YYYY-MM-DD format\ntoday &lt;- format(Sys.Date(), \"%Y-%m-%d\")\n\n# Create a filename with the date\ncsv_dated &lt;- file.path(\"Datasets\", paste0(\"maine_marketing_\", today, \".csv\"))\n\n# Save the file\nwrite.csv(maine_marketing, csv_dated, row.names = FALSE)\n\n# Check what's in your Datasets folder now\nlist.files(\"Datasets\")\n\nYou should see both your original file and a new dated version.\nTo load the dated file back:\n\ndated_maine_marketing &lt;- read.csv(csv_dated)\ndated_maine_marketing\n\nThis pattern is useful when you’re iterating on data cleaning or analysis and want to preserve earlier versions."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#submission",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#submission",
    "title": "Week 1: File Management Practice",
    "section": "Submission",
    "text": "Submission\nTo complete this assignment:\n\nMake sure you can successfully run all the code in this document\nTake a screenshot showing your RStudio with:\n\nYour script in the top-left panel\nThe maine_marketing data printed in the console\nThe Environment panel showing your loaded datasets\n\nSubmit the screenshot to Brightspace\n\n\n\n\n\n\n\nTipStuck?\n\n\n\nIf you’re having trouble, check the Resources page for links to helpful documentation, or come to office hours. File management issues are extremely common at the start—you’re not alone!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html",
    "href": "teaching/marketing-research/assignments/week4-exercise.html",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "",
    "text": "We’re continuing to work with our brewery client’s festival data. This exercise covers:\n\n(Re?)Familiarize ourselves with measures of how spread out or disperse our data are (variance and standard deviation)\nExplore the idea of a standard normal distribution and z-scores\nBuild some intuitions about how standard deviations and z-scores of a normal distribution relate to probabilities and percentiles\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#overview-and-goals",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#overview-and-goals",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "",
    "text": "We’re continuing to work with our brewery client’s festival data. This exercise covers:\n\n(Re?)Familiarize ourselves with measures of how spread out or disperse our data are (variance and standard deviation)\nExplore the idea of a standard normal distribution and z-scores\nBuild some intuitions about how standard deviations and z-scores of a normal distribution relate to probabilities and percentiles\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#part-0-load-the-dataset",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#part-0-load-the-dataset",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "Part 0 — Load the Dataset",
    "text": "Part 0 — Load the Dataset\nWe’re using the same “stout_festival.csv” dataset that we used previously, so you should already have it in your relevant folder. For me, loading it looks something like this:\n\nfestival &lt;- read.csv(\"Datasets/stout_festival.csv\")\n\n# Let's take a quick peek at what's going on here to re-familiarize ourselves.\nhead(festival)\nsummary(festival$WTP)     # We're going to be thinking primarily about WTP again"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#part-1-variance-and-standard-deviation",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#part-1-variance-and-standard-deviation",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "Part 1 — Variance and Standard Deviation",
    "text": "Part 1 — Variance and Standard Deviation\nOK, let’s warm up with our measures of spread. Remember: mean, median, mode tell us about the “center” of the data, but we also need to know how spread out the data are around that center. That’s where variance and standard deviation come in.\n\nVariance\nConceptually, variance is the “average squared distance” from the mean. Honestly, I don’t find that very intuitive…I have a difficult time picturing average square distance but it’s mathematically pretty convenient…\nWhy squared? Well, much like when we were thinking about measures of total model error, squaring helps versus a simple sum because if we just summed up raw differences, positives and negatives cancel each other out. Again as in our model error discussion, squaring also makes the big deviations (outliers) count extra — so logically this measure is pretty consistent with the calls that we made when we identified mean and SSE as our preferred measures of central tendency and error, respectively. Still, variance is what you will often see in formulas but MOST (not all) people find it easier to THINK in terms of standard deviation.\n\n\nStandard Deviation\nThe standard deviation is just the square root of the variance. In other words, we un-square things. It puts things back in the original units (dollars, in our case). While it’s not exactly the average distance from the mean, it’s close enough to that that most of us can think of it that way. That makes SD more intuitive for thinking or talking about the spread of a distribution or how “typical” or “unusual” a given value is.\n\n\nPopulation vs. Sample Formulas\nLet’s get to work calculating both but first, one quick thing…there’s a choice that we have to make as statisticians when we calculate the variance or standard deviation and that is whether we should use the population formulas or the sample formulas.\nNow I’m not going to say that you will ALWAYS use the sample formula…but I am going to say that if you wonder whether or not you should use the population formula, you’re probably wrong because you should almost never use the population formula. Not never, but close enough. After all, the formulas are called “population” and “sample” and we’ve been saying all semester that the population will nearly always be some giant group you seek to understand but will never have access to the complete dataset with every member represented. You’ll almost always have just a subset of the population you will examine and hope to generalize from to better understand the larger population.\nThat said, the difference between the two formulas is that for a population (which you almost certainly don’t have), you divide your squared deviations by n (the number of observations) and for a sample, you divide by n-1 (one fewer than the number of observations). If you’ve learned about degrees of freedom previously, this correction is akin to burning a degree of freedom generating a new predictor (if you’re not familiar with degrees of freedom, get ready to get familiar in the coming weeks). Another way to think about this is that we’re attempting to un-bias our estimate - we assume that our sample won’t PERFECTLY describe our population (despite our best hopes), and so we kind of tweak the odds a bit, making the measure of spread guess be just a bit broader for a sample (but the penalty will be reduced as our sample grows in size).\n\n\n\nStep 1: Calculate the mean\n\nwtp_mean &lt;- mean(festival$WTP)\nwtp_mean\n\n\n\nStep 2: Compute deviations\nDeviation = DATA - MODEL = each value minus the mean. (This should feel familiar…if we summed this column, it would give us SoE)\n\nfestival$dev &lt;- festival$WTP - wtp_mean\n\n# OK, so we should see this new deviation column in our dataset...let's check:\nhead(festival)\n\n\n\nStep 3: Square the deviations\n(This, too, should feel familiar…if we just summed this, we’d get SSE!)\n\nfestival$dev_sq &lt;- (festival$dev)^2\n\n#...and, once again, I want to prove it to myself, so let's check:\nhead(festival)\n\n\n\nStep 4: Average (with a penalty) the squared deviations\n(This should NOT feel like SSE - in SSE we sum these squared values)\nOK, so what’s the “penalty” I reference above…? That’s right, I know you tattooed my aside about population and sample formulas above right onto your heart so you knew right away - this is a SAMPLE not the entire population we’re interested in! We want to understand more than just these 50 people! So we’ll be kind of taking that average with a little penalty by dividing by n-1 instead of by n.\n\nvar_wtp &lt;- sum(festival$dev_sq) / (nrow(festival) - 1)\nvar_wtp # this should be ~0.7679\n\n\n\nStep 5: Square root of variance = StDev\nAll we need to do to calculate our standard deviation is to take the square root of the variance:\n\nsd_wtp &lt;- sqrt(var_wtp)\nsd_wtp\n\n\n\nStep 6: Check against built-in functions\nOK, so I made you calculate those two values “by hand” (or whatever the coding out the formulas equivalent term is) and you’ll probably never do that again… but you did it once and maybe that will help you to remember the formulas and concepts!\nHenceforth, you have my permission to instead use the following commands in R:\n\nvar_builtin &lt;- var(festival$WTP)\nsd_builtin  &lt;- sd(festival$WTP)\n\n# ...and now we confirm that everything worked correctly and the two versions\n# of calculating give us the same result...(trust but verify):\n\nvar_wtp; var_builtin\nsd_wtp; sd_builtin\n\nSweet success.\nAnd just to be clear while we’re here, the actual commands we are using to generate variance and sd above are the “var( )” and “sd( )” bits - everything before the “&lt;-” section is just a name we’re specifying for this particular example that we assigned and then later called. Prior to assigning a meaning to those names, if you type in “var_builtin” or “sd_builtin”, R would have had no idea whatsoever what you were talking about. Correspondingly, if you just run “var(festival$WTP)” or “sd(festival$WTP)”, you will get those values (assuming that the dataset and variable you are requesting those statistics for exist and are loaded and the correct format) because those are commands that mean something to R without you having to specify anything.\nWith that you understand perfectly, I’m sure. But what if we made a nice little figure to visualize?"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#part-1.5-a-tiny-picture-of-spread",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#part-1.5-a-tiny-picture-of-spread",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "Part 1.5 — A Tiny Picture of Spread",
    "text": "Part 1.5 — A Tiny Picture of Spread\nTime for a quick visual. Same idea as the app, mini-version here:\n\nBars = distribution of WTP (histogram)\nThick green line = mean (our model’s single-number “best guess” under SSE)\nLight shading = ±1 SD (darker) and ±2 SD (lighter) around the mean …because “how many SDs from the mean” is how we oftem talk and think about degree of “unusualness.”\n\n\n# install.packages(\"ggplot2\")  # &lt;- uncomment if you don't have it yet\nlibrary(ggplot2)\n\nwtp_df &lt;- data.frame(WTP = festival$WTP)\n\ng &lt;- ggplot(wtp_df, aes(x = WTP)) +\n  # shade ±2 SD first (lightest)\n  annotate(\"rect\",\n           xmin = wtp_mean - 2*sd_wtp, xmax = wtp_mean + 2*sd_wtp,\n           ymin = 0, ymax = Inf, alpha = 0.08, fill = \"#1B7837\") +\n  # shade ±1 SD (a bit darker)\n  annotate(\"rect\",\n           xmin = wtp_mean - 1*sd_wtp, xmax = wtp_mean + 1*sd_wtp,\n           ymin = 0, ymax = Inf, alpha = 0.14, fill = \"#1B7837\") +\n  geom_histogram(bins = 30, fill = \"grey80\", color = \"white\") +\n  # mean line (dark green to match the app)\n  geom_vline(xintercept = wtp_mean, color = \"#1B7837\", linewidth = 1.1) +\n  # 1 SD guides (dashed)\n  geom_vline(xintercept = wtp_mean + c(-1, 1)*sd_wtp,\n             color = \"#1B7837\", linetype = \"dashed\", linewidth = 0.8) +\n  # 2 SD guides (dotted)\n  geom_vline(xintercept = wtp_mean + c(-2, 2)*sd_wtp,\n             color = \"#1B7837\", linetype = \"dotted\", linewidth = 0.8) +\n  labs(\n    title = \"Willingness to Pay (WTP): mean (solid) and standard-deviation (dotted) lines\",\n    subtitle = \"Shaded bands: darker = ±1 SD, lighter = ±2 SD around the mean\",\n    x = \"WTP (dollars)\", y = \"Count\"\n  ) +\n  theme_minimal(base_size = 13)\n\ng\n\n\nThe 68-95-99.7 Rule\nWhile we’re looking at that picture, let’s quantify the “within k SD” idea. If WTP were exactly normally distributed, we’d expect ≈68% within ±1 SD, ≈95% within ±2 SD, and ≈99.7% within ±3 SD (the classic 68–95–99.7 rule).\nSo just for fun (this is your idea of fun, right?!), let’s check to see what % of our observations ACTUALLY fall within each one of those standard deviations from our mean…\n\n# Here we're going to calculate the ACTUAL % in our ACTUAL data:\np_within_1_emp &lt;- mean(abs(festival$WTP - wtp_mean) &lt;= 1*sd_wtp, na.rm = TRUE)\np_within_2_emp &lt;- mean(abs(festival$WTP - wtp_mean) &lt;= 2*sd_wtp, na.rm = TRUE)\np_within_3_emp &lt;- mean(abs(festival$WTP - wtp_mean) &lt;= 3*sd_wtp, na.rm = TRUE)\n\n# And here we calculate what would happen in a perfectly normal distribution\n# (mostly so that we can see it looking all nice in a table together below -\n# and don't get hung up on the pnorm() of it all just yet, I'll explain that in\n# the next section):\np_within_1_norm &lt;- pnorm(1)  - pnorm(-1)   # ~0.6827\np_within_2_norm &lt;- pnorm(2)  - pnorm(-2)   # ~0.9545\np_within_3_norm &lt;- pnorm(3)  - pnorm(-3)   # ~0.9973\n\n# Oh look, a beautiful table where we can easily compare:\ndata.frame(\n  band            = c(\"±1 SD\", \"±2 SD\", \"±3 SD\"),\n  empirical_share = round(c(p_within_1_emp, p_within_2_emp, p_within_3_emp), 4),\n  normal_expected = round(c(p_within_1_norm, p_within_2_norm, p_within_3_norm), 4)\n)\n\nI mean, nobody’s perfect but that looks pretty darn close!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#part-2-z-scores-making-it-better",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#part-2-z-scores-making-it-better",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "Part 2 — Z-Scores: Making It Better",
    "text": "Part 2 — Z-Scores: Making It Better\nStandard deviation is about understanding the spread in our data. But it can be hard to compare how far out in the spread of the data a value is. It would be nice to have a bit of a shortcut to get that kind of information other than having to work it out yourself with the mean and standard deviation (and that’s if we don’t even attempt to deal with whether or not the data are normally distributed).\nThis is where our friend the z-score comes in. We can standardize a value by calculating a z-score to put it on a common scale. It’s as easy as: z = (value - mean)/SD\nSo why exactly do we want this? A few reasons:\n\nComparability: Raw WTP in dollars, time-on-site in minutes, or customer satisfaction on a 1–7 scale can’t be compared directly. Once standardized, they’re all in “SD units,” so we can see which deviations are bigger relative to their own distributions.\nInterpretability: A z-score tells you immediately how unusual a value is. A z = +2 means “2 standard deviations above the mean” — regardless of whether the raw units were dollars, minutes, or survey points.\nProbability connection: If the variable is roughly normally distributed, the famous 68–95–99.7 rule applies. About 68% of values fall within ±1 SD, about 95% within ±2 SD, and about 99.7% within ±3 SD. That lets us go from a z-score straight to a probability with pnorm().\nModeling foundation: Lots of the tools we’ll use later (t-tests, regressions, ANOVAs) rely on this same idea of standardizing to build test statistics. So practicing it here gives you the muscle memory you’ll need downstream.\n\n\n\nCalculating Z-scores\nSo to summarive: z = (value - mean) / SD.\nThis puts everything on the same “SD units” scale.\nI’ll do it once, then it’s on you.\n\nfestival$z_wtp &lt;- (festival$WTP - wtp_mean) / wtp_sd\n\nYou probably got an error there…because I did while I was writing this (sometimes I plan things, sometimes they’re real mistakes I leave in).\nCan you figure it out/fix it? The error code in your console is a pretty huge clue….\n\n\n\n\n\n\nTipClick to reveal the answer\n\n\n\n\n\nDid you get it? I transposed the name for the standard deviation that I’d established earlier. R can’t run that command because it’s looking for “wtp_sd” but I only told it to create “sd_wtp” - my bad.\n\n\n\nIf you didn’t fix it already, let’s go try the following:\n\nfestival$z_wtp &lt;- (festival$WTP - wtp_mean) / sd_wtp\n\n# Peek so you can see raw WTP next to standardized z:\nhead(festival[, c(\"WTP\", \"z_wtp\")], 10)\n\n# This might be a bit more intuitive if you remind yourself of what the mean was\nwtp_mean\n\nSo as you can see, with a z-score we’re:\n\ncentering our distribution around 0 by subtracting the mean such that\nany negative z-score was below the mean and\nany positive z-score was above the mean, and\nthose positive and negative values are scaled by the spread (standard deviation) in the data.\n\n\n\n\nUnderstanding pnorm() and qnorm()\nSo remember when we used the pnorm() command just a little bit ago above? Let’s return to that now that we know a bit more about z-scores. pnorm() and qnorm() are commands in R that allow us to interrogate our data a bit more in the context of z-scores.\nSpecifically:\n\npnorm() → “p” stands for probability. You feed it a z-score, and it tells you the probability of being less than or equal to that z (the left-tail area under the curve). Example: pnorm(1.96) ≈ 0.975 → about 97.5% of a Normal distribution is below z = 1.96.\nqnorm() → “q” stands for quantile. You feed it a probability (between 0 and 1), and it gives you the z-score that cuts off that proportion of the distribution. Example: qnorm(0.975) ≈ 1.96 → the 97.5th percentile sits about 1.96 SD above the mean.\n\nTogether they’re opposites: pnorm goes z → probability, qnorm goes probability → z. That’s how we jump back and forth between “how many SDs away?” and “what percent of people fall there?”\nManagerially, having a good intuition for what’s going on here is REALLY valuable. It’s rare for the people you will be working with and reporting to to be as well-trained as you will be by the end of this course. Most people are pretty comfortable thinking in terms of percentiles though.\nLet’s play around with that idea a bit below:\n\n\n\nOutliers - How Often Do “Big” z’s Show Up?\nLet’s count how often |z| &gt; 2 and |z| &gt; 3 in our sample:\n\nprop_absz_gt2 &lt;- mean(abs(festival$z_wtp) &gt; 2, na.rm = TRUE)\nprop_absz_gt3 &lt;- mean(abs(festival$z_wtp) &gt; 3, na.rm = TRUE)\nprop_absz_gt2; prop_absz_gt3\n\n# As a point of reference, here's what we'd expect in a standard normal distribution:\np_absz_gt2_normal &lt;- 2 * (1 - pnorm(2))  # about 4.55%\np_absz_gt3_normal &lt;- 2 * (1 - pnorm(3))  # about 0.27%\np_absz_gt2_normal; p_absz_gt3_normal\n\n\n\n\nFrom Dollars → z → Probability (pnorm)\nOK, but to the managerial relevance/translation point, here’s the cool thing about z’s: they make it really easy to get answers to specific, intuitive, practical questions.\nFor example, suppose our brewery client asked:\n“What fraction of customers have WTP &gt;= $10?”\n\n(First we would specify the sampling limitations here but then we could do the following)\n\n# Step 1: Convert $10 into z.\nz_for_10 &lt;- (10 - wtp_mean) / sd_wtp\nz_for_10\n\n# Step 2: Identify the right-tail probability at that z:\n# (Right-tail refers to the right side of the distribution, so we're trying to \n# identify the percentage of observations to the right of (or greater than) a\n# particular value)\np_WTP_ge_10 &lt;- 1 - pnorm(z_for_10)\np_WTP_ge_10\n\n# Alternatively, you can specify within the command that you want a right-tail \n# probability by including \"lower.tail=FALSE\" within the command as seen below:\np_WTP_ge_10lt &lt;- pnorm(z_for_10, lower.tail = FALSE)  # same as 1 - pnorm(z_for_10)\np_WTP_ge_10lt\n\nWe get the same thing, so your preference. Specifying as 1-pnorm is just less typing and I’m lazy.\nNote: If you want a left-tail question - so if you are trying to identify the percentage of observations to the left of (or less than) a given value - you can just use pnorm(z_for_8) directly, you would not use 1-pnorm(z_for_8).)\nSimilarly, our client could ask us something like:\n“What WTP are we looking at around the 90th percentile?”\n\nIn other words, “at what value of WTP are only 10% of responses higher amounts?” - if this is a super small batch, specialty product, we might be perfectly happy capturing only that top 10% of the market in terms of WTP.\nWe can answer that question as follows:\n\nz_90   &lt;- qnorm(0.90)                 # z for the 90th percentile\nwtp_90 &lt;- wtp_mean + z_90 * sd_wtp    # back to dollars\nwtp_90\n\nHandy rule of thumb: qnorm(0.975) ≈ 1.96 (aka the 95% CI ±1.96 SD trick).\n\n\n\nComparing Two Customers Fairly (by Standardizing)\nAnother question we might get from our client could be something like\n“I asked two different client what their WTP was and got two pretty different answers - on said 8.50 and the other said 11.75. Which customer is farther from typical?”\nWe can do that, too!\n\nwtp_A &lt;- 8.50\nwtp_B &lt;- 11.75\n\nz_A &lt;- (wtp_A - wtp_mean) / sd_wtp\nz_B &lt;- (wtp_B - wtp_mean) / sd_wtp\nz_A; z_B\n\nThe bigger |z| is the more unusual the customer. That’s the whole point of z: one scale to rule them all, whether the raw units are dollars, minutes, or tacos.\n\nThat’s it. You computed center and spread, standardized values, and mapped z to probabilities and percentiles. If you want to keep playing, try different dollar thresholds or percentiles (e.g., 25th, 75th) and see how the answers move or play with the little bonus example below:"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week4-exercise.html#bonus-pnorm-and-qnorm-are-opposites",
    "href": "teaching/marketing-research/assignments/week4-exercise.html#bonus-pnorm-and-qnorm-are-opposites",
    "title": "Week 4 Exercise: Variance, Standard Deviation & Z-Scores",
    "section": "Bonus — pnorm() and qnorm() are Opposites",
    "text": "Bonus — pnorm() and qnorm() are Opposites\nOK, you made it this far… here’s a neat little trick to finish.\npnorm() takes a z and gives you a probability (area under the curve). qnorm() takes a probability and gives you a z (the cutoff point). They’re like two sides of the same coin.\nLet’s start with z = 2. pnorm(2) gives us the probability of being ≤ 2 SD above the mean.\n\npnorm(2)   # ≈ 0.9772 → about 97.7% of values are below z = 2\n\n# Now feed that probability right back into qnorm():\nqnorm(0.9772499)\n# …and you get 2 again (within tiny rounding error). \n\n# Note also that instead of re-entering the value we got from pnorm, we can just\n# nest the commands as below:\nqnorm(pnorm(2))\n# That's the round-trip: z → p → z.\n\nTry the reverse:\n\nqnorm(0.975)         # → 1.96 (the 97.5th percentile cutoff)\npnorm(1.959964)\n# or with nested commands instead:\npnorm(qnorm(0.975))  # → 0.975 (the probability we started with)\n\nSo pnorm() and qnorm() are mathematical opposites. That’s why they’re so useful: one jumps from SD-units to probabilities, the other jumps back from probabilities to SD-units.\nNot a ton of use to going roundtrip here but it is a fun way to check your intuition and your code."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html",
    "href": "teaching/marketing-research/assignments/stout_exercise.html",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "",
    "text": "TipDownload Option\n\n\n\nIf you prefer to work directly in RStudio, you can download the R script version of this exercise."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#overview",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#overview",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Overview",
    "text": "Overview\nSo, here’s the situation. We’ve been hired by a local brewery to help them figure out how to price this new stout they’re putting on the menu. They’ve given us a bit of data (it’s not much) and we have a recommended price from the Brewmaster and we’re just going to make the most of it. Sound good? Alright, let’s get to work.\nGoals for this exercise:\n\nLoad the stout_research.csv dataset (it’s in the folder for this week on Brightspace)\nBuild three different models of willingness to pay\nCalculate errors (DATA - MODEL)\nVisualize how model predictions compare to the actual data\n\nRemember: DATA = MODEL + ERROR\nThat means ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-1-load-the-dataset",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-1-load-the-dataset",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 1 — Load the Dataset",
    "text": "Part 1 — Load the Dataset\nFirst, let’s load our dataset directly from the .csv file. Make sure “stout_research.csv” is saved in your working directory. If you’re building from assignment 1 and working from your project file, this is likely in the class folder you established last time and you may or may not need to specify the Datasets subfolder.\nTip: use getwd() to check your current working directory and setwd() if you need to change where R is looking.\n\nstout_data &lt;- read.csv(\"Datasets/stout_research.csv\")\n\nLet’s take a quick peek at the dataset (this function shows us just the top rows of a dataset):\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-2-look-at-the-structure-of-the-data",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-2-look-at-the-structure-of-the-data",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 2 — Look at the Structure of the Data",
    "text": "Part 2 — Look at the Structure of the Data\nWe should see 4 columns:\n\nID — an ID number for each participant\nLikesStout — Yes/No\nLocalImportant — Yes/No\nWTP — willingness to pay in dollars\n\n\nstr(stout_data)\n\nYou can probably kind of tell what that function did, namely, produce a little summary of what each variable in the dataset is doing, but a nice tip in RStudio is that you always type “?” followed immediately by the function that you’re trying to run or better understand and it will pull up the documentation explaining what that particular command in R is doing in the bottom right of RStudio. Let’s give it a quick shot:\n\n?str\n\nAs you read that documentation, you’ll notice it says that str is a common alternative to summary. Maybe we should give that a try, too.\n\nsummary(stout_data)\n\nBut wait, there’s more. What if I want to ask R to summarize not the entire stout dataset, but just teh willingness to pay data? Well, that’s going to look something like this:\n\nsummary(stout_data$WTP)\n\nWhat we’re doing in that command is once again orienting R with exactly where we want it to look. The statement says “Hey, R, go ahead and do summary on this thing that exists in the dataset I have loaded named stout_data. Once you get in stout_data, you’re going to want to look for the variable named WTP. Use that and only that.”\nSo same info as before, but a way to be more specific in asking R to focus on what I specifically want more information about as opposed to just summarizing everything."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-3-model-0-the-brewmasters-intuition",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-3-model-0-the-brewmasters-intuition",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 3 — Model 0: The Brewmaster’s Intuition",
    "text": "Part 3 — Model 0: The Brewmaster’s Intuition\nOur brewmaster says: “Price it at $7 per pint.” That’s a model! It predicts 7 for everyone.\nSo here’s how we can specify and examine that model: we add a new column to the dataset literally predicting “7” for every single respondent. We’ll call that new column/variable Model_Brewmaster:\n\nstout_data$Model_Brewmaster &lt;- 7\n\nSo, breaking down that code really quick, we’re telling R “go into ‘stout_data’ dataset, look for the column/variable named ‘Model_Brewmaster’ and if there isn’t such a thing create it, then make every observation in that column of the dataset = 7.\nLet’s check to make sure that worked the way we expected…\n\nhead(stout_data)\n\nOK, now let’s calculate error according to our guiding light,\nERROR = DATA - MODEL\nRemember, the data in this case are the WTP data from the dataset we were given. The model we’re examining is the Brewmaster’s suggested price of $7. The formula above tells us that error is just going to be the difference between those two things, the data I have (WTP) and the predictions made by my model ($7).\n\nIn this case, we can just ask R to calculate the difference between each person’s reported amount in our dataset and that guess of 7. We’ll call that value in our dataset Error_Brewmaster:\n\nstout_data$Error_Brewmaster &lt;- stout_data$WTP - stout_data$Model_Brewmaster\n\nWhat this line of code is saying to R is: “R, I want you to go to that dataset I have loaded called stout_data and when you get there, create a NEW variable that you’re going to call Error_Brewmaster. The way that you will create Error_Brewmaster will be to subtract the value associated with the existing variable Model_Brewmaster from the value associated with the existing variable WTP for each observation in the dataset.”\n…And because I’m compulsive, I’m going to check again to make sure it did exactly that:\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-4-model-1-the-sample-mean",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-4-model-1-the-sample-mean",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 4 — Model 1: The Sample Mean",
    "text": "Part 4 — Model 1: The Sample Mean\nOK, so the brewmaster gave us a model but these clients are paying us to make a recommendation…so we should probably try at least a few things including some models of our own fit to the actual data.\n\nSo, let’s try a data-based model: predict the average WTP for everyone.\nHere we’re going to create a value in R that is called mean_wtp. We could call it FantasticBananas and it wouldn’t make a difference, R doesn’t care what we name things. R does care that we specify how to calculate or define that value, though, which is everything that is happening to the right of the &lt;-\nOn the right side of that operator where it says “mean(stout_data$WTP)”, we’re saying “R, that thing we asked you to create, here’s how you create it: run the function mean on the variable WTP that you will find in the dataset we have loaded titled stout_data”.\nNow we can just enter “mean_wtp” and it will show us what that calculated value is.\n\nmean_wtp &lt;- mean(stout_data$WTP)\nmean_wtp   # this should come out to 7.425\n\nNow, as before with our brewmaster models, we need to add predictions and errors for this model to our dataset:\n\nstout_data$Model_Mean &lt;- mean_wtp\nstout_data$Error_Mean &lt;- stout_data$WTP - stout_data$Model_Mean\n\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-5-model-2-split-by-stout-preference",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-5-model-2-split-by-stout-preference",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 5 — Model 2: Split by Stout Preference",
    "text": "Part 5 — Model 2: Split by Stout Preference\nFinally, let’s allow our model to use one variable to inform creating different estimates of WTP for customers with different characteristics. Namely, whether or not someone likes stout. We’ll calculate the average WTP separately for people who responded to the question asking if they like stout with a “Yes” versus a “No” as follows:\n\nmean_yes &lt;- mean(stout_data$WTP[stout_data$LikesStout == \"Yes\"])\nmean_no  &lt;- mean(stout_data$WTP[stout_data$LikesStout == \"No\"])\n\nOK, and now let’s check to see what they were:\n\nmean_yes  # should be ~8.46\nmean_no   # should be ~5.88\n\nAdd predictions to the dataset with this line of code:\n\nstout_data$Model_Like_Stout &lt;- ifelse(stout_data$LikesStout == \"Yes\", mean_yes, mean_no)\n\nThe line above is slightly more complex than the other variables we’ve created but if you break it down piece by piece, you’ll note that it all makes sense/isn’t that complicated. We’re telling R to create another new variable in the existing dataset (stout_data) and to call that new variable Model_Like_Stout. We then have to tell R how to fill in that column and this time, there are two different options. The statement we make there is saying IF the response on that row to the existing LikesStout item is “Yes”, then R should put the value we calculate for the mean of the yes respondents for that new variable. In all other cases, we’re telling R to go ahead and enter the mean of the no respondents for that new variable.\nAnd calculate the errors again:\n\nstout_data$Error_Like_Stout &lt;- stout_data$WTP - stout_data$Model_Like_Stout\n\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-6-compare-models-visually",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-6-compare-models-visually",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 6 — Compare Models Visually",
    "text": "Part 6 — Compare Models Visually\nFor this section, here in just a second I am going to ask you to run some code as a block all at once and not worry too much about exactly what’s happening — the point for now is to create the figure to look at what’s going on with the performance of our models (as captured by the differences in ERROR) and not so much to try to learn the ins and outs of the ggplot code creating the figure (plenty of time for that later).\nThat said, our goals for this plot:\n\nShow actual WTP for each customer (points).\nShow each model’s prediction for each customer.\nMake it crystal clear that Brewmaster and Sample Mean are static estimates (we make the same prediction for everyone), whereas the Likes Stout model changes by respondent (we have two possible estimates to choose from for any given observation).\n\n\nInstalling and Loading Packages\nIn order to create this figure, you’re going to need to use a couple of commands that don’t come with stock R…but don’t worry, it’s not a DLC situation where each additional command is going to cost you $20, it’s just a DLC in the sense that you will need to download (or install) a couple of packages in R so that it knows what you mean when you use these new commands.\nSo the first time you run this, if you haven’t installed these packages previously, you will need to run the following install code:\n\n##### AFTER YOU RUN THE FOLLOWING LINES ONE TIME, COMMENT THEM OUT BY ADDING\n##### A \"#\" IN FRONT OF EACH OF THE THREE LINES BELOW. YOU DON'T NEED TO INSTALL THE\n##### PACKAGES AGAIN\n#\n##### TO COMMENT OUT A LINE, JUST PUT A \"#\" AT THE VERY START OF THE LINE.\n##### FOR EXAMPLE, THE LINE BELOW IS COMMENTED OUT:\n##### # install.packages(\"ggplot2\")\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\n\nOK, but then once the packages are installed you will need to make sure that they’re loaded up and ready to go from the things in your library before you try to use them, so run the following three:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\nCreating the Visualization\nNow for the rest of this section, you can just go ahead and highlight the whole chunk of code and run it.\nTo be frank, I had to have chatgpt help me troubleshoot and write the very first section with the ID vars to get the figure to look the way I want it to look. Visualizations are important but coding them is, frankly, not my strong point and sometimes I need help.\n\n# Create a numeric x position for each row and keep the ID labels for display\nstout_data &lt;- stout_data |&gt;\n  arrange(ID) |&gt;\n  mutate(x_i = row_number())\n\n# Gather predictions to long format for faceting\nviz_long &lt;- stout_data |&gt;\n  select(ID, x_i, WTP, LikesStout, Model_Brewmaster, Model_Mean, Model_Like_Stout) |&gt;\n  pivot_longer(\n    cols = c(Model_Brewmaster, Model_Mean, Model_Like_Stout),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) |&gt;\n  mutate(\n    Model = factor(\n      Model,\n      levels = c(\"Model_Brewmaster\", \"Model_Mean\", \"Model_Like_Stout\"),\n      labels = c(\"Brewmaster ($7 flat)\", \n                 \"Sample Mean (~$7.43 flat)\", \n                 \"Likes Stout (two-level)\")\n    )\n  )\n\n\n# Create figure visualizing the residuals (ERROR = DATA – MODEL)\nggplot(viz_long, aes(x = x_i)) + \n  # thin vertical line shows leftover (residual): from prediction up to actual\n  geom_segment(aes(y = Pred, yend = WTP, xend = x_i),\n               linewidth = 0.6, alpha = 0.7, color = \"red\") +\n  # prediction planks\n  geom_segment(aes(y = Pred, yend = Pred,\n                   x = x_i - 0.3, xend = x_i + 0.3),\n               linewidth = 2, color = \"blue\") +\n  # actual dots\n  geom_point(aes(y = WTP), size = 2, color = \"black\") +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = stout_data$x_i, labels = stout_data$ID) +\n  labs(title = \"ERROR = DATA – MODEL (visualizing the ERROR or the residuals)\",\n       subtitle = \"Red lines = ERROR • Blue bars = MODEL predictions • Dots = DATA on actual WTP\",\n       x = \"Respondent ID\",\n       y = \"WTP ($)\") +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-7-review",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-7-review",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 7 — Review",
    "text": "Part 7 — Review\nWhat did we see?\n\nModel 0 (Brewmaster’s $7): simple, but lots of leftover error.\nModel 1 (Sample Mean): closer, error shrinks.\nModel 2 (Likes Stout): explains much more, error shrinks further.\n\nThis is DATA = MODEL + ERROR (and conversely ERROR = DATA - MODEL) in action."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-8-proportional-reduction-in-error-pre",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-8-proportional-reduction-in-error-pre",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 8 — Proportional Reduction in Error (PRE)",
    "text": "Part 8 — Proportional Reduction in Error (PRE)\nPRE = (Error_baseline - Error_new) / Error_baseline\nWe’ll use SSE (sum of squared errors) as our measure of error size here. Note there are alternative ways to measure/conceptualize error and we will discuss them in great detail next week but for now we’re going to just square it.\nThe code below will calculate SSE for each model:\n\nSSE_brewmaster &lt;- sum((stout_data$WTP - stout_data$Model_Brewmaster)^2)\nSSE_mean       &lt;- sum((stout_data$WTP - stout_data$Model_Mean)^2)\nSSE_likes_stout      &lt;- sum((stout_data$WTP - stout_data$Model_Like_Stout)^2)\n\nLet’s take a look at those values now:\n\nSSE_brewmaster\nSSE_mean\nSSE_likes_stout\n\n\nComparing Models with PRE\nOK, now we want to compare some of our models using the PRE formula.\nAs you look at these results, I want you to think about what we’re really doing in the “likes stout” model compared to the others in marketing terms… instead of just having the same strategy for understanding/predicting an entire market, we’re coming up with different strategies for understanding and predicting different segments of the market…which is a lot like…\nSEGMENTATION!\nThat’s right! A very rudimentary version of it, but look how well it performs compared to the others!\nLet’s give it a go:\nPRE: Sample Mean vs. Brewmaster\n\nPRE_mean_vs_brew &lt;- (SSE_brewmaster - SSE_mean) / SSE_brewmaster\nPRE_mean_vs_brew   # proportion of error reduced\n\nPRE: Likes Stout vs. Sample Mean\n\nPRE_likesstout_vs_mean &lt;- (SSE_mean - SSE_likes_stout) / SSE_mean\nPRE_likesstout_vs_mean\n\nPRE: Likes Stout vs. Brewmaster (direct comparison)\n\nPRE_likesstout_vs_brew &lt;- (SSE_brewmaster - SSE_likes_stout) / SSE_brewmaster\nPRE_likesstout_vs_brew"
  },
  {
    "objectID": "teaching/marketing-research/shinyapps/SimpleRegressionIntuitionBuilder.html",
    "href": "teaching/marketing-research/shinyapps/SimpleRegressionIntuitionBuilder.html",
    "title": "Simple Regression Intuition Builder",
    "section": "",
    "text": "TipAlternative Access\n\n\n\nIf the embedded app below doesn’t display correctly in your browser, you can open it directly in a new tab."
  },
  {
    "objectID": "teaching/marketing-research/shinyapps/SimpleModelsFStats.html",
    "href": "teaching/marketing-research/shinyapps/SimpleModelsFStats.html",
    "title": "Simple Models F Stats",
    "section": "",
    "text": "TipAlternative Access\n\n\n\nIf the embedded app below doesn’t display correctly in your browser, you can open it directly in a new tab."
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html",
    "href": "teaching/marketing-research/text-analysis-tutorial.html",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "",
    "text": "This tutorial walks through techniques for analyzing open-ended text responses using R. We’ll use real data from my teaching evaluations spanning 2017–2025, covering courses in Consumer Behavior, Personal Selling & Sales Management, and Marketing Research.\n\n\n\n\n\n\nNoteWhy Use This Data?\n\n\n\nOpen-ended survey responses are everywhere in marketing research — customer feedback, product reviews, social media comments, focus group transcripts. The techniques you’ll learn here apply to all of these contexts. Using teaching evaluations is just a convenient (and somewhat vulnerable!) way to demonstrate with real data.\n\n\nBy the end of this tutorial, you’ll be able to:\n\nLoad and clean text data\nTokenize text and remove stop words\nCalculate word frequencies\nCreate word clouds\nCompare text across groups (courses, modalities)\nPerform basic sentiment analysis"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#introduction",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#introduction",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "",
    "text": "This tutorial walks through techniques for analyzing open-ended text responses using R. We’ll use real data from my teaching evaluations spanning 2017–2025, covering courses in Consumer Behavior, Personal Selling & Sales Management, and Marketing Research.\n\n\n\n\n\n\nNoteWhy Use This Data?\n\n\n\nOpen-ended survey responses are everywhere in marketing research — customer feedback, product reviews, social media comments, focus group transcripts. The techniques you’ll learn here apply to all of these contexts. Using teaching evaluations is just a convenient (and somewhat vulnerable!) way to demonstrate with real data.\n\n\nBy the end of this tutorial, you’ll be able to:\n\nLoad and clean text data\nTokenize text and remove stop words\nCalculate word frequencies\nCreate word clouds\nCompare text across groups (courses, modalities)\nPerform basic sentiment analysis"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#setup",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#setup",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we’ll need:\n\n# Install packages if needed (uncomment and run once)\n# install.packages(c(\"tidyverse\", \"tidytext\", \"wordcloud\", \"wordcloud2\", \n#                    \"reshape2\", \"textdata\", \"scales\"))\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(scales)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-1-loading-and-exploring-the-data",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-1-loading-and-exploring-the-data",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 1: Loading and Exploring the Data",
    "text": "Part 1: Loading and Exploring the Data\n\nLoad the data\n\n# Load the teaching evaluation data\nevals &lt;- read_csv(\"teaching_evaluations.csv\")\n\n# Quick peek\nglimpse(evals)\n\n\n\nExplore the structure\n\n# How many comments per course?\nevals %&gt;%\n  count(course_name, sort = TRUE)\n\n# How many by modality?\nevals %&gt;%\n  count(modality)\n\n# How many by semester?\nevals %&gt;%\n  count(semester, sort = TRUE)\n\n\n\nPreview some comments\n\n# Look at a few random comments\nevals %&gt;%\n  sample_n(5) %&gt;%\n  select(course_name, modality, comment_text)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-2-text-cleaning-and-tokenization",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-2-text-cleaning-and-tokenization",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 2: Text Cleaning and Tokenization",
    "text": "Part 2: Text Cleaning and Tokenization\nText analysis requires breaking text into individual units (usually words). This process is called tokenization. Before we tokenize, we often need to clean the text.\n\nBasic tokenization with tidytext\nThe tidytext package makes this straightforward with unnest_tokens():\n\n# Tokenize: one row per word\nwords &lt;- evals %&gt;%\n  unnest_tokens(word, comment_text)\n\n# How many total words?\nnrow(words)\n\n# Preview\nhead(words, 20)\n\n\n\nRemoving stop words\nStop words are common words that don’t carry much meaning (the, a, is, and, etc.). We typically remove these:\n\n# tidytext includes a built-in stop word list\ndata(\"stop_words\")\nhead(stop_words, 20)\n\n# Remove stop words using anti_join\nwords_clean &lt;- words %&gt;%\n  anti_join(stop_words, by = \"word\")\n\n# How many words remain?\nnrow(words_clean)\n\n\n\nCustom stop words\nSometimes you’ll want to remove additional words that are common in your specific context but don’t add meaning:\n\n# Create custom stop words for teaching evaluations\ncustom_stops &lt;- tibble(word = c(\"class\", \"course\", \"professor\", \"carter\", \n                                 \"erin\", \"dr\", \"semester\", \"i'm\", \"i've\",\n                                 \"it's\", \"don't\", \"didn't\", \"wasn't\"))\n\n# Remove both standard and custom stop words\nwords_clean &lt;- words %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  anti_join(custom_stops, by = \"word\") %&gt;%\n  filter(str_detect(word, \"^[a-z]+$\"))  # Keep only alphabetic words\n\nnrow(words_clean)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-3-word-frequencies",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-3-word-frequencies",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 3: Word Frequencies",
    "text": "Part 3: Word Frequencies\nNow let’s see what words appear most often.\n\nOverall word frequency\n\n# Count word frequencies\nword_counts &lt;- words_clean %&gt;%\n  count(word, sort = TRUE)\n\n# Top 20 words\nword_counts %&gt;%\n  head(20)\n\n\n\nVisualize top words\n\nword_counts %&gt;%\n  head(20) %&gt;%\n  mutate(word = fct_reorder(word, n)) %&gt;%\n  ggplot(aes(x = n, y = word)) +\n  geom_col(fill = \"#2D6A4F\") +\n  labs(\n    title = \"Most Frequent Words in Teaching Evaluations\",\n    x = \"Frequency\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\nWord frequency by course\n\n# Count words within each course\nword_counts_by_course &lt;- words_clean %&gt;%\n  count(course_name, word, sort = TRUE)\n\n# Top 10 words per course\nword_counts_by_course %&gt;%\n  group_by(course_name) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder_within(word, n, course_name)) %&gt;%\n  ggplot(aes(x = n, y = word, fill = course_name)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~course_name, scales = \"free_y\") +\n  scale_y_reordered() +\n  scale_fill_manual(values = c(\"#2D6A4F\", \"#52796F\", \"#C85A3B\")) +\n  labs(\n    title = \"Top Words by Course\",\n    x = \"Frequency\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 11)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-4-word-clouds",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-4-word-clouds",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 4: Word Clouds",
    "text": "Part 4: Word Clouds\nWord clouds are a popular (if sometimes maligned) way to visualize text data. They’re great for quick impressions and presentations.\n\nBasic word cloud\n\n# Using the wordcloud package\nset.seed(42)  # For reproducibility\n\nword_counts %&gt;%\n  with(wordcloud(\n    words = word,\n    freq = n,\n    max.words = 100,\n    random.order = FALSE,\n    colors = brewer.pal(8, \"Dark2\")\n  ))\n\n\n\nInteractive word cloud with wordcloud2\n\n# wordcloud2 creates an interactive HTML widget\nwordcloud2(\n  data = word_counts %&gt;% head(100),\n  size = 0.8,\n  color = \"random-dark\",\n  backgroundColor = \"#FAF8F5\"\n)\n\n\n\nWord clouds by group\nLet’s create separate word clouds for in-person vs. online courses:\n\n# Get word counts by modality\ninperson_words &lt;- words_clean %&gt;%\n  filter(modality == \"in-person\") %&gt;%\n  count(word, sort = TRUE)\n\nonline_words &lt;- words_clean %&gt;%\n  filter(modality == \"online\") %&gt;%\n  count(word, sort = TRUE)\n\n# In-person word cloud\npar(mfrow = c(1, 2))  # Two plots side by side\n\nwordcloud(\n  words = inperson_words$word,\n  freq = inperson_words$n,\n  max.words = 75,\n  random.order = FALSE,\n  colors = brewer.pal(8, \"Greens\")[4:8],\n  main = \"In-Person\"\n)\n\nwordcloud(\n  words = online_words$word,\n  freq = online_words$n,\n  max.words = 75,\n  random.order = FALSE,\n  colors = brewer.pal(8, \"Blues\")[4:8],\n  main = \"Online\"\n)\n\npar(mfrow = c(1, 1))  # Reset\n\n\n\nComparison cloud\nA comparison cloud shows which words are distinctive to each group:\n\n# Reshape for comparison cloud\ncomparison_data &lt;- words_clean %&gt;%\n  count(modality, word) %&gt;%\n  pivot_wider(names_from = modality, values_from = n, values_fill = 0) %&gt;%\n  column_to_rownames(\"word\") %&gt;%\n  as.matrix()\n\ncomparison.cloud(\n  comparison_data,\n  max.words = 100,\n  colors = c(\"#2D6A4F\", \"#C85A3B\"),\n  title.size = 1.5\n)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-5-comparing-groups-with-tf-idf",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-5-comparing-groups-with-tf-idf",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 5: Comparing Groups with TF-IDF",
    "text": "Part 5: Comparing Groups with TF-IDF\nWord frequency alone can be misleading — common words will dominate across all groups. TF-IDF (Term Frequency-Inverse Document Frequency) helps identify words that are distinctive to each group.\n\n# Calculate TF-IDF by course\ncourse_tfidf &lt;- words_clean %&gt;%\n  count(course_name, word, sort = TRUE) %&gt;%\n  bind_tf_idf(word, course_name, n)\n\n# Top distinctive words per course\ncourse_tfidf %&gt;%\n  group_by(course_name) %&gt;%\n  slice_max(tf_idf, n = 10) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder_within(word, tf_idf, course_name)) %&gt;%\n  ggplot(aes(x = tf_idf, y = word, fill = course_name)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~course_name, scales = \"free_y\") +\n  scale_y_reordered() +\n  scale_fill_manual(values = c(\"#2D6A4F\", \"#52796F\", \"#C85A3B\")) +\n  labs(\n    title = \"Most Distinctive Words by Course (TF-IDF)\",\n    subtitle = \"Words that are uniquely common in each course\",\n    x = \"TF-IDF\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 11)"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-6-sentiment-analysis",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-6-sentiment-analysis",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 6: Sentiment Analysis",
    "text": "Part 6: Sentiment Analysis\nSentiment analysis attempts to determine the emotional tone of text. We’ll use a lexicon-based approach, where words are matched against a dictionary of words labeled with sentiments.\n\nLoad a sentiment lexicon\n\n# The \"bing\" lexicon classifies words as positive or negative\n# You may be prompted to download it the first time\nbing &lt;- get_sentiments(\"bing\")\n\nhead(bing, 20)\n\n\n\nCalculate sentiment\n\n# Join words with sentiment lexicon\nword_sentiments &lt;- words_clean %&gt;%\n  inner_join(bing, by = \"word\")\n\n# Count positive vs negative words\nword_sentiments %&gt;%\n  count(sentiment)\n\n\n\nSentiment by course\n\n# Sentiment breakdown by course\nsentiment_by_course &lt;- word_sentiments %&gt;%\n  count(course_name, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    total = positive + negative,\n    pct_positive = positive / total\n  )\n\nsentiment_by_course\n\n# Visualize\nsentiment_by_course %&gt;%\n  pivot_longer(cols = c(positive, negative), names_to = \"sentiment\", values_to = \"n\") %&gt;%\n  ggplot(aes(x = course_name, y = n, fill = sentiment)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = c(\"negative\" = \"#C85A3B\", \"positive\" = \"#2D6A4F\")) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Sentiment Distribution by Course\",\n    x = NULL,\n    y = \"Percentage of sentiment-bearing words\"\n  ) +\n  theme_minimal(base_size = 13) +\n  coord_flip()\n\n\n\nSentiment by modality\n\n# Compare in-person vs online sentiment\nsentiment_by_modality &lt;- word_sentiments %&gt;%\n  count(modality, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    total = positive + negative,\n    pct_positive = positive / total\n  )\n\nsentiment_by_modality\n\n\n\nSentiment over time\n\n# Extract year from semester\nword_sentiments_time &lt;- word_sentiments %&gt;%\n  mutate(year = str_extract(semester, \"\\\\d{4}\") %&gt;% as.numeric())\n\n# Sentiment by year\nsentiment_by_year &lt;- word_sentiments_time %&gt;%\n  count(year, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    total = positive + negative,\n    pct_positive = positive / total\n  )\n\nggplot(sentiment_by_year, aes(x = year, y = pct_positive)) +\n  geom_line(color = \"#2D6A4F\", size = 1) +\n  geom_point(color = \"#2D6A4F\", size = 3) +\n  scale_y_continuous(labels = percent, limits = c(0.5, 1)) +\n  labs(\n    title = \"Sentiment Over Time\",\n    subtitle = \"Percentage of positive sentiment-bearing words\",\n    x = \"Year\",\n    y = \"% Positive\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\nMost common positive and negative words\n\n# Top positive words\nword_sentiments %&gt;%\n  filter(sentiment == \"positive\") %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(15) %&gt;%\n  mutate(word = fct_reorder(word, n)) %&gt;%\n  ggplot(aes(x = n, y = word)) +\n  geom_col(fill = \"#2D6A4F\") +\n  labs(title = \"Most Common Positive Words\", x = \"Frequency\", y = NULL) +\n  theme_minimal()\n\n# Top negative words\nword_sentiments %&gt;%\n  filter(sentiment == \"negative\") %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(15) %&gt;%\n  mutate(word = fct_reorder(word, n)) %&gt;%\n  ggplot(aes(x = n, y = word)) +\n  geom_col(fill = \"#C85A3B\") +\n  labs(title = \"Most Common Negative Words\", x = \"Frequency\", y = NULL) +\n  theme_minimal()"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#part-7-going-further",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#part-7-going-further",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Part 7: Going Further",
    "text": "Part 7: Going Further\n\nN-grams\nInstead of single words, we can look at pairs (bigrams) or triplets (trigrams):\n\n# Extract bigrams\nbigrams &lt;- evals %&gt;%\n  unnest_tokens(bigram, comment_text, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\n# Count bigrams\nbigram_counts &lt;- bigrams %&gt;%\n  count(bigram, sort = TRUE)\n\nhead(bigram_counts, 20)\n\n\n\nFilter bigrams to remove stop words\n\n# Separate, filter, and reunite\nbigrams_filtered &lt;- bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word,\n         !word1 %in% custom_stops$word,\n         !word2 %in% custom_stops$word) %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigrams_filtered %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  head(20)\n\n\n\nAlternative sentiment lexicons\nThe tidytext package provides access to several sentiment lexicons:\n\n# AFINN: words scored from -5 (very negative) to +5 (very positive)\nafinn &lt;- get_sentiments(\"afinn\")\n\n# Calculate average sentiment score per course\nwords_clean %&gt;%\n  inner_join(afinn, by = \"word\") %&gt;%\n  group_by(course_name) %&gt;%\n  summarize(\n    avg_sentiment = mean(value),\n    n_words = n()\n  )"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#wrapping-up",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#wrapping-up",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nYou’ve now learned the fundamentals of text analysis in R:\n\nTokenization: Breaking text into words\nStop word removal: Filtering out common words\nWord frequencies: Counting and visualizing common terms\nWord clouds: Visual summaries of text\nTF-IDF: Finding distinctive words across groups\nSentiment analysis: Measuring emotional tone\n\nThese techniques form the foundation for more advanced text analysis methods like topic modeling, named entity recognition, and text classification.\n\n\n\n\n\n\nTipPractice Ideas\n\n\n\n\nTry analyzing your own text data (reviews, social media posts, survey responses)\nExperiment with different stop word lists\nCompare sentiment lexicons (bing vs. AFINN vs. NRC)\nCreate word clouds for different subsets of the data"
  },
  {
    "objectID": "teaching/marketing-research/text-analysis-tutorial.html#resources",
    "href": "teaching/marketing-research/text-analysis-tutorial.html#resources",
    "title": "Analyzing Open-Ended Responses in R",
    "section": "Resources",
    "text": "Resources\n\nText Mining with R by Julia Silge & David Robinson — free online book\ntidytext documentation\nwordcloud2 documentation"
  },
  {
    "objectID": "teaching/marketing-research/index.html",
    "href": "teaching/marketing-research/index.html",
    "title": "Marketing Research & Analytics",
    "section": "",
    "text": "This course provides hands-on training in research design, data collection, and analysis using R. You’ll learn to translate quantitative findings into actionable marketing insights—and build skills that transfer to any data-driven role."
  },
  {
    "objectID": "teaching/marketing-research/index.html#interactive-tools",
    "href": "teaching/marketing-research/index.html#interactive-tools",
    "title": "Marketing Research & Analytics",
    "section": "Interactive Tools",
    "text": "Interactive Tools\nThese Shiny applications help you explore statistical concepts visually. Play with them to build intuition before diving into the math.\n\nBagel Run Predictor — Our first attempts at some basic model comparisons…but make it about bagels…\nCentral Tendency & CLT Explorer — Visualizing samples, distributions, measures of central tendency, and Z scores\nSimple Models and F Stats — This week we are beginning to do some very simple model comparisons and start working with new model comparison statistics\nSimple Models and F Stats — We’re officially working with model comparisons of mean only versus simple regression models!\n\n\n\n\n\n\n\nMore Coming Soon · Links to Shiny apps will be added as they’re migrated to the new site."
  },
  {
    "objectID": "teaching/marketing-research/index.html#coding-assignments",
    "href": "teaching/marketing-research/index.html#coding-assignments",
    "title": "Marketing Research & Analytics",
    "section": "Coding Assignments",
    "text": "Coding Assignments\nThese assignments build your R skills progressively. Each is designed to be accessible—you don’t need prior programming experience.\n\nGetting Started\n\nWeek 1: File Management Practice — Install R/RStudio, set up your folders, learn relative paths\nWeek 2: Stout Exercise — Practice loading data, intro to model comparison\nWeek 3: Coffee Errors Exercise — In this exercise you will calculate A LOT (sorry but it’s important for building understanding long term) of different types of error\nWeek 3: Stout Festival Exercise — New data from our week 2 client! Now we’ll play with different measures of central tendency and error\nWeek 4: Examining Spread and Standardization — In this exercise we examine why and how to examine the spread or variability of our data. We also have some fun (maybe?) with Z scores\nWeek 5: PRE, Critical Values, and F Tests — In this exercise we start really working on some more substantial model comparisons and the PRE, critical value, and F Test conversations that allows follow\nWeek 6: Power, Sensitivity, SESOI, Effect Size, CIs, and OPTIONAL Intro to Bootstrapping — This week we examine different ways of measuring how sure we are of an effect - that we’ll find it, that it is “big enough” to matter, and how to talk about these concerns with statistics or otherwise\n\n\n\n\n\n\n\nMore coming · Additional assignments will be added throughout the semester."
  },
  {
    "objectID": "teaching/marketing-research/index.html#resources",
    "href": "teaching/marketing-research/index.html#resources",
    "title": "Marketing Research & Analytics",
    "section": "Resources",
    "text": "Resources\n\nSetup\nBefore the first assignment, you’ll need:\n\nR — Download from CRAN\nRStudio — Download from Posit\n\nI’ll walk through installation in class, but these links have everything you need.\n\n\nReferences\n\nR for Data Science — Free online textbook; excellent for beginners\nQuarto Documentation — For when you’re ready to create your own reports\nggplot2 Cheat Sheet — Keep this handy\n\n\n\nGetting Help\nStuck on an assignment? In order:\n\nRe-read the error message carefully\nGoogle the error message (seriously—this is what professionals do)\nCheck the course discussion board\nCome to office hours"
  },
  {
    "objectID": "teaching/time-management/index.html",
    "href": "teaching/time-management/index.html",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "",
    "text": "This course examines the psychological foundations of time management, moving beyond simple productivity hacks to understand why certain strategies work and when they’re most effective."
  },
  {
    "objectID": "teaching/time-management/index.html#course-format",
    "href": "teaching/time-management/index.html#course-format",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Course Format",
    "text": "Course Format\nAsynchronous online. All materials are designed to be accessible and self-paced."
  },
  {
    "objectID": "teaching/time-management/index.html#modules",
    "href": "teaching/time-management/index.html#modules",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse modules and materials are under development for the current semester."
  },
  {
    "objectID": "teaching/time-management/index.html#readings",
    "href": "teaching/time-management/index.html#readings",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nReadings will be added as the course develops."
  },
  {
    "objectID": "teaching/time-management/index.html#assignments",
    "href": "teaching/time-management/index.html#assignments",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment materials will be added here."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "I teach courses that help students understand how consumers think, how to build relationships that drive sales, and how to use data to make better marketing decisions. I also develop professional skills courses on topics I wish someone had taught me earlier."
  },
  {
    "objectID": "teaching/index.html#core-courses",
    "href": "teaching/index.html#core-courses",
    "title": "Teaching",
    "section": "Core Courses",
    "text": "Core Courses\n\nConsumer Behavior\nMKT 382 · Undergraduate\nWhy do people buy what they buy? This course examines the psychological, social, and cultural factors that shape consumer decisions—from perception and attention to attitudes, emotions, and social influence. Students learn to apply these insights to marketing strategy.\nOffered in-person and asynchronously online.\nCourse Materials →\n\n\nSales Management & Personal Selling\nMKT 374 · Undergraduate\nSales is fundamentally about understanding people and building relationships. This course develops both the interpersonal skills of effective selling and the strategic thinking required to manage sales teams and processes.\nOffered in-person and asynchronously online.\nCourse Materials →\n\n\nMarketing Research & Analytics\nMKT 378 · Undergraduate\nHands-on training in research design, data collection, and analysis using R. Students work with real data and learn to translate quantitative findings into actionable marketing insights. Includes custom-built interactive tools for exploring statistical concepts.\nCourse Materials →"
  },
  {
    "objectID": "teaching/index.html#professional-skills-courses",
    "href": "teaching/index.html#professional-skills-courses",
    "title": "Teaching",
    "section": "Professional Skills Courses",
    "text": "Professional Skills Courses\nOne-credit courses focused on building specific capabilities. Both are offered asynchronously online.\n\nTime Management: The Psychology of Getting Things Done\n1 Credit · Professional Skills\nBeyond productivity hacks: this course examines the psychological foundations of time management—attention, motivation, habit formation, and sustainable productivity. Students learn why certain strategies work and when to apply them.\nCourse Materials →\n\n\nMoral Judgment & Decision Making in the Marketplace\n1 Credit · Professional Skills · Coming Spring 2026\nHow do consumers, managers, and organizations navigate ethical decisions? This course examines frameworks for moral reasoning and applies them to contemporary business challenges—from pricing fairness to data ethics to sustainability.\nCourse Materials →"
  },
  {
    "objectID": "teaching/index.html#special-topics",
    "href": "teaching/index.html#special-topics",
    "title": "Teaching",
    "section": "Special Topics",
    "text": "Special Topics\nI’ve also developed experiential courses connecting business students with sustainable agriculture in Maine, in partnership with the BARD Institute and organizations including Maine Farmland Trust, the Maine Cheese Guild, and Maine Fibershed. These courses—funded in part by a congressional earmark—give students hands-on experience applying business skills in contexts they rarely encounter in traditional coursework."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research uses behavioral economics and social psychology to understand how people make judgments and decisions as consumers, employees, and citizens. I’m drawn to questions where understanding the psychology can lead to better outcomes—for individuals, for organizations, and for society.\nFor a complete list of publications and working papers, see my CV."
  },
  {
    "objectID": "research.html#well-being",
    "href": "research.html#well-being",
    "title": "Research",
    "section": "Meaning, Pleasure, and Well-Being",
    "text": "Meaning, Pleasure, and Well-Being\nThe pursuit of happiness drives many consumption decisions. But what do people actually mean when they say they want to be happy?\nWith Lawrence Williams, I study the distinction between happiness as pleasure (hedonic well-being) and happiness as meaning (eudaimonic well-being). We find that consumers approach these differently: compared to pursuing pleasure, pursuing meaning involves expectations of lasting benefits—and requires greater minimum time investments to derive those benefits.\nThis has implications for how we design experiences, how we market them, and how consumers allocate their most precious resource: time.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nConsumers’ Minimum Time Investments in Meaningful Consumption Percival Carter, Williams, & Light • Marketing Letters, 2023 Read →"
  },
  {
    "objectID": "research.html#fairness",
    "href": "research.html#fairness",
    "title": "Research",
    "section": "Fairness, Harm, and Moral Judgment",
    "text": "Fairness, Harm, and Moral Judgment\nWhen is a price unfair? When does a company cross an ethical line? These aren’t just philosophical questions—they drive consumer behavior, shape reputations, and determine market outcomes.\nMy work in this area approaches price fairness as fundamentally a moral judgment, shaped by perceptions of harm to consumers. I’ve also examined how pregnancy is perceived in academic institutions, how harassment and discrimination policies fail, and how consumers make demands of entrepreneurs based on gender.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nPainful Prices: A Moral Harm Approach to Price Fairness Campbell, Pomerance, & Percival Carter • Journal of Consumer Research, 2025\nReframing and Restructuring Organizational Strategies for Addressing Workplace Harassment and Discrimination Percival Carter & Obenauer • Group & Organization Management, 2025\nPower and the Perception of Pregnancy in the Academy Percival Carter • Gender, Work & Organization, 2023 Read →"
  },
  {
    "objectID": "research.html#sustainability",
    "href": "research.html#sustainability",
    "title": "Research",
    "section": "Sustainability and Food Systems",
    "text": "Sustainability and Food Systems\nHow do consumers think about local, sustainable, and artisan food? And how can small-scale producers compete in markets dominated by industrial agriculture?\nThis research stream connects my academic interests to work I care about deeply. With partners including Maine Farmland Trust, the Maine Cheese Guild, and Maine Fibershed, I study how consumers perceive small-scale producers’ products differently from conventional alternatives—and how producers can design experiences that cultivate genuine connection with their customers.\nCurrent work examines how Maine can communicate about PFAS contamination in ways that inform consumers without unfairly damaging perceptions of all Maine food.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nDesigning and Distinguishing Meaningful Artisan Food Experiences Percival Carter & Welcomer • Sustainability, 2021 Read →"
  },
  {
    "objectID": "research.html#authenticity",
    "href": "research.html#authenticity",
    "title": "Research",
    "section": "Authenticity and Imperfection",
    "text": "Authenticity and Imperfection\nCan a product be too perfect?\nWith Pete McGraw, I study how flaws can actually increase product evaluations. The key insight: when consumers care about production processes that are difficult to verify (organic farming, handmade crafts), visible imperfections serve as credible signals. A blemished apple might be more appealing than a perfect one—because the blemish reduces uncertainty about how it was grown.\nThis work has implications for artisan producers, sustainable agriculture, and anyone competing on authenticity rather than polish.\n\n\n\n\n\n\nNoteWorking Paper\n\n\n\n\n\nIn Pursuit of Imperfection: Flawed Products Reduce Process Uncertainty Percival Carter & McGraw • Preparing for submission\nAs discussed on the Here We Are podcast and demonstrated memorably by Ron Swanson."
  },
  {
    "objectID": "research.html#hype",
    "href": "research.html#hype",
    "title": "Research",
    "section": "Hype and Its Discontents",
    "text": "Hype and Its Discontents\nChampionships. Series finales. Award shows. Millions of people tune in to hyped events even when they have no intrinsic interest in boxing, period dramas, or filmmaking.\nWith Lawrence Williams and Pete McGraw, I studied how “believing the hype” affects consumer well-being. Using data from nearly 7,000 people across 16 hyped television events, we found that giving in to hype is largely detrimental—with one exception: it can improve social well-being by helping solitary viewers feel connected through shared cultural experiences.\n\n\n\n\n\n\nNoteWorking Paper\n\n\n\n\n\nHow Hype Helps and Hinders Well-Being Percival Carter, Williams, & McGraw • Under revision"
  },
  {
    "objectID": "research.html#humor",
    "href": "research.html#humor",
    "title": "Research",
    "section": "Humor",
    "text": "Humor\nIn a previous life, I managed the Humor Research Lab at the University of Colorado Boulder. That work examined when humor helps and hurts brand attitudes—finding that the key isn’t whether an ad is funny, but whether it triggers negative emotional reactions independent of humor.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nBeing Funny is Not Enough: Negative Feelings Predict When Humor is Persuasive Warren, Percival Carter, & McGraw • International Journal of Advertising, 2019"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Erin Percival Carter Associate Professor of Marketing Maine Business School, University of Maine\nEmail · erin.p.carter@maine.edu Office · 5723 Donald P. Corbett Business Building, Orono, ME Phone · (207) 481-4944"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Erin Percival Carter Associate Professor of Marketing Maine Business School, University of Maine\nEmail · erin.p.carter@maine.edu Office · 5723 Donald P. Corbett Business Building, Orono, ME Phone · (207) 481-4944"
  },
  {
    "objectID": "contact.html#for-students",
    "href": "contact.html#for-students",
    "title": "Contact",
    "section": "For Students",
    "text": "For Students\nCurrent students: reach out via email or Brightspace. Office hours are posted each semester and announced in class.\nI’m always happy to talk about research opportunities, career questions, or course material you’re wrestling with."
  },
  {
    "objectID": "contact.html#for-collaboration",
    "href": "contact.html#for-collaboration",
    "title": "Contact",
    "section": "For Collaboration",
    "text": "For Collaboration\nI occasionally take on consulting projects focused on behavioral research to inform marketing strategy and brand development. I’m most interested in projects that:\n\nHave potential for academic publication\nFocus on sustainable agriculture, food systems, or Maine-based organizations\nInvolve interesting behavioral questions\n\nIf your project isn’t a fit for me, I’m glad to refer you to someone who might be better suited."
  },
  {
    "objectID": "contact.html#media-speaking",
    "href": "contact.html#media-speaking",
    "title": "Contact",
    "section": "Media & Speaking",
    "text": "Media & Speaking\nFor media inquiries or speaking requests, email me with details about your project or event. I’m particularly happy to discuss consumer psychology, sustainable agriculture, or the intersection of the two."
  },
  {
    "objectID": "index.html#how-i-work",
    "href": "index.html#how-i-work",
    "title": "Erin Percival Carter",
    "section": "How I Work",
    "text": "How I Work\n\n\n\n◈\n\n\nResearch\nUsing behavioral experiments to understand how consumers think about meaning, morality, authenticity, and sustainability.\n\n\n\n\n◇\n\n\nTeaching\nI primarily teach consumer behavior, sales, marketing research. Building courses on the psychology of time and moral decision-making.\n\n\n\n\n▽\n\n\nPractice\nThrough BARD, connecting small-scale agricultural producers with the business insights they need to thrive."
  },
  {
    "objectID": "index.html#navigating-risk-in-food-systems",
    "href": "index.html#navigating-risk-in-food-systems",
    "title": "Erin Percival Carter",
    "section": "Navigating Risk in Food Systems",
    "text": "Navigating Risk in Food Systems\nI’m currently investigating how Maine can communicate about PFAS contamination in ways that inform consumers without unfairly damaging perceptions of all Maine-grown food. This work sits at the intersection of risk perception, regional branding, and the psychology of trust.\nI’m also developing new courses on the psychology of time management and moral judgment in business—topics I wish someone had taught me earlier in my career.\nExplore my research →"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download as PDF · Last updated January 2025"
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Curriculum Vitae",
    "section": "Contact",
    "text": "Contact\nErin Percival Carter · Associate Professor of Marketing Maine Business School, University of Maine\n5723 Donald P. Corbett Business Building, Orono, ME 04469-5723 erin.p.carter@maine.edu · erinlpc.com Office: (207) 481-4944"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nPh.D., Marketing Leeds School of Business, University of Colorado Boulder, 2017\nBachelor of Science, Marketing University of Wyoming, 2010"
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Curriculum Vitae",
    "section": "Research Interests",
    "text": "Research Interests\nJudgment and Decision Making · Well-Being · Authenticity · Morality · Sustainability and Food Systems"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nPainful Prices: A Moral Harm Approach to Price Fairness Campbell, Margaret C., Justin Pomerance, and Erin Percival Carter Journal of Consumer Research, 2025\n\n\nReframing and Restructuring Organizational Strategies for Addressing Workplace Harassment and Discrimination in the Workplace Percival Carter, Erin, and William G. Obenauer Group & Organization Management, 2025\n\n\nConsumers’ minimum time investments in meaningful consumption Percival Carter, Erin, Lawrence E. Williams, Nicholas Light Marketing Letters, 2023 DOI →\n\n\nWhat Was Yours is (For Now) Mine: Prior User Knowledge Reduces Product Satisfaction but Can Improve Experiential Satisfaction in Access-Based Consumption Stough, Rusty and Erin Percival Carter Journal of Consumer Behavior, 2023\n\n\nPower and the Perception of Pregnancy in the Academy: Reflection, Review, and Recommendations for Institutional Change Percival Carter, Erin Gender, Work & Organization, 2023 DOI →\n\n\nDesigning and Distinguishing Meaningful Artisan Food Experiences Percival Carter, Erin, and Stephanie Welcomer Sustainability, 2021 DOI →\n\n\nBeing Funny is Not Enough: Negative Feelings Predict When Humor is Persuasive Warren, Caleb, Erin Percival Carter, and A. Peter McGraw International Journal of Advertising, 2019"
  },
  {
    "objectID": "cv.html#working-papers",
    "href": "cv.html#working-papers",
    "title": "Curriculum Vitae",
    "section": "Working Papers",
    "text": "Working Papers\n\nControl Without Consensus: Innovation Pressures in Tension with Consumer Preferences in Controlled Environment Agriculture Entsminger, Jason, Lucy McGowan, and Erin Percival Carter Preparing for Submission · Target: Science, Technology, and Human Values\n\n\nMessages that Help Consumers of Maine Food Navigate PFAS Information Percival Carter, Erin, Caroline Noblet, and Qiujie (Angie) Zheng Discussing outlets with co-authors\n\n\nGive a Little Bit: Consumers Ask More of Women Entrepreneurs Percival Carter, Erin, Jennifer Dinger, and Molly Rapert Submitting · Target: Journal of Research in Marketing and Entrepreneurship\n\n\nHow Hype Helps and Hinders Well-Being Percival Carter, Erin, Lawrence Williams, and A. Peter McGraw Preparing for submission Based on 3rd essay of dissertation research\n\n\nIn Pursuit of Imperfection: Flawed Products Reduce Process Uncertainty Percival Carter, Erin, and A. Peter McGraw Preparing for submission · Target: Food Quality and Preference\n\n\nAdapting Hackathons for Online Marketing Education Percival Carter, Erin Preparing for submission\n\n\nPrice Gouging at the Pumpkin Patch? Expense Neglect in Agritourism Leads to Perceptions of Price Unfairness Percival Carter, Erin Data collected, writing in progress\n\n\nIs Food Art? Looking at the Role of Food Entrepreneurs as Artists Percival Carter, Erin, Jason Entsminger, and Rusty Stough Writing in progress\n\n\nAgricultural Work-Based Learning Fosters Business Students’ Interest in Pursuing Agricultural Careers Welcomer, Stephanie, and Erin Percival Carter Writing in progress\n\n\nHumor Production and Perceptions of Psychological Health McGraw, A. Peter, Erin Percival Carter, and Jennifer Harman Preparing for submission Available at https://ssrn.com/abstract=2727829"
  },
  {
    "objectID": "cv.html#grants",
    "href": "cv.html#grants",
    "title": "Curriculum Vitae",
    "section": "Grants",
    "text": "Grants\n\nAwarded\nThe Agritourism Premium: Culinary Trails as an Experiential Marketing Strategy for State-Branded Farm, Fish, and Fiber Products $14,091 · 2024 · Northeast SARE · PI\nBARD (Business, Agriculture, and Rural Development) Technical Assistance Pipeline $292,000 · 2022 · US Congressionally Directed Spending (Senators Collins and King) · Co-Developer and Key Personnel\n\n\nSubmitted & Pending\nBurnout Contagion – Network Effects of Employee Burnout Spillover on Interdependent Organizations $288,127 · 2025 · NSF · Co-PI\nPFAS Risk Perception and Communication: Hypersensitivity to Language, Insensitivity to Dose $50,233 · 2025 · UMaine Research Funding Opportunity · Co-PI · Notice expected October 2025\nExpanding solar projects at the University of Maine, grazing dairy heifers at Witter Farm $49,875 · 2025 · US DOE LASSO · Co-PI · Final decisions pending\nPARTNERSHIP: Sustainable Agrofood, Consumer Response, and Venturing: Uncertainty Reduction within the context of Controlled Environment Agriculture $807,822 · 2024 · USDA, NIFA · Co-PI · Final decision pending\nOffshoreWind4Maine: 2.0 Offshore Wind Workforce Development Academy $496,918 · 2024 · State of Maine Governor’s Office · Co-PI"
  },
  {
    "objectID": "cv.html#awards-honors",
    "href": "cv.html#awards-honors",
    "title": "Curriculum Vitae",
    "section": "Awards & Honors",
    "text": "Awards & Honors\n\nMaine Business School Excellence in Research Award (2024)\nNominated for MBS Teaching Award (2022, 2023)\nSelected by Beta Gamma Sigma students for induction (2021)\nUniversity of Maine nominee for Donald Harward Award for Excellence in Service Learning (2021)\nUniversity of Maine CUGR Faculty Fellow (2020-2021)\nMaine Business School Dean’s Research Award (2019)\nQualtrics Behavioral Research Grant (2015)\nAMA Sheth Doctoral Consortium Fellow (2015)"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Curriculum Vitae",
    "section": "Teaching",
    "text": "Teaching\n\nCurrent Courses\n\nMKT 382 · Consumer Behavior\nMKT 374 · Personal Selling and Sales Management\nMKT 378 · Marketing Research"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Curriculum Vitae",
    "section": "Service",
    "text": "Service\n\nJournal Reviewing\nJournal of Marketing · Journal of Consumer Research (Trainee) · International Journal of Research in Marketing · International Journal of Advertising · British Food Journal · Humor: International Journal of Humor Research · Association for Consumer Research Conference\n\n\nProfessional Affiliations\nSociety for Agriculture and Human Values · American Marketing Association · Association for Consumer Research · International Positive Psychology Association · Society for Consumer Psychology · Society for Personality and Social Psychology\n\n\nPublic Service\n\nMaine Fibershed, Steering Committee Member\nThe Maine Food Strategy, Steering Committee Member\nOrono Town Council Ad-Hoc Committee on Diversity Equity and Inclusion, Associate Member and Data Consultant"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\nCo-Founder & Director · BARD Institute Orono, ME · Present\nDirector and Principal Behavioral Science Consultant · ELPC, LLC Orono, ME · Present\nDigital and Interactive Marketing Coordinator · University of Wyoming Laramie, WY · 2011-2012\nMarketing Manager · The Blue Sky Group Inc. Laramie, WY · 2010-2011"
  },
  {
    "objectID": "bard.html",
    "href": "bard.html",
    "title": "BARD Institute",
    "section": "",
    "text": "The BARD Institute is an independent organization I co-founded and co-direct. We work to strengthen sustainable agriculture by bringing contemporary business thinking and research to small-scale producers.\nThis work was inspired by Dr. Ayana Elizabeth Johnson’s Climate Venn Diagram—finding the intersection of what you’re good at, what the world needs, and what brings you joy. For me, that intersection sits squarely at the junction of behavioral science, business strategy, and sustainable food systems."
  },
  {
    "objectID": "bard.html#what-we-do",
    "href": "bard.html#what-we-do",
    "title": "BARD Institute",
    "section": "What We Do",
    "text": "What We Do\n\nBusiness Advising\nTailored guidance for small-scale agricultural producers navigating markets, pricing, branding, and growth—grounded in research on how consumers actually perceive and value local and artisan products.\n\n\nResearch & Analytics\nUnderstanding consumer perceptions and market opportunities through rigorous research methods. Our work helps producers make evidence-based decisions rather than relying on intuition alone.\n\n\nStudent Engagement\nDeep-dive courses and research assistantships that give business students hands-on experience in sustainable agriculture—filling a critical gap in both business education and agricultural service provision."
  },
  {
    "objectID": "bard.html#partners",
    "href": "bard.html#partners",
    "title": "BARD Institute",
    "section": "Partners",
    "text": "Partners\nWe’ve been fortunate to work with organizations committed to Maine’s agricultural future:\n\nMaine Farmland Trust\nMaine Cheese Guild\nMaine Fibershed\nMaine Fiber Frolic"
  },
  {
    "objectID": "bard.html#learn-more",
    "href": "bard.html#learn-more",
    "title": "BARD Institute",
    "section": "Learn More",
    "text": "Learn More\nVisit bardinstitute.com for current projects and ways to get involved.\nInterested in partnering with BARD or having us work with your organization? Get in touch."
  },
  {
    "objectID": "teaching/moral-judgment/index.html",
    "href": "teaching/moral-judgment/index.html",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "",
    "text": "WarningComing Spring 2026\n\n\n\nThis course is currently under development.\nThis course explores how consumers, managers, and organizations navigate ethical decisions in commercial contexts. We examine frameworks for moral reasoning and apply them to contemporary business challenges."
  },
  {
    "objectID": "teaching/moral-judgment/index.html#course-format",
    "href": "teaching/moral-judgment/index.html#course-format",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Course Format",
    "text": "Course Format\nAsynchronous online. All materials will be designed to be accessible and self-paced."
  },
  {
    "objectID": "teaching/moral-judgment/index.html#topics",
    "href": "teaching/moral-judgment/index.html#topics",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Topics",
    "text": "Topics\nPlanned topics include:\n\nFoundations of moral psychology\nConsumer responses to corporate ethics and CSR\nPricing and fairness\nPrivacy and data ethics\nEnvironmental and sustainability ethics\nWhistleblowing and organizational moral courage"
  },
  {
    "objectID": "teaching/moral-judgment/index.html#contact",
    "href": "teaching/moral-judgment/index.html#contact",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Contact",
    "text": "Contact\nInterested in this course? Get in touch with questions."
  },
  {
    "objectID": "teaching/sales-management/index.html",
    "href": "teaching/sales-management/index.html",
    "title": "Sales Management & Personal Selling",
    "section": "",
    "text": "This course develops skills in personal selling, sales force management, and relationship building. You’ll learn both the interpersonal dynamics of effective selling and the strategic considerations of managing a sales organization."
  },
  {
    "objectID": "teaching/sales-management/index.html#course-format",
    "href": "teaching/sales-management/index.html#course-format",
    "title": "Sales Management & Personal Selling",
    "section": "Course Format",
    "text": "Course Format\nThis course alternates between in-person and asynchronous online formats depending on the semester."
  },
  {
    "objectID": "teaching/sales-management/index.html#lecture-videos",
    "href": "teaching/sales-management/index.html#lecture-videos",
    "title": "Sales Management & Personal Selling",
    "section": "Lecture Videos",
    "text": "Lecture Videos\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nLecture video links will be added here."
  },
  {
    "objectID": "teaching/sales-management/index.html#readings",
    "href": "teaching/sales-management/index.html#readings",
    "title": "Sales Management & Personal Selling",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse readings will be added here."
  },
  {
    "objectID": "teaching/sales-management/index.html#assignments",
    "href": "teaching/sales-management/index.html#assignments",
    "title": "Sales Management & Personal Selling",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment materials will be added here."
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html",
    "href": "teaching/marketing-research/generate_visualizations.html",
    "title": "Teaching Evaluation Visualizations",
    "section": "",
    "text": "Code\n# Load the teaching evaluation data\nevals &lt;- read_csv(\"teaching_evaluations.csv\")\n\n# Define stop words (standard + custom)\ndata(\"stop_words\")\n\ncustom_stops &lt;- tibble(word = c(\n  # Course/university terms\n  \"class\", \"course\", \"professor\", \"semester\", \"classes\", \"courses\",\n  \"umaine\", \"mbs\", \"maine\", \"university\", \"business\", \"school\",\n  # My name variations\n\n  \"carter\", \"erin\", \"dr\", \"percival\",\n  # Common contractions/fragments\n \"i'm\", \"i've\", \"it's\", \"don't\", \"didn't\", \"wasn't\", \"wouldn't\", \"couldn't\",\n  \"i'll\", \"we're\", \"they're\", \"you're\", \"isn't\", \"aren't\", \"won't\", \"can't\",\n  # Other common but meaningless terms\n  \"lot\", \"much\", \"well\", \"really\", \"also\", \"even\", \"always\", \"never\",\n  \"feel\", \"felt\", \"thing\", \"things\", \"made\", \"make\", \"time\", \"week\"\n))\n\n# Tokenize and clean\nwords_clean &lt;- evals %&gt;%\n  unnest_tokens(word, comment_text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  anti_join(custom_stops, by = \"word\") %&gt;%\n  filter(\n    str_detect(word, \"^[a-z]+$\"),  # Only alphabetic\n    nchar(word) &gt; 2                 # At least 3 characters\n  )"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#load-and-prepare-data",
    "href": "teaching/marketing-research/generate_visualizations.html#load-and-prepare-data",
    "title": "Teaching Evaluation Visualizations",
    "section": "",
    "text": "Code\n# Load the teaching evaluation data\nevals &lt;- read_csv(\"teaching_evaluations.csv\")\n\n# Define stop words (standard + custom)\ndata(\"stop_words\")\n\ncustom_stops &lt;- tibble(word = c(\n  # Course/university terms\n  \"class\", \"course\", \"professor\", \"semester\", \"classes\", \"courses\",\n  \"umaine\", \"mbs\", \"maine\", \"university\", \"business\", \"school\",\n  # My name variations\n\n  \"carter\", \"erin\", \"dr\", \"percival\",\n  # Common contractions/fragments\n \"i'm\", \"i've\", \"it's\", \"don't\", \"didn't\", \"wasn't\", \"wouldn't\", \"couldn't\",\n  \"i'll\", \"we're\", \"they're\", \"you're\", \"isn't\", \"aren't\", \"won't\", \"can't\",\n  # Other common but meaningless terms\n  \"lot\", \"much\", \"well\", \"really\", \"also\", \"even\", \"always\", \"never\",\n  \"feel\", \"felt\", \"thing\", \"things\", \"made\", \"make\", \"time\", \"week\"\n))\n\n# Tokenize and clean\nwords_clean &lt;- evals %&gt;%\n  unnest_tokens(word, comment_text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  anti_join(custom_stops, by = \"word\") %&gt;%\n  filter(\n    str_detect(word, \"^[a-z]+$\"),  # Only alphabetic\n    nchar(word) &gt; 2                 # At least 3 characters\n  )"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#overall-word-cloud",
    "href": "teaching/marketing-research/generate_visualizations.html#overall-word-cloud",
    "title": "Teaching Evaluation Visualizations",
    "section": "Overall Word Cloud",
    "text": "Overall Word Cloud\nThis word cloud represents all teaching evaluation comments from 2017-2025.\n\n\nCode\n# Get word counts\nword_counts &lt;- words_clean %&gt;%\n  count(word, sort = TRUE)\n\n# Color palette - Maine/forest inspired\nforest_colors &lt;- c(\"#1B4332\", \"#2D6A4F\", \"#40916C\", \"#52796F\", \"#74C69D\", \"#84A98C\")\n\n# Generate word cloud\nset.seed(42)\npng(\"/home/claude/wordcloud_overall.png\", width = 1200, height = 900, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = word_counts$word,\n  freq = word_counts$n,\n  min.freq = 2,\n  max.words = 150,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = forest_colors,\n  scale = c(4, 0.5)\n)\ndev.off()\n\n# Also display in document\nwordcloud(\n  words = word_counts$word,\n  freq = word_counts$n,\n  min.freq = 2,\n  max.words = 150,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = forest_colors,\n  scale = c(4, 0.5)\n)"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#word-clouds-by-course",
    "href": "teaching/marketing-research/generate_visualizations.html#word-clouds-by-course",
    "title": "Teaching Evaluation Visualizations",
    "section": "Word Clouds by Course",
    "text": "Word Clouds by Course\n\nConsumer Behavior\n\n\nCode\ncb_words &lt;- words_clean %&gt;%\n  filter(course_name == \"Consumer Behavior\") %&gt;%\n  count(word, sort = TRUE)\n\nset.seed(42)\npng(\"/home/claude/wordcloud_consumer_behavior.png\", width = 1000, height = 700, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = cb_words$word,\n  freq = cb_words$n,\n  min.freq = 2,\n  max.words = 100,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Greens\")[2:6],\n  scale = c(3.5, 0.4)\n)\ndev.off()\n\nwordcloud(\n  words = cb_words$word,\n  freq = cb_words$n,\n  min.freq = 2,\n  max.words = 100,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Greens\")[2:6],\n  scale = c(3.5, 0.4)\n)\n\n\n\n\nPersonal Selling & Sales Management\n\n\nCode\nsales_words &lt;- words_clean %&gt;%\n  filter(course_name == \"Personal Selling & Sales Management\") %&gt;%\n  count(word, sort = TRUE)\n\nset.seed(42)\npng(\"/home/claude/wordcloud_sales.png\", width = 1000, height = 700, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = sales_words$word,\n  freq = sales_words$n,\n  min.freq = 2,\n  max.words = 80,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Blues\")[2:6],\n  scale = c(3.5, 0.4)\n)\ndev.off()\n\nwordcloud(\n  words = sales_words$word,\n  freq = sales_words$n,\n  min.freq = 2,\n  max.words = 80,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Blues\")[2:6],\n  scale = c(3.5, 0.4)\n)\n\n\n\n\nMarketing Research\n\n\nCode\nresearch_words &lt;- words_clean %&gt;%\n  filter(course_name == \"Marketing Research\") %&gt;%\n  count(word, sort = TRUE)\n\nset.seed(42)\npng(\"/home/claude/wordcloud_research.png\", width = 1000, height = 700, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = research_words$word,\n  freq = research_words$n,\n  min.freq = 1,\n  max.words = 60,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Oranges\")[2:6],\n  scale = c(3, 0.4)\n)\ndev.off()\n\nwordcloud(\n  words = research_words$word,\n  freq = research_words$n,\n  min.freq = 1,\n  max.words = 60,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = brewer.pal(6, \"Oranges\")[2:6],\n  scale = c(3, 0.4)\n)"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#word-clouds-by-modality",
    "href": "teaching/marketing-research/generate_visualizations.html#word-clouds-by-modality",
    "title": "Teaching Evaluation Visualizations",
    "section": "Word Clouds by Modality",
    "text": "Word Clouds by Modality\n\nIn-Person Courses\n\n\nCode\ninperson_words &lt;- words_clean %&gt;%\n  filter(modality == \"in-person\") %&gt;%\n  count(word, sort = TRUE)\n\nset.seed(42)\npng(\"/home/claude/wordcloud_inperson.png\", width = 1000, height = 700, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = inperson_words$word,\n  freq = inperson_words$n,\n  min.freq = 2,\n  max.words = 100,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = c(\"#1B4332\", \"#2D6A4F\", \"#40916C\", \"#52796F\", \"#74C69D\"),\n  scale = c(3.5, 0.4)\n)\ndev.off()\n\nwordcloud(\n  words = inperson_words$word,\n  freq = inperson_words$n,\n  min.freq = 2,\n  max.words = 100,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = c(\"#1B4332\", \"#2D6A4F\", \"#40916C\", \"#52796F\", \"#74C69D\"),\n  scale = c(3.5, 0.4)\n)\n\n\n\n\nOnline/Asynchronous Courses\n\n\nCode\nonline_words &lt;- words_clean %&gt;%\n  filter(modality == \"online\") %&gt;%\n  count(word, sort = TRUE)\n\nset.seed(42)\npng(\"/home/claude/wordcloud_online.png\", width = 1000, height = 700, res = 150)\npar(mar = c(0, 0, 0, 0))\nwordcloud(\n  words = online_words$word,\n  freq = online_words$n,\n  min.freq = 2,\n  max.words = 80,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = c(\"#4A5568\", \"#2D3748\", \"#1A202C\", \"#718096\", \"#A0AEC0\"),\n  scale = c(3.5, 0.4)\n)\ndev.off()\n\nwordcloud(\n  words = online_words$word,\n  freq = online_words$n,\n  min.freq = 2,\n  max.words = 80,\n  random.order = FALSE,\n  rot.per = 0.15,\n  colors = c(\"#4A5568\", \"#2D3748\", \"#1A202C\", \"#718096\", \"#A0AEC0\"),\n  scale = c(3.5, 0.4)\n)"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#sentiment-analysis",
    "href": "teaching/marketing-research/generate_visualizations.html#sentiment-analysis",
    "title": "Teaching Evaluation Visualizations",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\nCode\n# Load bing sentiment lexicon\nbing &lt;- get_sentiments(\"bing\")\n\n# Join with words\nword_sentiments &lt;- words_clean %&gt;%\n  inner_join(bing, by = \"word\")\n\n\n\nSentiment by Course\n\n\nCode\nsentiment_by_course &lt;- word_sentiments %&gt;%\n  count(course_name, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    total = positive + negative,\n    pct_positive = positive / total\n  )\n\n# Create stacked bar chart\nsentiment_plot_course &lt;- word_sentiments %&gt;%\n  count(course_name, sentiment) %&gt;%\n  ggplot(aes(x = reorder(course_name, -n), y = n, fill = sentiment)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(\n    values = c(\"negative\" = \"#C85A3B\", \"positive\" = \"#2D6A4F\"),\n    labels = c(\"Negative\", \"Positive\")\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_flip() +\n  labs(\n    title = \"Sentiment in Teaching Evaluations by Course\",\n    subtitle = \"Proportion of positive vs. negative sentiment-bearing words\",\n    x = NULL,\n    y = NULL,\n    fill = \"Sentiment\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\"\n  )\n\nggsave(\"/sentiment_by_course.png\", sentiment_plot_course, \n       width = 9, height = 5, dpi = 150)\nsentiment_plot_course\n\n\n\n\nSentiment by Modality\n\n\nCode\nsentiment_by_modality &lt;- word_sentiments %&gt;%\n  count(modality, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    total = positive + negative,\n    pct_positive = positive / total\n  )\n\nsentiment_plot_modality &lt;- word_sentiments %&gt;%\n  count(modality, sentiment) %&gt;%\n  mutate(modality = ifelse(modality == \"in-person\", \"In-Person\", \"Online\")) %&gt;%\n  ggplot(aes(x = modality, y = n, fill = sentiment)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(\n    values = c(\"negative\" = \"#C85A3B\", \"positive\" = \"#2D6A4F\"),\n    labels = c(\"Negative\", \"Positive\")\n  ) +\n  scale_y_continuous(labels = percent) +\n  coord_flip() +\n  labs(\n    title = \"Sentiment by Course Modality\",\n    x = NULL,\n    y = NULL,\n    fill = \"Sentiment\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\"\n  )\n\nggsave(\"sentiment_by_modality.png\", sentiment_plot_modality, \n       width = 8, height = 4, dpi = 150)\nsentiment_plot_modality\n\n\n\n\nTop Positive and Negative Words\n\n\nCode\n# Top positive words\ntop_positive &lt;- word_sentiments %&gt;%\n  filter(sentiment == \"positive\") %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(12) %&gt;%\n  mutate(word = fct_reorder(word, n))\n\n# Top negative words\ntop_negative &lt;- word_sentiments %&gt;%\n  filter(sentiment == \"negative\") %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(12) %&gt;%\n  mutate(word = fct_reorder(word, n))\n\n# Combine plots\nlibrary(patchwork)\n\np1 &lt;- ggplot(top_positive, aes(x = n, y = word)) +\n  geom_col(fill = \"#2D6A4F\") +\n  labs(title = \"Most Common Positive Words\", x = \"Frequency\", y = NULL) +\n  theme_minimal(base_size = 11)\n\np2 &lt;- ggplot(top_negative, aes(x = n, y = word)) +\n  geom_col(fill = \"#C85A3B\") +\n  labs(title = \"Most Common Negative Words\", x = \"Frequency\", y = NULL) +\n  theme_minimal(base_size = 11)\n\ncombined_sentiment &lt;- p1 + p2\n\nggsave(\"top_sentiment_words.png\", combined_sentiment, \n       width = 10, height = 5, dpi = 150)\ncombined_sentiment"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#summary-statistics",
    "href": "teaching/marketing-research/generate_visualizations.html#summary-statistics",
    "title": "Teaching Evaluation Visualizations",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\n# Overall stats\ncat(\"Total comments:\", nrow(evals), \"\\n\")\ncat(\"Total words (after cleaning):\", nrow(words_clean), \"\\n\")\ncat(\"Unique words:\", n_distinct(words_clean$word), \"\\n\\n\")\n\n# By course\nevals %&gt;%\n  count(course_name, name = \"n_comments\") %&gt;%\n  arrange(desc(n_comments)) %&gt;%\n  knitr::kable(col.names = c(\"Course\", \"Number of Comments\"))\n\n# By modality\nevals %&gt;%\n  count(modality, name = \"n_comments\") %&gt;%\n  knitr::kable(col.names = c(\"Modality\", \"Number of Comments\"))\n\n# Sentiment summary\ncat(\"\\nOverall Sentiment:\\n\")\nword_sentiments %&gt;%\n  count(sentiment) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  knitr::kable(col.names = c(\"Sentiment\", \"Count\", \"Percentage\"), digits = 3)"
  },
  {
    "objectID": "teaching/marketing-research/generate_visualizations.html#files-generated",
    "href": "teaching/marketing-research/generate_visualizations.html#files-generated",
    "title": "Teaching Evaluation Visualizations",
    "section": "Files Generated",
    "text": "Files Generated\nThe following image files have been saved:\n\nwordcloud_overall.png - All courses combined\nwordcloud_consumer_behavior.png - Consumer Behavior only\nwordcloud_sales.png - Personal Selling & Sales Management only\nwordcloud_research.png - Marketing Research only\nwordcloud_inperson.png - In-person courses only\nwordcloud_online.png - Online courses only\nsentiment_by_course.png - Sentiment breakdown by course\nsentiment_by_modality.png - Sentiment breakdown by modality\ntop_sentiment_words.png - Most common positive/negative words\n\nCopy these to your website’s images folder and reference them in your teaching index page."
  },
  {
    "objectID": "teaching/marketing-research/shinyapps/BagelRunPredictor.html",
    "href": "teaching/marketing-research/shinyapps/BagelRunPredictor.html",
    "title": "Bagel Run Predictor App",
    "section": "",
    "text": "TipAlternative Access\n\n\n\nIf the embedded app below doesn’t display correctly in your browser, you can open it directly in a new tab."
  },
  {
    "objectID": "teaching/marketing-research/shinyapps/CentralTendencyVisualizer.html",
    "href": "teaching/marketing-research/shinyapps/CentralTendencyVisualizer.html",
    "title": "Central Tendency and CLT Explorer",
    "section": "",
    "text": "TipAlternative Access\n\n\n\nIf the embedded app below doesn’t display correctly in your browser, you can open it directly in a new tab."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html",
    "href": "teaching/marketing-research/assignments/week5-exercise.html",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "",
    "text": "Before we get going, here’s a quick reference:\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nModel C (compact)\nIn this exercise/chapter, predicts a constant C for all observations\n\n\nModel A (augmented)\nIn this exercise/chapter, predicts the sample mean ȳ for all observations\n\n\nSSE\nSum of Squared Errors\n\n\nSSR\nSSE(C) - SSE(A) — Sum of Squares Reduced by moving to A from C\n\n\nPRE\nSSR / SSE(C) — Proportional Reduction in Error\n\n\ndf1\np_A - p_C = 1 - 0 = 1 — degrees of freedom for numerator\n\n\ndf2\nn - p_A = n - 1 — degrees of freedom for denominator\n\n\nMSR\nSSR / df1\n\n\nMSE\nSSE(A) / df2\n\n\nF\nMSR / MSE\n\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#notation-reminders",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#notation-reminders",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "",
    "text": "Before we get going, here’s a quick reference:\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nModel C (compact)\nIn this exercise/chapter, predicts a constant C for all observations\n\n\nModel A (augmented)\nIn this exercise/chapter, predicts the sample mean ȳ for all observations\n\n\nSSE\nSum of Squared Errors\n\n\nSSR\nSSE(C) - SSE(A) — Sum of Squares Reduced by moving to A from C\n\n\nPRE\nSSR / SSE(C) — Proportional Reduction in Error\n\n\ndf1\np_A - p_C = 1 - 0 = 1 — degrees of freedom for numerator\n\n\ndf2\nn - p_A = n - 1 — degrees of freedom for denominator\n\n\nMSR\nSSR / df1\n\n\nMSE\nSSE(A) / df2\n\n\nF\nMSR / MSE\n\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#part-0-load-the-dataset",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#part-0-load-the-dataset",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "Part 0 — Load the Dataset",
    "text": "Part 0 — Load the Dataset\nWe’re still rocking good ol’ stout_festival.csv so load it up.\n\nfestival &lt;- read.csv(\"Datasets/stout_festival.csv\")\n\n# Quick peeks \nhead(festival)\nstr(festival)\nsummary(festival) \n\nThere’s another command I really like when I’m loading up/re-familiarizing myself with a dataset and it’s called describe() but it lives in the psych package.\nYou may need to install it if you haven’t…uncomment the install.packages line below if library doesn’t work, run it, then try the library command again. Remember, once you’ve installed a package once, you don’t need to install it again, just call it with the library command.\n\nlibrary(psych)\n#install.packages(\"psych\")\n\n# OK, and here's the describe command I actually wanted to use:\ndescribe(festival)\n\nI just personally prefer this to the summary output."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#part-1-pre-refresher-with-mean-only-models",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#part-1-pre-refresher-with-mean-only-models",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "Part 1 — PRE Refresher with Mean-Only Models",
    "text": "Part 1 — PRE Refresher with Mean-Only Models\nHere we compare:\n\nModel C: constant prediction C\nModel A: mean-only prediction\n\n\n# Choose a constant C to compare against the mean (feel free to change)\nC &lt;- 9.33\n\n# Compute sample mean (ȳ)\nybar &lt;- mean(festival$WTP, na.rm = TRUE)\n\n# Build the two sets of predictions\nfestival$yhat_C &lt;- C   \nfestival$yhat_A &lt;- ybar\n\nfestival$eiC &lt;- festival$WTP - festival$yhat_C\nfestival$eiA &lt;- festival$WTP - festival$yhat_A\n\nhead(festival)\n\n\nCompute SSEs\n\nSSE_C &lt;- sum(festival$eiC^2)\nSSE_A &lt;- sum(festival$eiA^2)\n\nSSE_C\nSSE_A\n\n\n\nHow much did Model A reduce the error relative to Model C?\n\nSSR &lt;- SSE_C - SSE_A\nSSR\n\n# Proportional reduction in error\nPRE &lt;- SSR / SSE_C\nPRE\n\n\n\nQuick Check\nIf you go back to the line of code where you set the value for C and change C so that it is now defined as being equal to ybar and RERUN the code from that point to here, you should wind up with PRE == 0.\nTry it to prove it to yourself and think about why/what’s going on to make that happen…\nNote: because the mean minimizes SSE, SSE(A) ≤ SSE(C) for any constant C, so PRE ≥ 0 here.\n(Note, depending on how you ask for the mean of WTP/ybar, it might round the value… the values that appear from the describe() function, for example, will be rounded to 2 decimal places while in the summary() function it’s rounded to three…those values for C will give you something close to 0 but it won’t be quite there. You can get the true, not rounded value quickly by explicitly asking for the mean using the command below)\n\nmean(festival$WTP)\n\n# For the record, you can also get it from summary() and describe(), you just have to\n# specify it:\nsummary(festival, digits=10)\n\nfully.describe &lt;- describe(festival)\nfully.describe\nprint(fully.describe, digits=10)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#part-2-hand-coded-f-test",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#part-2-hand-coded-f-test",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "Part 2 — Hand-coded F Test",
    "text": "Part 2 — Hand-coded F Test\nRemember, with an F Test we’re trying to understand whether the improvement in PRE we’re seeing for the number of predictors we add when we move from Model C to Model A is THAT much better than what we would expect to see from any old predictor just by chance. So we work through/build on the following:\n\n\n\n\n\n\n\n\nStep\nFormula\nDescription\n\n\n\n\nSSR\nSSE(C) - SSE(A)\nThe improvement in squared error between models\n\n\nMSR\nSSR / df1\nThe improvement per additional predictor\n\n\nMSE\nSSE(A) / df2\nResidual MS in Model A divided by the total number of available predictors that remain\n\n\nF\nMSR / MSE\nThe final ratio we’re looking to calculate - performance we’re seeing versus average performance expected\n\n\n\n\nDegrees of Freedom\n\n# Degrees of freedom for our contrast (this corresponds to the parameters in each model):\nn   &lt;- nrow(festival) # nrow() is what it sounds like - it returns the number of rows in\n# (and thus the sample size of) the dataset (and the maximmum number of parameters)\ndf1 &lt;- 1           # this is # parameters in Model A - # parameters in Model C, (1 - 0 = 1)\ndf2 &lt;- n - 1       # n - # parameters in Model A, (50 - 1 = 49)\n\n\n\nCalculate F and p-value\nLarger F supports Model A.\n\nMSR &lt;- SSR / df1\nMSR\nMSE &lt;- SSE_A / df2\nMSE\nFval &lt;- MSR / MSE\nFval\npval &lt;- 1 - pf(Fval, df1, df2)\npval\n\ncat(\"MSR =\", round(MSR, 4), \"  MSE =\", round(MSE, 4), \"\\n\")\ncat(\"F   =\", round(Fval, 4), \"  p   =\", round(pval, 4), \"\\n\\n\")\n\n\n\nDecision Rule Examples\n\nCompare F to the α=.05 cutoff in crit_table$F_crit.\nOr compare PRE to the α=.05 PRE_crit. They agree."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#part-3-write-a-tiny-helper-function",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#part-3-write-a-tiny-helper-function",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "Part 3 — Write a Tiny Helper Function",
    "text": "Part 3 — Write a Tiny Helper Function\nLet’s wrap the logic into a small utility. This will be handy for experimenting with different constants C without retyping:\n\nsimple_f_test &lt;- function(y, C) {\n  n   &lt;- length(y)\n  ybar&lt;- mean(y, na.rm = TRUE)\n  SSE_C &lt;- sum((y - C)^2, na.rm = TRUE)\n  SSE_A &lt;- sum((y - ybar)^2, na.rm = TRUE)\n  SSR   &lt;- SSE_C - SSE_A\n  PRE   &lt;- SSR / SSE_C\n  df1 &lt;- 1\n  df2 &lt;- n - 1\n  MSR &lt;- SSR / df1\n  MSE &lt;- SSE_A / df2\n  Fval&lt;- MSR / MSE\n  p   &lt;- 1 - pf(Fval, df1, df2)\n  c(PRE = PRE, F = Fval, p = p, df1 = df1, df2 = df2, ybar = ybar,\n    SSE_C = SSE_C, SSE_A = SSE_A, SSR = SSR, MSR = MSR, MSE = MSE)\n}\n\n# Try it out with C = 9.5 and 9.33 (or any other constant you want to test)\nsimple_f_test(festival$WTP, 9.5)\nsimple_f_test(festival$WTP, 9.33)\n\n# Quick sanity check: using C = ybar should give PRE = 0, F = 0, p = 1\nsimple_f_test(festival$WTP, mean(festival$WTP))"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week5-exercise.html#part-4-critical-values-for-f-and-for-pre",
    "href": "teaching/marketing-research/assignments/week5-exercise.html#part-4-critical-values-for-f-and-for-pre",
    "title": "Week 5 Exercise: PRE, Critical Values, and the F Test",
    "section": "Part 4 — Critical Values for F and for PRE",
    "text": "Part 4 — Critical Values for F and for PRE\nAs we learned from the reading this week, the thing about adding predictors is that even if Model C were “true” and Model A were not an improvement, we would still expect PRE to bounce above zero by chance…so we need to decide how big of an improvement in PRE we need to see before we feel like there’s a meaningful, not-as-likely-to-be-just-chance improvement present in Model A.\nWe use “critical values” to decide when the improvement is enough for us to take seriously and feel confident in rejecting Model C for the superior Model A.\n\nF Critical Values\n\n# F critical values for common alphas (right tail)\nF_crit_10 &lt;- qf(0.90, df1 = df1, df2 = df2)   # alpha = .10\nF_crit_07 &lt;- qf(0.93, df1 = df1, df2 = df2)   # alpha = .07 (approx)\nF_crit_05 &lt;- qf(0.95, df1 = df1, df2 = df2)   # alpha = .05\nF_crit_01 &lt;- qf(0.99, df1 = df1, df2 = df2)   # alpha = .01\nF_crit_001&lt;- qf(0.999,df1 = df1, df2 = df2)   # alpha = .001\n\ncrit_table &lt;- data.frame(\n  alpha = c(0.10, 0.07, 0.05, 0.01, 0.001),\n  F_crit = round(c(F_crit_10, F_crit_07, F_crit_05, F_crit_01, F_crit_001), 4)\n)\ncrit_table\n\n\n\nConvert F Critical to PRE Critical\nWe can also convert F criticals into PRE criticals (remember, there’s a one-to-one mapping):\nPRE = (df1 × F) / (df1 × F + df2)\nLet’s go ahead and write another function to do exactly that:\n\nPRE_from_F &lt;- function(Fval, df1, df2) (df1 * Fval) / (df1 * Fval + df2)\n\ncrit_table$PRE_crit &lt;- round(PRE_from_F(crit_table$F_crit, df1, df2), 4)\ncrit_table\n\n#...and just for fun, let's do it in reverse, too:\nF_from_pre &lt;- function(PRE, df1, df2) (PRE/df1) / ((1 - PRE)/df2)\n\n# Compare your observed PRE to PRE_crit at alpha = .05, .01, etc.\ncat(\"Observed PRE =\", round(PRE, 4), \"\\n\\n\")\n\n\n\nThis is Kind of Badass\nOK, so just to be clear because this is kind of badass for the first 5 weeks of your undergraduate level class…\nYou have written three different functions in the code above - like from scratch - that for a simple model comparison can perform a simple F test comparing an input value for Model C to that function to the Model A that uses the mean as the sole predictor as well as a function that can calculate PRE values that correspond to that F OR F from PRE.\n\n# For example, I could examine the F test comparing a Model C of 9.19:\nsimple_f_test(festival$WTP, 9.19)\n\n# with the f test I get from a Model C with an estimate of 8.88:\nsimple_f_test(festival$WTP, 8.88)\n\n# and I can generate the F from the PRE as well:\nF_from_pre(.125, 1, 49)\n\n\n\nExploring: What C Gives a 30% PRE?\nNow I can also figure out what F I need/would correspond to a 30% reduction in error:\n\nF_from_pre(.3, 1, 49)\n\n# and I could iterate with the simple_F_test function to determine what mean produces\n# that F...\nsimple_f_test(festival$WTP, 3.33)\nsimple_f_test(festival$WTP, 5.55)\nsimple_f_test(festival$WTP, 7.77)\nsimple_f_test(festival$WTP, 8.30)\nsimple_f_test(festival$WTP, 8.35)\nsimple_f_test(festival$WTP, 8.45)              \nsimple_f_test(festival$WTP, 8.59)\nsimple_f_test(festival$WTP, 8.62)\nsimple_f_test(festival$WTP, 8.65)\nsimple_f_test(festival$WTP, 8.64)\n\nWell, that’s pretty darn close! So the low estimate for Model C that corresponds to a PRE of 30% and a corresponding F of 21 (within rounding error and limited to two decimal points) is 8.64…\nWhat’s the high estimate…?"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "",
    "text": "This exercise walks through:\n\nBuilding a tiny dataset of ticket totals + item descriptions\nPicking a one-number model (b₀) to predict EVERY ticket\nCalculating, by hand, the error columns:\n\nERROR = DATA - MODEL\n\nComputing four summary measures of error:\n\nCE = count of mismatches (who isn’t exactly b₀?)\nSE = sum of residuals (signed) — can cancel!\nSAE = sum of absolute errors\nSSE = sum of squared errors\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#overview-and-goals",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#overview-and-goals",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "",
    "text": "This exercise walks through:\n\nBuilding a tiny dataset of ticket totals + item descriptions\nPicking a one-number model (b₀) to predict EVERY ticket\nCalculating, by hand, the error columns:\n\nERROR = DATA - MODEL\n\nComputing four summary measures of error:\n\nCE = count of mismatches (who isn’t exactly b₀?)\nSE = sum of residuals (signed) — can cancel!\nSAE = sum of absolute errors\nSSE = sum of squared errors\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-0-build-the-dataset",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-0-build-the-dataset",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 0 — Build the Dataset",
    "text": "Part 0 — Build the Dataset\nWe’ll hard-code a small sample dataset with six tickets. The first column will be a respondent ID, the second the ticket total, and the third column will be an order summary.\n\nID    &lt;- 1:6\nTotal &lt;- c(6, 7, 7, 8, 30, 300)  # dollars per ticket (continuous outcome)\nItem  &lt;- c(\"8 oz drip\",\n           \"12 oz drip\",\n           \"8 oz drip plus flavor\",\n           \"Pumpkin spice latte\",\n           \"Matcha latte + salad to go, breakfast bowl\",\n           \"Catering order - full coffee bar\")\n\ncoffee &lt;- data.frame(ID, Total, Item, stringsAsFactors = FALSE)\n\n# Peek:\ncoffee"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-1-mode",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-1-mode",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 1 — Mode",
    "text": "Part 1 — Mode\nNow we need to go through and create a new variable to capture what kind of drink (if any) appears on the ticket. There’s no right or wrong answer as far as EXACTLY what the catergories could be here (for example, depending on what all is included in the “coffee bar” catering order, that might be just drip coffee but a lot of it though I’m not treating it that way). This type of recoding of data is something it’s important to make sure is well-documented and that whoever is going to use and rely on your analysis either agrees with beforehand (ideal) or at a minimum is able to decipher easily on their own.\nNote I could just use my silly human eyes and brain to go through and code each one by one but instead, I’m going to use a function in R from the grep family of commands. This is basically a series of commands that tells R how to do some pattern matching. You do need to be careful about confirming results when you use something like this but it’s pretty handy especially if we’re attempting to recode more than 7 observations. This is not a section of code I want you to get particularly bogged down in (I won’t be testing you on it) but for those of you looking to stretch some skills, you might have fun playing around with it a bit. For what it’s worth, typically my advice would be to do your best to design your survey or other data collection instrument so that all of this coding happens at the time of data collection so that you can avoid needing to recode as much as possible.\nThat said, we’ll classify each Item into a drink category using base R grepl().\nI’m doing this to provide a simple categorical lens for us to estimate “mode”.\n\nDrinkCategory &lt;- ifelse(grepl(\"drip\",      Item, ignore.case = TRUE), \"Drip coffee\",\n                        ifelse(grepl(\"latte\",     Item, ignore.case = TRUE), \"Latte\",\n                               ifelse(grepl(\"catering\",  Item, ignore.case = TRUE), \"Catering\",\n                                      \"Other\")))\ncoffee$DrinkCategory &lt;- DrinkCategory\n\nNow we’re going to use the table() command to give us a count of items in each of the categories in that new variable we created (a categorical “mode” demo). Expect “Drip coffee” to be the modal category here (3 of 6).\n\ntable(coffee$DrinkCategory)\n\nNote we can also generate a similar table to try to calculate the modal ticket total.\n\ntable(coffee$Total) #You should see that the mode here is 7\n\nThe problem is that for continuous data like income or ticket total or wtp, mode isn’t necessarily THAT informative…it tells us what the most frequent amount people paid was but we have no idea based on this value if that most frequent value is near the center of our distribution, if those tickets were similar in composition of the order, or if they values just happened a lot by chance, etc.\nThat said, we’re going to go ahead and record 7 as our mode with a new column:\n\nb0_mode &lt;- 7\ncoffee$b0_mode &lt;- b0_mode\n\nNow let’s try some alternative measures of central tendency."
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-2-alternative-single-b₀-models",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-2-alternative-single-b₀-models",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 2 — Alternative Single b₀ Models",
    "text": "Part 2 — Alternative Single b₀ Models\nLet’s try each of the following:\n\nmedian → a robust “typical” ticket, insensitive to extremes\nmean → our average value, sensitive to extremes\na number you choose (e.g., 7, 8, 12) to see how the error changes\n\nNote that for median and mean, we can just use the command median() and mean():\n\nb0_median &lt;- median(coffee$Total)  # with even n, it's the average of the two middle values\nb0_mean   &lt;- mean(coffee$Total)\n\nNow we make new columns in the dataset for each measure of central tendency (remember, this is the full extent of our MODEL for these simple models):\n\nb0_median &lt;- b0_median\nb0_mean &lt;- b0_mean\n\nAnd for fun, you pick a number for yourself here. I have it set at 12 but go ahead and change it to something else.\n\nb0_your_guess &lt;- 12       # try any number you like OTHER than 7, 12, 7.5, or 59.67\n\nOK, let’s take a look at our dataset really quick and make sure it looks how we expect it to look… remember, we’re expecting to see each one of our b₀’s in a new column on the right.\n\nhead(coffee)\n\nWell that’s not correct. Why do we only see one column for mode? There should be one for median, one for mean, and one for your guess…\nCan you figure out what went wrong? Give it a shot before revealing the hint below.\n\n\n\n\n\n\nTipClick to reveal hint\n\n\n\n\n\nUgh, we didn’t specify that we were creating new variables in the coffee dataset for the median, mean, and your guess values. We calculated the values and stored them in objects, but we never added them as columns to our dataframe!\n\n\n\nOK, fixed below:\n\ncoffee$b0_median &lt;- b0_median\ncoffee$b0_mean &lt;- b0_mean\ncoffee$b0_your_guess &lt;- b0_your_guess\n\n# Now let's check again:\nhead(coffee)\n\nNICE!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-3-calculate-error-columns-by-hand",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-3-calculate-error-columns-by-hand",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 3 — Calculate Error Columns BY HAND",
    "text": "Part 3 — Calculate Error Columns BY HAND\n(Some of you probably know that we could use commands to calculate these error columns instead of coding the math by hand…me too. We’re going to do it by hand anyway because: 1. it’s not that tough, 2. it’s good practice, 3. I think it does a better job of helping you intuit what’s going on with error and that is like priority 1 in this chapter.)\n\nResiduals (signed): ERROR = DATA - MODEL\n\ncoffee$ei_mode &lt;- coffee$Total - coffee$b0_mode\ncoffee$ei_median &lt;- coffee$Total - coffee$b0_median\ncoffee$ei_mean &lt;- coffee$Total - coffee$b0_mean\ncoffee$ei_your_guess &lt;- coffee$Total - coffee$b0_your_guess\n\n# Let's take a peek at the dataset now (it's going to be looking a bit wider)\nhead(coffee)\n\nOK, so at this point we have calculated an error for EACH PREDICTION. The question is, how do we turn that into a single measure of error for the ENTIRE MODEL…?\nWe’re going to try 4 different strategies.\n\n\n\nStrategy 1: Count of Errors (CE)\nWe define ERROR as any time our MODEL prediction for any given particular DATA is not EXACTLY correct. This is the “Count of Errors” style from the book.\nSo one way that we could check this in our code is to tell R the following: For a given b₀, mark each row TRUE if the model nails it exactly (in other words, if Total == b₀), FALSE otherwise. Then count the TRUEs and FALSEs.\nRemember for continuous data like ticket totals, exact matches are rare — that’s the whole point here - we’re going to see CE kind of fall apart in terms of its usefulness for continuous data.\n\n# Row-by-row TRUE/FALSE hits for each constant model\ncoffee$hit_mode        &lt;- coffee$Total == coffee$b0_mode\ncoffee$hit_median      &lt;- coffee$Total == coffee$b0_median\ncoffee$hit_mean        &lt;- coffee$Total == coffee$b0_mean\ncoffee$hit_your_guess  &lt;- coffee$Total == coffee$b0_your_guess\n\n# Peek at what we just made (Note in the code below, I'm asking R to just show \n# me a peek of only specific columns in the dataset because we've now added a lot \n# of columns; everything we need to evaluate what's happening here and nothing \n# else will appear using this code with left side = data, right = our results for\n# each measure of central tendency and when DATA == MODEL exactly)\nhead(coffee[, c(\"ID\", \"Total\",\n                \"b0_mode\", \"b0_median\", \"b0_mean\", \"b0_your_guess\",\n                \"hit_mode\", \"hit_median\", \"hit_mean\", \"hit_your_guess\")])\n\nThis is a small dataset (and for all predictors other than MODE, we’re going to have a count of errors == our n since not a single observation in data == our other predictions) but still, what we want here is not a column of TRUEs and FALSEs, but instead a count of how many FALSEs we generate for any given measure of central tendency. So let’s make that using code below:\n\n# Summarize the TRUE/FALSE counts for each b0\ncat(\"\\n--- TRUE/FALSE counts (does Total equal b0?) ---\\n\")\ncat(\"Mode b0:\\n\");       print(table(coffee$hit_mode))\ncat(\"Median b0:\\n\");     print(table(coffee$hit_median))\ncat(\"Mean b0:\\n\");       print(table(coffee$hit_mean))\ncat(\"Your guess b0:\\n\"); print(table(coffee$hit_your_guess))\n\nIf you want one compact little table of counts, we can actually create OUR OWN FUNCTION to do that for us. We’re going to call that function count_tf:\n\ncount_tf &lt;- function(log_vec) c(\"FALSE\" = sum(!log_vec), \"TRUE\" = sum(log_vec))\nce_counts &lt;- rbind(\n  mode.      = count_tf(coffee$hit_mode),\n  median     = count_tf(coffee$hit_median),\n  mean       = count_tf(coffee$hit_mean),\n  your_guess = count_tf(coffee$hit_your_guess)\n)\n\ncat(\"\\nCompact CE-style counts (rows = model, cols = FALSE/TRUE):\\n\")\nprint(ce_counts)\n\n(Don’t get terribly bogged down in the function of it all if that’s overwhelming right now - that’s more of a fun little easter egg for anyone interested and also so that you can say that you’ve coded a new function in R by week 3 of the class which sounds pretty badass to be honest, so enjoy that)\nSo to be clear, our count of errors == 6 for all measures of central tendency except for mode, in which case the count of errors = 4.\nBut there’s another way that we could get this same count using 1 column in our dataset instead of two… any ideas…?\n\n\n\n\n\n\nTipClick to reveal hint\n\n\n\n\n\nThat’s right! We could also just count the number of eᵢ’s == 0 in any of our calculated ERROR columns! Because if our MODEL == DATA, our ERROR == 0 (because ERROR == DATA - MODEL, so if our model predicts 7 and a given observation is 7, error for that value is ERROR == 7-7 which == 0)\n\n\n\nSo let’s check that out:\n\n# Count exact hits by checking residuals equal zero\nei_equal_0_count_mode       &lt;- coffee$ei_mode == 0\nei_equal_0_count_median     &lt;- coffee$ei_median == 0\nei_equal_0_count_mean       &lt;- coffee$ei_mean == 0\nei_equal_0_count_your_guess &lt;- coffee$ei_your_guess == 0\n\n# we can just look at each of the objects that we just made (they're going to be\n# just a series of TRUEs and FALSEs for whether each value == 0)\nei_equal_0_count_mode\nei_equal_0_count_median\nei_equal_0_count_mean\nei_equal_0_count_your_guess\n\nBut what we would really rather do is get a table of values, so as before let’s summarize the TRUE/FALSE counts for each eᵢ:\n\ncat(\"\\n--- TRUE/FALSE counts (does ei == 0?) ---\\n\")\ncat(\"Mode ei:\\n\");         print(table(ei_equal_0_count_mode))\ncat(\"Median ei:\\n\");       print(table(ei_equal_0_count_median))\ncat(\"Mean ei:\\n\");         print(table(ei_equal_0_count_mean))\ncat(\"Your guess ei:\\n\");   print(table(ei_equal_0_count_your_guess))\n\nOK, and finally, the entire point of doing this two ways (using counts of times in which DATA=MODEL and ERROR==0 as hits and every other instance in our sample as misses or a count of errors) is that the two are equivalent, so let’s just really quick check to make sure that both of the strategies we tried gave us the same answers:\n\ncat(\"\\n--- Check the equivalence (hits via DATA==b0 vs ei==0) ---\\n\")\ncat(\"Mode:       \", all(coffee$hit_mode     == ei_equal_0_count_mode),     \"\\n\")\ncat(\"Median:     \", all(coffee$hit_median   == ei_equal_0_count_median),   \"\\n\")\ncat(\"Mean:       \", all(coffee$hit_mean     == ei_equal_0_count_mean),     \"\\n\")\ncat(\"Your guess: \", all(coffee$hit_your_guess == ei_equal_0_count_your_guess), \"\\n\")\n\nThat was honestly a lot of time spent on count of errors as a summary measure of total error in our model for me to now say that we will rarely use that as a summary measure of error in our model again…mode just isn’t that useful for continuous data which will be the majority of our focus for at least the next while. So, if we want to have a measure of error that gets at the idea of how close our model predictions are to our actual data (without requiring that those predictions be EXACT), what might we try…\nOne thing that might make sense, given that we have these handy error columns calculated and ready to go, is that we could just take a sum of all of our errors. This frankly sounds pretty reasonable. Those values are the difference between DATA and our MODEL (we literally calculated them by subtracting MODEL predictions from individual observations in data), so let’s just add up those differences and call it a day, yeah?\n\n\n\nStrategy 2: Sum of Errors (SE)\n\nSoE_mode &lt;- sum(coffee$ei_mode)\nSoE_median &lt;- sum(coffee$ei_median)\nSoE_mean &lt;- sum(coffee$ei_mean)\nSoE_your_guess &lt;- sum(coffee$ei_your_guess)\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSoE_mode\nSoE_median\nSoE_mean\nSoE_your_guess\n\nOK, so what the heck is this telling us…one thing should really stand out immediately, which is that we are getting a VERY, VERY small number for the SoE for our mean. Like really small. That’s a really impressive model, hardly any error at all!\nAs a reminder, this value is showing in scientific notation because it is so so small - if it wasn’t in scientific notation with that e-14 at the end it would be extremely obnoxious trying to count the zeroes between the decimal and the first non-zero number…see for yourself when I tell it to give me a version not in scientific notation below:\n\nSoE_mean_nonsci &lt;- format(SoE_mean, scientific=F)\nSoE_mean_nonsci\n\nBut wait a second, that doesn’t seem right…when we looked at the dataset earlier, I don’t remember that column having a bunch of exceptionally small error values…let’s check again to confirm:\n\ncoffee$ei_mean\n\nThose are not small errors. Well wtf.\nLet’s think about this for a second. What’s wrong with this measure…?!?!\n\n\n\n\n\n\nTipClick to reveal the answer\n\n\n\n\n\nDid you figure it out?\nThe problem is that we are summing the eᵢ’s but when our eᵢ’s are pretty evenly distributed around 0, summing is going to allow the MODEL predictions that UNDERPREDICT the data to be positive values and the MODEL predictions that OVERPREDICT the data to be negative values. We then sum those positive and negative values and can end up with something that looks like zero error because the positive and negative errors cancel out. But we don’t have 0 error. We have a lot of error.\nWe can do better than this. Suggestions…?\n\n\n\n\n\n\nStrategy 3: Sum of Absolute Errors (SAE)\nWell, one thing we can do is to just take the absolute value of each eᵢ (making each a positive value or akin to a measure of distance). Let’s try that.\nAbsolute errors (remove the ability for some errors to be negative - let’s make it more like a measure of the distance from MODEL to DATA regardless of whether we under- or overpredict with MODEL):\n\nSAE_mode &lt;- sum(abs(coffee$ei_mode))\nSAE_median &lt;- sum(abs(coffee$ei_median))\nSAE_mean &lt;- sum(abs(coffee$ei_mean))\nSAE_your_guess &lt;- sum(abs(coffee$ei_your_guess))\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSAE_mode\nSAE_median\nSAE_mean\nSAE_your_guess\n\nOK, that looks more reasonable, yeah? When we move from a sum of errors to a sum of ABSOLUTE errors, our model error estimate for the simple model using mean as our b₀ significantly changes:\n\nSoE_mean\nSAE_mean\n\nSo now that’s behaving in a much more logical way (and understandably we won’t really ever use SoE again). But let’s think about what’s going on with SAE. What is it really doing for us? It’s giving us a simple sum of the distance measures of our predictions and we get a total measure of the extent to which our predictions “miss.” It doesn’t really take into account the severity of the extent to which our MODEL misses, though.\n\n\n\nStrategy 4: Sum of Squared Errors (SSE)\nOK, so SAE gives us the linear, additive “miss.” What if we want to have much harsher penalties (greater errors, a non-linear effect) for our MODEL predictions that miss DATA to a greater extent? Well, we could make our error non-linear, then. We could inflate those errors more significantly the greater the difference between the MODEL and DATA.\nHere’s what that’s going to look like:\n\nSSE_mode &lt;- sum(coffee$ei_mode^2)\nSSE_median &lt;- sum(coffee$ei_median^2)\nSSE_mean &lt;- sum(coffee$ei_mean^2)\nSSE_your_guess &lt;- sum(coffee$ei_your_guess^2)\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSSE_mode\nSSE_median\nSSE_mean\nSSE_your_guess\n\nThose are some big old whopping errors. But honestly, you know what? As long as we are consistent in how we calculate our errors between any given Model C and Model A that we might want to compare, it doesn’t much matter what the ABSOLUTE value of the errors is; we don’t calculate ARE (absolute reduction in error) in the approach to building models we use in this class, we calculate the PRE (proportional reduction in error). So it’s all relative.\nThat said, the difference between these two numbers does tell us something:\n\nSAE_mean\nSSE_mean\n\nIt tells us that because of those non-linear penalties for MODEL mispredicting DATA by more than a little bit, we must have some predictions that are off the mark by a pretty significant amount."
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#summary-table",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#summary-table",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Summary Table",
    "text": "Summary Table\nOK, let’s try putting this all together:\n\n# Table of summary values\nerror_summary &lt;- data.frame(\n  Model = c(\"Mode (7)\", \"Median (7.5)\", \"Mean (~59.67)\", sprintf(\"Your guess (%.2f)\", b0_your_guess)),\n  CE    = c(sum(!coffee$hit_mode), sum(!coffee$hit_median), sum(!coffee$hit_mean), sum(!coffee$hit_your_guess)),\n  SoE   = c(SoE_mode, SoE_median, SoE_mean, SoE_your_guess),\n  SAE   = c(SAE_mode, SAE_median, SAE_mean, SAE_your_guess),\n  SSE   = c(SSE_mode, SSE_median, SSE_mean, SSE_your_guess)\n)\n\nerror_summary\n\nIf the scientific notation in the SoE column throws you, we can create a version without scientific notation here:\n\nerror_summary_nsdisplay &lt;- error_summary\nerror_summary_nsdisplay$SoE &lt;- format(error_summary$SoE, scientific = FALSE, trim = TRUE)\n\nerror_summary_nsdisplay"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#visualizations",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#visualizations",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Visualizations",
    "text": "Visualizations\n\n\n\n\n\n\nWarningNote on “Your guess” labels\n\n\n\nThe visualization code below uses hardcoded labels like \"Your guess (b0 = 12)\". If you changed your guess to a different value, you’ll want to update these labels in the visualization code to match your actual value!\n\n\nYou can just run all the code from here to the end (unless you’ve added lines of code as you’ve worked through the assignment).\n\nOPTIONAL - Visualization Code\nThe code below creates the visualizations from the lecture video. If you worked through the above code EXACTLY as written (with the exception of changing the value for “your_guess”, which would be good/fine), you should be able to recreate these.\n\n# Load required packages\n# If any of these library calls throws an error, you probably haven't \n# installed that package. Run install.packages(\"packagename\")\n# and then call it from library with the code below again.\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)\n\n\n\n\nCount of Errors (CE) Visualization\n\n# Long format with one row per ticket x model (add numeric x_i)\nviz_ce &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    hit = (Total == Pred),\n    x_i = as.numeric(ID),           \n    col = ifelse(hit, \"hit\", \"miss\")\n  )\n\n# Count of errors (misses) per model for annotation\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_ce$x_i)\nce_counts &lt;- viz_ce %&gt;%\n  group_by(Model) %&gt;%\n  summarize(CountErrors = sum(!hit), .groups = \"drop\") %&gt;%\n  mutate(x_pos = x_last, y_pos = y_top)\n\n# Plot (facet 4 panels)\nCEplot &lt;- ggplot(viz_ce, aes(x = x_i)) +\n  geom_segment(\n    aes(y = Pred, yend = Pred,\n        x = x_i - 0.35, xend = x_i + 0.35),\n    color = \"blue\", linewidth = 2\n  ) +\n  geom_point(aes(y = Total, color = col), size = 2.6) +\n  scale_color_manual(\n    values = c(hit = \"forestgreen\", miss = \"red3\"),\n    labels = c(hit = \"Hit (MODEL == DATA)\", miss = \"Miss\"),\n    name = NULL\n  ) +\n  geom_text(\n    data = ce_counts,\n    aes(x = x_pos, y = y_pos,\n        label = paste0(\"Count of errors: \", CountErrors)),\n    inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5\n  ) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_ce$x_i[!duplicated(viz_ce$x_i)],\n                     labels = viz_ce$ID[!duplicated(viz_ce$ID)]) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Count of Errors (CE)\",\n    subtitle = \"Dots == DATA;\\n\n      • Green dots = exact hits (ei == 0 or MODEL==DATA) \\n\n      • Red dots = misses.\\n\\nBlue planks = model predictions\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\nCEplot\n\nBut that catering order outlier is a bit obnoxious, making it hard to see. Let’s take a log of our Y axis and try again:\n\n# CE (log y)\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\nce_counts_log &lt;- ce_counts %&gt;%\n  mutate(x_pos = max(viz_ce$x_i), y_pos = ymax_log * 0.95)\n\nCElogplot &lt;- ggplot(viz_ce, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total, color = col), size = 2.6) +\n  scale_color_manual(values = c(hit = \"forestgreen\", miss = \"red3\"),\n                     labels = c(hit = \"Hit (MODEL == DATA)\", miss = \"Miss\"),\n                     name = NULL) +\n  geom_text(data = ce_counts_log,\n            aes(x = x_pos, y = y_pos, label = paste0(\"Count of errors: \", CountErrors)),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_ce$x_i[!duplicated(viz_ce$x_i)],\n                     labels = viz_ce$ID[!duplicated(viz_ce$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(title = \"Count of Errors (CE) — log scale\",\n       subtitle = \"Dots == DATA;\\n\n      • Green dots = exact hits (ei == 0 or MODEL==DATA) \\n\n      • Red dots = misses.\\n\\nBlue planks = model predictions\",\n       x = \"Ticket ID\", y = \"Total ($, log10)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\nCElogplot\n\n\n\n\nSum of Errors (SE) Visualization\n\n# Long format with residuals and numeric xi\nviz_se &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    x_i   = as.numeric(ID),\n    resid = Total - Pred,\n    # color for the residual segment: black if positive (Pred &lt; Actual), \n    # red if negative (Pred &gt; Actual)\n    seg_col = ifelse(resid &gt;= 0, \"pos\", \"neg\")\n  )\n\n# Per-panel SE annotation (sum of residuals)\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_se$x_i)\nse_annot &lt;- viz_se %&gt;%\n  group_by(Model) %&gt;%\n  summarize(SE = sum(resid), .groups = \"drop\") %&gt;%\n  mutate(\n    label = sprintf(\"SE = %.2f\", SE),\n    x_pos = x_last, y_pos = y_top\n  )\n\n# Plot (facet 4 panels)\nSoEplot &lt;- ggplot(viz_se, aes(x = x_i)) +\n  geom_segment(\n    aes(y = Pred, yend = Total, xend = x_i, color = seg_col),\n    linewidth = 0.7, alpha = 0.9\n  ) +\n  scale_color_manual(values = c(pos = \"black\", neg = \"red3\"), guide = \"none\") +\n  geom_segment(\n    aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n    color = \"blue\", linewidth = 2\n  ) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(\n    data = se_annot,\n    aes(x = x_pos, y = y_pos, label = label),\n    inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5\n  ) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(\n    breaks = viz_se$x_i[!duplicated(viz_se$x_i)],\n    labels = viz_se$ID[!duplicated(viz_se$ID)]\n  ) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Sum of Error (SE)\",\n    subtitle = \"Line lengths == ERROR\\n\n    • Black line = underpredict (MODEL &lt; DATA, +ei) \\n\n    • Red line = overpredict (MODEL &gt; DATA, -ei)\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSoEplot\n\nBut let’s take the log of our y axis again:\n\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\nse_annot_log &lt;- se_annot %&gt;%\n  mutate(x_pos = max(viz_se$x_i), y_pos = ymax_log * 0.95)\n\nSoElogplot &lt;- ggplot(viz_se, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i, color = seg_col),\n               linewidth = 0.7, alpha = 0.9) +\n  scale_color_manual(values = c(pos = \"black\", neg = \"red3\"), guide = \"none\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = se_annot_log,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_se$x_i[!duplicated(viz_se$x_i)],\n                     labels = viz_se$ID[!duplicated(viz_se$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(title = \"Sum of Error (SE) log\",\n       subtitle = \"Line lengths == ERROR\\n\n            • Black line = underpredict (MODEL &lt; DATA, +ei) \\n\n            • Red line = overpredict (MODEL &gt; DATA, -ei)\",\n                   x = \"Ticket ID\", y = \"Total ($, log10)\") +\n  theme_minimal(base_size = 12)\n\nSoElogplot\n\n\n\n\nSum of Absolute Error (SAE) Visualization\nBut that’s not great (as we well know) because it suggests we have 0 error for mean because it’s treating some errors as negative and others as positive. Let’s represent making all line lengths positive by simply making the lines all black:\n\n# Long format with abs residuals\nviz_sae &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    x_i       = as.numeric(ID),\n    resid     = Total - Pred,\n    abs_resid = abs(resid)\n  )\n\n# Per-panel SAE annotation\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_sae$x_i)\nsae_annot &lt;- viz_sae %&gt;%\n  group_by(Model) %&gt;%\n  summarize(SAE = sum(abs_resid), .groups = \"drop\") %&gt;%\n  mutate(\n    label = sprintf(\"SAE = %.2f\", SAE),\n    x_pos = x_last, y_pos = y_top\n  )\n\n# Plot (facet 4 panels)\nSAEplot &lt;- ggplot(viz_sae, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i),\n               linewidth = 0.7, alpha = 0.9, color = \"black\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = sae_annot,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(\n    breaks = viz_sae$x_i[!duplicated(viz_sae$x_i)],\n    labels = viz_sae$ID[!duplicated(viz_sae$ID)]\n  ) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Sum of Absolute Error (SAE): Sum of absolute value of line lengths\",\n    subtitle = \"\\nBlack lines ==  ERROR = DATA - MODEL \\n\n    • Blue planks = MODEL\\n\n    • Dots = DATA\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSAEplot\n\nAnd now we use a log Y axis again for visualization given our outlier:\n\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\n\nsae_annot_log &lt;- sae_annot %&gt;%\n  mutate(x_pos = x_last, y_pos = ymax_log * 0.95)\n\nSAElogplot &lt;- ggplot(viz_sae, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i),\n               linewidth = 0.7, alpha = 0.9, color = \"black\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = sae_annot_log,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_sae$x_i[!duplicated(viz_sae$x_i)],\n                     labels = viz_sae$ID[!duplicated(viz_sae$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(\n    title = \"Sum of Absolute Error (SAE) LOG: Sum of absolute value of line lengths\",\n    subtitle = \"\\nBlack lines ==  ERROR = DATA - MODEL \\n\n    • Blue planks = MODEL\\n\n    • Dots = DATA\",\n    x = \"Ticket ID\",\n    y = \"Total ($, log10)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSAElogplot\n\n\n\n\nSum of Squared Errors (SSE) Visualization\nFinally, we’re going to plot the sum of squared errors but it’s going to look a bit different because we need to represent that the eᵢ are being SQUARED to produce a nonlinear effect on our total error estimate. The best way to do that? Turn the error lines into SQUARES…literally.\n\n# Set to 1:6 to include all tickets. \n# Could change to 1:5 if you want to remove the outlier catering order and\n# see what things look like near the b0.\nids_to_use &lt;- 1:6\n\n# Long data with a b0 per panel\nviz_sse_base &lt;- coffee %&gt;%\n  filter(ID %in% ids_to_use) %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"b0\"\n  )\n\n# Compute residual, square side, and per-panel square placement\ngap &lt;- 0.6 \nviz_sse &lt;- viz_sse_base %&gt;%\n  mutate(\n    resid = Total - b0,\n    side  = abs(resid)\n  ) %&gt;%\n  group_by(Model) %&gt;%\n  arrange(Model, ID) %&gt;%\n  mutate(\n    xmin = cumsum(dplyr::lag(side + gap, default = 0)),\n    xmax = xmin + side,\n    ymin = ifelse(resid &gt;= 0, b0, b0 - side),\n    ymax = ifelse(resid &gt;= 0, b0 + side, b0),\n    xmid = (xmin + xmax)/2\n  ) %&gt;%\n  ungroup()\n\n# Baseline (b0) per panel\nbaseline_df &lt;- viz_sse %&gt;%\n  group_by(Model) %&gt;%\n  summarise(b0 = dplyr::first(b0), .groups = \"drop\")\n\n# compute global limits across all panels\nx_max &lt;- max(viz_sse$xmax)\ny_min &lt;- min(viz_sse$ymin)\ny_max &lt;- max(viz_sse$ymax)\npad_x &lt;- 0.10 * x_max\npad_y &lt;- 0.10 * (y_max - y_min)\n\n# panel SSE labels \nsse_annot &lt;- viz_sse %&gt;%\n  dplyr::group_by(Model) %&gt;%\n  dplyr::summarise(SSE = sum(side^2), .groups = \"drop\") %&gt;%\n  dplyr::mutate(\n    label = sprintf(\"SSE = %.2f\", SSE),\n    x_pos = x_max,                 # place at common right edge\n    y_pos = y_max + pad_y * 0.5    # a bit above the tallest square\n  )\n\n# Plot: Now with squares!\nSSEplot &lt;- ggplot(viz_sse) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n            fill = \"grey60\", color = \"grey35\") +\n  geom_hline(data = baseline_df, aes(yintercept = b0),\n             linetype = \"dashed\", color = \"grey30\") +\n  geom_point(aes(x = (xmin + xmax)/2, y = Total), size = 2.4, color = \"black\") +\n  geom_text(data = sse_annot, aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1) +\n  coord_fixed(ratio = 1,\n              xlim = c(0, x_max + pad_x),\n              ylim = c(y_min - pad_y, y_max + pad_y)) +\n  scale_x_continuous(breaks=NULL, labels=NULL) +\n  labs(\n    title = \"Sum of Squared Errors (SSE) \\n(they're literal squared line lengths)\\n\",\n    subtitle = \"Each side of each square = |DATA-MODEL|; \\n\n    The summed area = our measure of ERROR which is now |ERROR|^2 \\n\n    Dashed line is b0 (the model estimate)\",\n    x = \"\",\n    y = \"Total ($)\",\n  ) +\n  theme_minimal(base_size = 12)\n\nSSEplot"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#review-all-plots",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#review-all-plots",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Review All Plots",
    "text": "Review All Plots\nLet’s take a look at all the plots we just made:\n\nCEplot\n\n\nCElogplot\n\n\nSoEplot\n\n\nSoElogplot\n\n\nSAEplot\n\n\nSAElogplot\n\n\nSSEplot"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html",
    "href": "teaching/marketing-research/assignments/week6-exercise.html",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "",
    "text": "We’re still focused on only the simplest model comparisons; comparing a model with a prediction that required no “peek” at the data with a model that requires one “peek” at the data — which we will use to calculate the mean and use that as our augmented model for reasons we’ve discussed previously. So, that means throughout this exercise:\n\n\n\nTerm\nDefinition\n\n\n\n\nModel C (Compact)\nPredicts a fixed constant C (p_C = 0)\n\n\nModel A (Augmented)\nPredicts the sample mean (p_A = 1)\n\n\ndf1\np_A - p_C = 1 - 0 = 1\n\n\ndf2\nn - p_A = n - 1 = 50 - 1 = 49\n\n\n\nF(1, n-1; or 1, 49) is the reference distribution.\nThroughout this exercise, you’ll explore:\n\nPower from a “true” PRE (design stage)\nPower from an observed PRE (post-hoc)\nBias adjustment for observed PRE\nSensitivity analysis: smallest detectable PRE and Smallest Effect Size of Interest (SESOI)\nInterpreting and communicating findings (effect size and CI)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#reminders-before-we-get-started",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#reminders-before-we-get-started",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "",
    "text": "We’re still focused on only the simplest model comparisons; comparing a model with a prediction that required no “peek” at the data with a model that requires one “peek” at the data — which we will use to calculate the mean and use that as our augmented model for reasons we’ve discussed previously. So, that means throughout this exercise:\n\n\n\nTerm\nDefinition\n\n\n\n\nModel C (Compact)\nPredicts a fixed constant C (p_C = 0)\n\n\nModel A (Augmented)\nPredicts the sample mean (p_A = 1)\n\n\ndf1\np_A - p_C = 1 - 0 = 1\n\n\ndf2\nn - p_A = n - 1 = 50 - 1 = 49\n\n\n\nF(1, n-1; or 1, 49) is the reference distribution.\nThroughout this exercise, you’ll explore:\n\nPower from a “true” PRE (design stage)\nPower from an observed PRE (post-hoc)\nBias adjustment for observed PRE\nSensitivity analysis: smallest detectable PRE and Smallest Effect Size of Interest (SESOI)\nInterpreting and communicating findings (effect size and CI)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#load-the-data",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#load-the-data",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Load the Data",
    "text": "Load the Data\n\n# Load data (note, we're using our old friend stout_festival.csv)\nfestival &lt;- read.csv(\"Datasets/stout_festival.csv\")\nn &lt;- nrow(festival)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#part-1-design-power-from-a-true-pre",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#part-1-design-power-from-a-true-pre",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Part 1 — Design Power from a True PRE",
    "text": "Part 1 — Design Power from a True PRE\nThis is the “ideal” calculation you’d use when designing a study. It asks: If the true PRE were X, how often would we detect it?\nSo it’s kind of wild that “we’re just going to write a quick custom function” is just a thing we’ve been doing in this class for a while now…but we’re just going to write a quick custom function.\nBefore we write the “big” function, we’re going to write/define a couple of helper functions — you’ll see that f2_from_PRE that I define here is then referenced in the power_from_PRE_true function we define below that. Functions on functions.\n\nHelper Functions\n\nf2_from_PRE &lt;- function(PRE) PRE / (1 - PRE)\nPRE_from_f2 &lt;- function(f2)  f2 / (1 + f2)\n\n\n\nThe “Big” Function\n\npower_from_PRE_true &lt;- function(PRE_true, n, alpha = 0.05) {\n  df1 &lt;- 1\n  df2 &lt;- n - 1\n  Fcrit  &lt;- qf(1 - alpha, df1, df2)\n  f2     &lt;- f2_from_PRE(PRE_true)\n  lambda &lt;- df2 * f2\n  1 - pf(Fcrit, df1, df2, ncp = lambda)\n}\n\nOK, now with that defined, we can use our function. First let’s try an example that you’ll be able to confirm with Table 5.3 in the textbook: true PRE = .20, n = 50, alpha = .05 (though if we want alpha = .05, we don’t have to specify anything, we set alpha = .05 as a default when we wrote the function above, so it will default to that):\n\npower_from_PRE_true(0.20, n = 50, alpha = .05)\n\npower_from_PRE_true(0.20, n = 50)\n\nSo this answers the question: “If the true proportional reduction in error were 20%, how often would we detect it with n = 50 and α = .05?”\n\n\nWhat if the True PRE is Smaller?\nWhat if we want to change the question a bit? What happens if the true PRE is smaller than .2 (which is a pretty whopping PRE in behavioral research, fwiw)? Let’s make our effect size estimate a bit more conservative, and say it’s more like .1:\n\npower_from_PRE_true(0.10, n = 50)\n\nHmmm…not horrible but not what I want to see…assuming we leave the effect size at .1, what are the two things we could further alter when using this function to calculate power from a “true” PRE value to increase our power…?\n\n\n\n\n\n\nTipClick to reveal hint\n\n\n\n\n\nCome up with any ideas yet?\nRight! You could try changing the sample size (n) OR the alpha (Type I error).\nPlay around a bit with that and see what you can get to happen before moving on."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#part-2-post-hoc-power-from-observed-pre",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#part-2-post-hoc-power-from-observed-pre",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Part 2 — Post-hoc Power from Observed PRE",
    "text": "Part 2 — Post-hoc Power from Observed PRE\nOK, now we need to consider a different situation in which you might want to calculate power, and that is when you’ve already conducted a study — the ideal situation in which this would happen is when you’re planning/designing a new study and can refer to a similar enough previous study in order to determine the power of the soon-to-be study. Anyway, here we go…\nLet’s say we fit Model C = 9.44 and Model A = mean(WTP):\n\n# Observed PRE from your C vs mean model\nC     &lt;- 9.44\ny     &lt;- festival$WTP\nn     &lt;- length(y)\nybar  &lt;- mean(y)\nSSE_C &lt;- sum((y - C)^2)\nSSE_A &lt;- sum((y - ybar)^2)\nPRE_obs &lt;- (SSE_C - SSE_A) / SSE_C\n\n# Naïve post-hoc power (treating observed PRE as true)\npower_naive &lt;- power_from_PRE_true(PRE_obs, n)\n\nPost-hoc “naïve” power here is pretending observed PRE is the true one. The problem? Observed PRE tends to be too high (just like R²). So we use the simplified formula to adjust for bias given our super simple models:\nPRE_adj = 1 - (1 - PRE_obs) × (n / (n - 1))\n(Remember, Model C df = 0, Model A df = 1)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#part-3-adjusting-pre-for-bias",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#part-3-adjusting-pre-for-bias",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Part 3 — Adjusting PRE for Bias",
    "text": "Part 3 — Adjusting PRE for Bias\n\n# Bias-adjusted observed PRE (book formula), then power\nPRE_adj &lt;- 1 - (1 - PRE_obs) * (n / (n - 1))\npower_adj &lt;- power_from_PRE_true(PRE_adj, n)\n\nc(\n  PRE_observed   = round(PRE_obs, 4),\n  PRE_adjusted   = round(PRE_adj, 4),\n  power_naive    = round(power_naive, 3),\n  power_adjusted = round(power_adj, 3)\n)\n\nSo that’s basically the adjustment factor we’re going to use to correct for the fact that calculated PRE’s tend to overestimate TRUE PRE’s."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#part-4-sensitivity-analysis-and-sesoi",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#part-4-sensitivity-analysis-and-sesoi",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Part 4 — Sensitivity Analysis and SESOI",
    "text": "Part 4 — Sensitivity Analysis and SESOI\nNow let’s ask some more managerial questions…like: “Given our n and α, what is the smallest PRE we could detect with 80% power?”\nIn other words: “What’s the smallest effect size our study was sensitive to?”\n\ntarget_power &lt;- 0.80\ngrid_PRE &lt;- seq(0.01, 0.30, by = 0.001)\n\npwr_curve_PRE &lt;- sapply(grid_PRE, function(pre) power_from_PRE_true(pre, n))\nmin_PRE_detectable &lt;- min(grid_PRE[pwr_curve_PRE &gt;= target_power])\nmin_PRE_detectable\n\n\nSmallest Effect Size of Interest (SESOI)\nAlternatively, instead of just asking if Model A is different or the sensitivity of our model, we could ask whether a study is sufficiently powered to detect the smallest possible effect size that would still be different enough to matter.\nSo, let’s assume that in this context (predicting Willingness to Pay for a stout), even a 5% reduction in error (PRE = .05) would be valuable from a managerial perspective — enough to affect pricing or marketing strategy.\nThe question: Are we powered to detect that level of improvement with our current design (n = 50, α = .05)?\n\n# Step 1 — Define our SESOI (smallest meaningful effect)\nSESOI_PRE &lt;- 0.05      # \"A 5% reduction in error is meaningful for this problem\"\nalpha_now &lt;- 0.05\nn_now     &lt;- n         # our current sample size, 50\n\n# Step 2 — Estimate our power to detect that effect\npower_SESOI_now &lt;- power_from_PRE_true(SESOI_PRE, n_now, alpha_now)\npower_SESOI_now\n\nStep 3 — Interpret the output:\nRemember, this number is the probability that, if the TRUE PRE = .05, we would actually detect it (i.e., get a statistically significant F). Rule of thumb: we usually want power ≈ .80 (or 80%) or higher.\nSo if you’re seeing something like .40 or .50 here, that means there’s a 50–60% chance you’d miss a true 5% effect. That’s not “no effect”; that’s a study too small to tell.\n\n\n\nOK… So What Could We Do About That?\nThere are three main knobs we can turn:\n\nIncrease n (sample size)\nIncrease α (our tolerance for false positives)\nImprove design (reduce residual noise → increase PRE)\n\nLet’s explore all three.\n\n\n\nOption 1 — Increase Sample Size\nWe can write a short loop to see how power changes with n. (The goal: find the smallest n that gives us ~.80 power for our SESOI)\n\ntarget_power &lt;- 0.80\ngrid_n &lt;- 30:250\n\npwr_curve &lt;- sapply(grid_n, function(nn) power_from_PRE_true(SESOI_PRE, nn, alpha_now))\nmin_n_ok &lt;- min(grid_n[pwr_curve &gt;= target_power])\nmin_n_ok\n\nManagerial translation: “To have roughly 80% power to detect a 5% reduction in error, we’d need around min_n_ok observations.” That gives us an evidence-based sample size goal rather than a guess.\n\n\n\nOption 2 — Adjust Alpha (α)\nAlpha reflects how cautious we are about Type I errors. In many marketing contexts, a slightly higher α (say, .10) is reasonable for exploratory research — we’d rather risk a false alarm than miss an actionable pattern.\n\nalphas &lt;- c(.10, .07, .05, .01)\nalpha_results &lt;- sapply(alphas, function(a) power_from_PRE_true(SESOI_PRE, n_now, a))\nalpha_results\n\nTalking points:\n\n“At α = .10, power increases to X.”\n“At α = .01, power drops dramatically.”\n\nIt’s all about the tradeoff: a higher α = fewer false negatives, but more false positives.\n\n\n\nOption 3 — Improve Design (Increase Effect, Reduce Noise)\nHere we can’t change n or α in code — but we can change the data quality. Strategies include:\n\nUsing a more reliable measure of WTP (reduces residual variance)\nBlocking by obvious subgroups (e.g., heavy vs. light stout drinkers)\nEnsuring consistent experimental procedures (reduces measurement error)\nReducing heterogeneity (e.g., focus on one product line)\n\nAll of these reduce error (SSE(A)), which increases PRE and power.\nYou can demonstrate this numerically by pretending PRE_true = .08 or .10 and recalculating power — just to see the difference:\n\npower_from_PRE_true(0.08, n_now)\npower_from_PRE_true(0.10, n_now)\n\nThose small changes in effect size make a difference in power."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#part-5-effect-sizes-and-confidence-intervals",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#part-5-effect-sizes-and-confidence-intervals",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Part 5 — Effect Sizes and Confidence Intervals",
    "text": "Part 5 — Effect Sizes and Confidence Intervals\nSo far we’ve been asking prospective questions: How likely are we to detect an effect? How big does it need to be? How many participants do we need?\nBut once we’ve run a study and have results in hand, the questions shift: How big was the effect? And how confident can we be in that estimate? That’s where effect sizes and confidence intervals come in.\n\n\nStep 1 — Compute an Effect Size (Cohen’s f²)\nRemember, we can translate PRE into f², a standardized measure of effect size used across regression and ANOVA analyses (totally germane to what we’ve been doing with our simple models and stout_festival).\nf² = PRE / (1 − PRE)\n(We’ve already got a helper function for that from earlier in this exercise!)\n\nf2_obs &lt;- f2_from_PRE(PRE_obs)\nf2_adj &lt;- f2_from_PRE(PRE_adj)\n\nc(\n  Observed_f2 = round(f2_obs, 3),\n  Adjusted_f2 = round(f2_adj, 3)\n)\n\nRules of thumb (your book is — mostly rightfully — against these but they’re very commonly used/discussed in statistics and marketing research so I’m providing them here…just know these are imperfect to say the least; Cohen, 1988):\n\n\n\nEffect Size\nf²\n\n\n\n\nSmall\n≈ .02\n\n\nMedium\n≈ .15\n\n\nLarge\n≈ .35\n\n\n\nMarketing and behavioral research often lives in the small–medium range. So even something like f² = .02 might actually be meaningful in practice.\n\n\n\nStep 2 — Calculate Confidence Intervals (CIs) for PRE\nThe PRE we observe is an estimate. We can calculate a CI around it using the F distribution. This gives us a range of plausible values for the true proportional reduction in error.\n\nci_PRE &lt;- function(PRE, df1, df2, alpha = 0.05) {\n  # Compute confidence interval for PRE using F-based limits\n  F_obs &lt;- (PRE / (1 - PRE)) * (df2 / df1)\n  F_low &lt;- F_obs / qf(1 - alpha / 2, df1, df2)\n  F_high &lt;- F_obs * qf(1 - alpha / 2, df1, df2)\n  PRE_low  &lt;- F_low / (F_low + df2 / df1)\n  PRE_high &lt;- F_high / (F_high + df2 / df1)\n  c(Lower = PRE_low, Upper = PRE_high)\n}\n\nPRE_CI &lt;- ci_PRE(PRE_obs, df1 = 1, df2 = (n - 1))\nPRE_CI\n\nInterpretation: This gives a range of plausible values for the true PRE. For example, if PRE_obs = 7% with CI [1.3%, 28%], it means: “We estimate that the new model reduces error by 7%, but the true reduction could plausibly be anywhere between 1.3% and 28%.”\nThat’s much richer information than just “p &lt; .05.”\n\n\n\nStep 3 — Connect the Dots: Power, Effect Size, and CI\nLet’s put this all together conceptually.\n\nEffect size (PRE, f²): tells us how big the improvement is.\nPower: tells us how likely we were to detect it if it exists.\nConfidence interval: tells us how precisely we can estimate it.\n\nIn practice:\n\nHigh power + narrow CI = strong, credible finding.\nLow power + wide CI = weak evidence; result might be noise.\nModerate power + medium CI = “suggestive but uncertain” result."
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#try-it-yourself",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#try-it-yourself",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "Try It Yourself",
    "text": "Try It Yourself\nNow it’s your turn to explore. Try answering the following questions by modifying the code above:\nQuestion 1: What happens to the CI width if you increase n to 100?\n\n\n\n\n\n\nTipClick to reveal solution\n\n\n\n\n\n\n# With n = 100, df2 changes to 99\nci_PRE(PRE_obs, df1 = 1, df2 = 99)\n\nYou should see a narrower confidence interval — more data = more precision in our estimate.\n\n\n\nQuestion 2: Calculate the CI for C = 10 instead of 9.44. How does it change?\n\n\n\n\n\n\nTipClick to reveal solution\n\n\n\n\n\n\n# First, recalculate PRE with C = 10\nC_new &lt;- 10\nSSE_C_new &lt;- sum((y - C_new)^2)\nPRE_new &lt;- (SSE_C_new - SSE_A) / SSE_C_new\nPRE_new\n\n# Then calculate the CI\nci_PRE(PRE_new, df1 = 1, df2 = (n - 1))\n\nA different value of C will give you a different PRE (and thus different CI). Notice how the CI shifts depending on your choice of compact model.\n\n\n\nQuestion 3: What sample size would you need to get 80% power if your SESOI were PRE = .03 (a 3% reduction in error)?\n\n\n\n\n\n\nTipClick to reveal solution\n\n\n\n\n\n\nSESOI_small &lt;- 0.03\ngrid_n_large &lt;- 50:500\n\npwr_curve_small &lt;- sapply(grid_n_large, function(nn) power_from_PRE_true(SESOI_small, nn, 0.05))\nmin_n_small &lt;- min(grid_n_large[pwr_curve_small &gt;= 0.80])\nmin_n_small\n\nDetecting smaller effects requires substantially larger samples!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/week6-exercise.html#optional-bootstrapping-confidence-intervals",
    "href": "teaching/marketing-research/assignments/week6-exercise.html#optional-bootstrapping-confidence-intervals",
    "title": "Week 6 Exercise: Power, Sensitivity, SESOI, Effect Size, and CIs",
    "section": "OPTIONAL: Bootstrapping Confidence Intervals",
    "text": "OPTIONAL: Bootstrapping Confidence Intervals\n\n\n\n\n\n\nNoteThis section is entirely optional\n\n\n\nThe material below is for students who want to explore an alternative approach to calculating confidence intervals. It’s not required and won’t be tested — but if you’re curious about resampling methods (which are very cool), read on!\n\n\n\nWhat is Bootstrapping?\nSo far, we’ve calculated confidence intervals using the F distribution, which assumes our data are roughly normally distributed. But what if we’re not sure that assumption holds? Or what if we want a CI for some statistic where there’s no nice formula?\nBootstrapping is a resampling technique that lets us estimate uncertainty by repeatedly sampling from our own data. Here’s the basic idea:\n\nTake your original sample of n observations\nDraw a new sample of n observations with replacement (in other words, after being drawn each observations gets added back to the bag…so some observations get picked multiple times and others not at all)\nCalculate the statistic of interest (in our case, PRE) on this resampled data\nRepeat steps 2-3 many times (e.g., 1000 or 10000 times - you specify)\nThe distribution of those bootstrapped statistics gives you an empirical estimate of uncertainty - but like, an actual distribution, not an in theory distribution. We actually created these statistics and then actually construct the distribution.\n\nThe 2.5th and 97.5th percentiles of the bootstrapped distribution give you a 95% CI — no normality assumption required.\n\n\nQuick Demo\n\n\nFirst, Step-By-Step What Bootstrapping Actually Does\nBefore we automate anything, let’s walk through exactly what happens in a single bootstrap iteration. This will help you understand what the loop is doing.\nStep 1: Resample the data with replacement\n\nset.seed(42)  # for reproducibility\n\n# Our original sample has n observations\nn &lt;- nrow(festival)\n\n# Draw a new sample of the same size, WITH REPLACEMENT\n# This means some observations will be picked multiple times, others not at all\nboot_indices &lt;- sample(1:n, size = n, replace = TRUE)\n\n# Look at the first 20 indices we drew - notice some repeats!\nboot_indices[1:20]\n\nStep 2. Create the resampled dataset\n\n# Pull the WTP values for our resampled indices\nboot_WTP &lt;- festival$WTP[boot_indices]\n\n# Compare: original vs bootstrapped sample\nlength(festival$WTP)  # same length\nlength(boot_WTP)      # same length, but different composition\n\nStep 3. Calculate PRE on the resampled data\n\n# Calculate the mean of our bootstrapped sample\nboot_ybar &lt;- mean(boot_WTP)\nboot_ybar  # slightly different from our original ybar!\n\n# Calculate SSE for Model C (using our original constant C)\nboot_SSE_C &lt;- sum((boot_WTP - C)^2)\n\n# Calculate SSE for Model A (using the bootstrapped mean)\nboot_SSE_A &lt;- sum((boot_WTP - boot_ybar)^2)\n\n# Calculate PRE for this bootstrapped sample\nboot_PRE_single &lt;- (boot_SSE_C - boot_SSE_A) / boot_SSE_C\nboot_PRE_single  # one estimate of PRE from one resampled dataset\n\nThat’s it - we did one bootsrap iteration.\nWe drew a random resample, calculated PRE, and got a slightly different answer than our original PRE. The magic of bootstrapping is that we do this thousands of times to build up a distribution of plausible PRE values.\nNow that you’ve done it once, let’s just automate that process and have R do it a specified number of times\n\n\nNow We Automate a Bootstrap Loop\nNow that you understand what’s happening in each iteration, let’s automate the process and do it 2,000 times:\n\nset.seed(42)  # we need to do this for reproducibility - so the values I generate are the same you will generate\n\n# Number of bootstrap iterations\nn_boot &lt;- 2000\n\n# Storage for bootstrapped PRE values\nboot_PREs &lt;- numeric(n_boot)\n\n# The bootstrap loop\nfor (i in 1:n_boot) {\n  # Resample the data with replacement\n  boot_indices &lt;- sample(1:n, size = n, replace = TRUE)\n  boot_WTP &lt;- festival$WTP[boot_indices]\n  \n  # Calculate PRE on the resampled data\n  boot_ybar &lt;- mean(boot_WTP)\n  boot_SSE_C &lt;- sum((boot_WTP - C)^2)\n  boot_SSE_A &lt;- sum((boot_WTP - boot_ybar)^2)\n  boot_PREs[i] &lt;- (boot_SSE_C - boot_SSE_A) / boot_SSE_C\n}\n\n# Bootstrap 95% CI (2.5th and 97.5th percentiles)\nboot_CI &lt;- quantile(boot_PREs, probs = c(0.025, 0.975))\nboot_CI\n\n###Visualizing the Bootstrap Distribution Let’s see what we just created — a distribution of 2,000 PRE estimates, each from a different resampled version of our data:\n\n# Create a data frame for plotting\nboot_df &lt;- data.frame(PRE = boot_PREs)\n\n# Get the CI bounds for annotation\nci_lower &lt;- boot_CI[1]\nci_upper &lt;- boot_CI[2]\n\n# Plot the bootstrap distribution\nggplot(boot_df, aes(x = PRE)) +\n  # Histogram of bootstrapped PRE values\n  geom_histogram(bins = 50, fill = \"grey70\", color = \"white\", alpha = 0.8) +\n  \n  # Shade the middle 95% (the confidence interval)\n  annotate(\"rect\", \n           xmin = ci_lower, xmax = ci_upper, \n           ymin = 0, ymax = Inf, \n           alpha = 0.2, fill = \"#1B7837\") +\n  \n  # Vertical lines at the CI bounds\n  geom_vline(xintercept = ci_lower, color = \"#1B7837\", linewidth = 1.2, linetype = \"dashed\") +\n  geom_vline(xintercept = ci_upper, color = \"#1B7837\", linewidth = 1.2, linetype = \"dashed\") +\n  \n  # Vertical line at the observed PRE\n  geom_vline(xintercept = PRE_obs, color = \"#C85A3B\", linewidth = 1.2) +\n  \n  # Labels for the CI bounds\n  annotate(\"text\", x = ci_lower, y = Inf, \n           label = paste0(\"2.5%\\n\", round(ci_lower, 3)), \n           vjust = 1.5, hjust = 1.1, size = 3.5, color = \"#1B7837\", fontface = \"bold\") +\n  annotate(\"text\", x = ci_upper, y = Inf, \n           label = paste0(\"97.5%\\n\", round(ci_upper, 3)), \n           vjust = 1.5, hjust = -0.1, size = 3.5, color = \"#1B7837\", fontface = \"bold\") +\n  \n  # Label for observed PRE\n  annotate(\"text\", x = PRE_obs, y = Inf, \n           label = paste0(\"Observed PRE\\n\", round(PRE_obs, 3)), \n           vjust = 1.5, hjust = -0.1, size = 3.5, color = \"#C85A3B\", fontface = \"bold\") +\n  \n  # Labels and theme\n  labs(\n    title = \"Bootstrap Distribution of PRE (2,000 iterations)\",\n    subtitle = \"Green shaded region = 95% confidence interval\",\n    x = \"PRE\",\n    y = \"Count\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(color = \"grey40\")\n  )\n\nThe histogram shows all 2,000 PRE values we calculated from our resampled datasets. The green dashed lines mark the 2.5th and 97.5th percentiles. Everything between them is our 95% confidence interval. The orange line shows our original observed PRE.\nNotice how the distribution is roughly centered on our observed PRE, but there’s quite a bit of spread. That spread represents our uncertainty about the true PRE given the variability in our data.\n\n\nCompare the Two Approaches\n\n# F-based CI (from earlier)\nF_based_CI &lt;- ci_PRE(PRE_obs, df1 = 1, df2 = (n - 1))\n\n# Put them side by side\ncomparison &lt;- data.frame(\n  Method = c(\"F-based\", \"Bootstrap\"),\n  Lower = c(F_based_CI[1], boot_CI[1]),\n  Upper = c(F_based_CI[2], boot_CI[2])\n)\ncomparison\n\nIn many cases, the two methods give similar results. But when your data are skewed or have outliers, bootstrapping can provide a more robust estimate. It’s also useful when you want a CI for something more complex (like a ratio of two statistics) where deriving a formula would be difficult.\n\n\nWhen to Use Bootstrapping\n\nWhen you’re unsure about distributional assumptions\nWhen sample sizes are small and normality is questionable\nWhen you want a CI for a complex or non-standard statistic\nWhen you want to double-check your parametric CI\n\nThe main downside? It’s computationally more intensive (though with modern computers, 2000 iterations takes only a second or two)."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html",
    "title": "Stout Festival Exercise",
    "section": "",
    "text": "We’re going to continue working with our brewery client brief but there’s been an exciting development - NEW DATA!\nActually two new sets of data but we’re going to focus on one for now. One of the brewery managers traveled to a beer festival and collected additional data there. That data is available in the class folder for this week for you to download. Your job is to make a first pass at analyzing the data to start to figure out what’s going on here.\n\n\n\n\n\n\nImportantComplete the Coffee Shop Exercise First\n\n\n\nI recommend you complete the readings, lecture, and associated coffee shop exercise before attempting to complete this exercise. I do a lot more walking you through the logic and providing every step of the code in the coffee shop exercise and I assume you need less of that by the time you start working on this one."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#introduction",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#introduction",
    "title": "Stout Festival Exercise",
    "section": "",
    "text": "We’re going to continue working with our brewery client brief but there’s been an exciting development - NEW DATA!\nActually two new sets of data but we’re going to focus on one for now. One of the brewery managers traveled to a beer festival and collected additional data there. That data is available in the class folder for this week for you to download. Your job is to make a first pass at analyzing the data to start to figure out what’s going on here.\n\n\n\n\n\n\nImportantComplete the Coffee Shop Exercise First\n\n\n\nI recommend you complete the readings, lecture, and associated coffee shop exercise before attempting to complete this exercise. I do a lot more walking you through the logic and providing every step of the code in the coffee shop exercise and I assume you need less of that by the time you start working on this one."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#load-the-data",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#load-the-data",
    "title": "Stout Festival Exercise",
    "section": "Load the Data",
    "text": "Load the Data\nLoad the dataset (make sure the .csv lives where R can see it).\nTip: getwd() shows where R is looking. setwd(\"path/…\") changes it.\n\nfestival &lt;- read.csv(\"Datasets/stout_festival.csv\", stringsAsFactors = FALSE)\n\n# Quick peeks (these should *not* error)\nhead(festival)\nstr(festival)\nsummary(festival$WTP)   # &lt;- the star of the show for most of this exercise"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-1-calculate-measures-of-central-tendency",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-1-calculate-measures-of-central-tendency",
    "title": "Stout Festival Exercise",
    "section": "Part 1: Calculate Measures of Central Tendency",
    "text": "Part 1: Calculate Measures of Central Tendency\nFirst, you need to calculate 2 measures of central tendency for WTP.\n\nCalculate the median\n\nfestival_median &lt;- median(festival$WTP)\n\n\n\nCalculate the mean\n(You’re going to have to do this one on your own. You can do it. Promise.)\n\n# YOUR CODE HERE\n\n\n\nAdd b₀ predictions to the dataset\nNow add those values as b₀ predictions to the dataset. In other words, create new columns in the dataset for each model prediction that list that prediction for each observation in the dataset.\n\nCreate b0_median variable in the festival dataset\n(You’re going to have to do this one on your own. I believe in you.)\n\n# YOUR CODE HERE\n\n\n\nCreate b0_mean variable in teh festival dataset\n(I’m going to take a stab at this one but I can’t know for sure because it will depend on what you did when you calculated the mean above. Trying my best but you will need to check and possibly troubleshoot my work)\n\nfestival$b0_mean &lt;- festival_mean"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-2-calculate-error-measures",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-2-calculate-error-measures",
    "title": "Stout Festival Exercise",
    "section": "Part 2: Calculate Error Measures",
    "text": "Part 2: Calculate Error Measures\nSecond, you’re going to calculate three different types of error:\n\nSum of Errors\nSum of Absolute Errors\nSum of Squared Error\n\nYou’ll need to calculate each type of error for b0_median and b0_mean.\n\nError calculations for Median\nI’ll help you get started with median.\n\nIndividual eᵢ (DATA - MODEL)\nWe’ll begin by calculating our individual eᵢ - the straight DATA-MODEL version:\n\nfestival$ei_median &lt;- festival$WTP - festival$b0_median\n\n\n\nSum of Errors\nThen you’re going to need to calculate the Sum of Errors…pretty simple:\n\nSoE_median &lt;- sum(festival$ei_median)\nSoE_median\n\n\n\nSum of Absolute Errors\nNow you’re going to need to calculate the sum of absolute errors (you’re on your own here):\n\n# YOUR CODE HERE\n\n\n\nSum of Squared Errors\nAnd finally, let’s calculate the sum of squared error:\n\nSSE_median &lt;- sum((festival$ei_median)^2)\nSSE_median\n\n\n\n\n\nError calculations for Mean\nOK, now everything we just did for median, you’re going to need to do again for mean. This time, I’ll just watch. You got this.\n\nIndividual eᵢ (DATA - MODEL)\nBegin by calculating the individual eᵢ - the straight DATA-MODEL for b0_mean:\n\n# YOUR CODE HERE\n\n\n\nSum of Errors\nNow calculate the Sum of Errors:\n\n# YOUR CODE HERE\n\n\n\nSum of Absolute Errors\n…and the Sum of Absolute Errors:\n\n# YOUR CODE HERE\n\n\n\nSum of Squared Errors\n… and finally the Sum of Squared Errors:\n\n# YOUR CODE HERE"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#summary-table",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#summary-table",
    "title": "Stout Festival Exercise",
    "section": "Summary Table",
    "text": "Summary Table\nOK, we’re getting to the end.\nYou don’t have to, but something like this might be handy (though whether or not it will work as is will depend on the variable names you specified above):\n\nfestivalresults &lt;- data.frame((\n  Model = c(\"Mean\", \"Median\")),\n  SoE    = c(SoE_mean, SoE_median),\n  SAE   = c(SAE_mean, SAE_median),\n  SSE   = c(SSE_mean, SSE_median)\n)\n\nfestivalresults\n\nAnd if you got the above to work but want it without the scientific notation, you should be able to make light work of the following:\n\nfestival_results_pretty &lt;- festivalresults\nfestival_results_pretty$SoE &lt;- format(festival_results_pretty$SoE, scientific = FALSE, trim = TRUE)\nfestival_results_pretty\n\n\nNice job! You made it to the end! I have a little (OPTIONAL) treat for you that has been waiting here."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#optional-word-cloud",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#optional-word-cloud",
    "title": "Stout Festival Exercise",
    "section": "OPTIONAL: Word Cloud",
    "text": "OPTIONAL: Word Cloud\nWord cloud from the open-text “Notes” responses in our festival dataset.\n\n# If you haven't installed these packages before, uncomment them, run the\n# install lines ONCE (then # them out).\n\n# install.packages(\"tidytext\")\n# install.packages(\"wordcloud\")     \n# install.packages(\"wordcloud2\")    \n# install.packages(\"stringr\")\n# install.packages(\"dplyr\")\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(wordcloud)    \nlibrary(wordcloud2)   \n\n# 1) Grab the text column \ntxt &lt;- festival$OpenText\n\n# 2) Replace NAs with blanks; build a small tibble\ntexts &lt;- tibble(text = ifelse(is.na(txt), \"\", txt))\n\n# 3) Tokenize into words and remove common stop words, general cleanup\ntokens &lt;- texts %&gt;%\n  mutate(text = tolower(text)) %&gt;%\n  mutate(text = str_replace_all(text, \"[^a-z\\\\s]\", \" \")) %&gt;%  # keep letters + space\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"^[a-z]+$\"), nchar(word) &gt; 2) %&gt;%\n  anti_join(stop_words, by = \"word\")\n\n# 4) Count word frequencies\nword_counts &lt;- tokens %&gt;% count(word, sort = TRUE)\n\n# Quick peek at top 20 terms:\nhead(word_counts, 20)\n\n# OK, so this was a new wordcloud function to me and actually generates HTML that\n# is interactive which is wild and crazy and cool. If you hover over the different\n# words, it gives you the counts which is kind of awesome if you ask me but I \n# don't get out much.\nwordcloud2(word_counts, size = 1, minSize = 2, rotateRatio = 0.1)"
  }
]