[
  {
    "objectID": "teaching/consumer-behavior/index.html",
    "href": "teaching/consumer-behavior/index.html",
    "title": "Consumer Behavior",
    "section": "",
    "text": "This course examines the psychological, social, and cultural factors that influence consumer decision-making. We explore how consumers perceive information, form attitudes, make choices, and experience consumption—and how marketers can apply these insights ethically and effectively."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#course-format",
    "href": "teaching/consumer-behavior/index.html#course-format",
    "title": "Consumer Behavior",
    "section": "Course Format",
    "text": "Course Format\nThis course alternates between in-person and asynchronous online formats depending on the semester."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#lecture-videos",
    "href": "teaching/consumer-behavior/index.html#lecture-videos",
    "title": "Consumer Behavior",
    "section": "Lecture Videos",
    "text": "Lecture Videos\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nLecture video links will be added here."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#readings",
    "href": "teaching/consumer-behavior/index.html#readings",
    "title": "Consumer Behavior",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse readings and supplementary materials will be added here."
  },
  {
    "objectID": "teaching/consumer-behavior/index.html#assignments",
    "href": "teaching/consumer-behavior/index.html#assignments",
    "title": "Consumer Behavior",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment descriptions and materials will be added here."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html",
    "title": "Week 1: File Management Practice",
    "section": "",
    "text": "Goals for this assignment:\n\nDownload and install R and RStudio\nOpen and run an R script\nFamiliarize yourself with file management on your personal computer\nPractice a clean folder setup\nSave a CSV to a /Datasets folder\nLoad it back using a relative path in R/RStudio\n\n\n\n\n\n\n\nNoteWhat’s a relative path?\n\n\n\nA relative path tells R where to find a file relative to your current working directory—like saying “go into the Datasets folder from here.” An absolute path spells out the entire route from the root of your computer, like /Users/erin.p.carter/Documents/Marketing_Research/Datasets/file.csv. Relative paths are shorter, portable, and won’t break when you move your project folder or switch computers."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#overview",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#overview",
    "title": "Week 1: File Management Practice",
    "section": "",
    "text": "Goals for this assignment:\n\nDownload and install R and RStudio\nOpen and run an R script\nFamiliarize yourself with file management on your personal computer\nPractice a clean folder setup\nSave a CSV to a /Datasets folder\nLoad it back using a relative path in R/RStudio\n\n\n\n\n\n\n\nNoteWhat’s a relative path?\n\n\n\nA relative path tells R where to find a file relative to your current working directory—like saying “go into the Datasets folder from here.” An absolute path spells out the entire route from the root of your computer, like /Users/erin.p.carter/Documents/Marketing_Research/Datasets/file.csv. Relative paths are shorter, portable, and won’t break when you move your project folder or switch computers."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-1-set-up-your-class-folders",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-1-set-up-your-class-folders",
    "title": "Week 1: File Management Practice",
    "section": "Part 1: Set Up Your Class Folders",
    "text": "Part 1: Set Up Your Class Folders\nBefore we touch R, let’s get your computer organized. Create a main class folder somewhere you control—your Desktop or Documents folder works well.\nName it something like:\nMarketing_Research_R_Fall2025\nThe exact names don’t matter for the code to work—you could call your folder dinofartbroccolini and save your scripts as various Kings of England if you want. You just need to know where things are stored so you can find them later.\nThat said, I’d recommend a nice, safe, boring file structure. Inside your main folder, create subfolders like:\nMarketing_Research_R_Fall2025/\n├── Code/\n├── Datasets/\n├── Homework/\n└── Resources/\n\n\n\n\n\n\nWarningAvoid these locations\n\n\n\nDo NOT work from your Downloads folder. Files there get buried and accidentally deleted.\nBe careful with cloud-only locations (Google Drive, iCloud, OneDrive) unless the folder is fully synced to your local machine. Cloud sync can cause weird errors when R tries to read or write files that are still uploading."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-2-install-r-and-rstudio",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-2-install-r-and-rstudio",
    "title": "Week 1: File Management Practice",
    "section": "Part 2: Install R and RStudio",
    "text": "Part 2: Install R and RStudio\nYou only need to do this once.\n\nStep 1: Install R\nGo to https://cran.r-project.org\n\nChoose your operating system (Mac, Windows, or Linux)\nDownload and run the installer\nAccept the defaults\n\n\n\nStep 2: Install RStudio\nGo to https://posit.co/download/rstudio-desktop/\n\nDownload the free RStudio Desktop version\nRun the installer\nAccept the defaults\n\n\n\nStep 3: Open RStudio\nOpen RStudio (not R directly—RStudio is the friendlier interface that runs R for you).\nTo create a new script file:\n\nGo to File → New File → R Script\nIf that’s grayed out, use the toolbar icon (white page with a green “+”) or press Ctrl+Shift+N (Windows) / Cmd+Shift+N (Mac)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-3-create-an-rstudio-project",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-3-create-an-rstudio-project",
    "title": "Week 1: File Management Practice",
    "section": "Part 3: Create an RStudio Project",
    "text": "Part 3: Create an RStudio Project\nAn RStudio Project ties your work to a specific folder. When you open the Project, R automatically knows where your files are. This is the key to making relative paths work.\n\nIn RStudio: File → New Project\nChoose Existing Directory\nNavigate to your Marketing_Research_R_Fall2025 folder and select it\nClick Create Project\n\nNow, whenever you open this Project, your working directory will automatically be set to your class folder.\n\n\n\n\n\n\nTipHow do I open a Project later?\n\n\n\nLook for a file called Marketing_Research_R_Fall2025.Rproj in your class folder. Double-click it, and RStudio will open with everything configured."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-4-create-and-save-your-first-script",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-4-create-and-save-your-first-script",
    "title": "Week 1: File Management Practice",
    "section": "Part 4: Create and Save Your First Script",
    "text": "Part 4: Create and Save Your First Script\nNow let’s create a script file to store our code.\n\nIn RStudio: File → New File → R Script (or click the white page icon with a green “+”)\nYou should see a blank script panel in the top-left of RStudio\nSave this file to your Code folder:\n\nFile → Save As\nNavigate to Marketing_Research_R_Fall2025/Code/\nName it something like HW1_file_management_practice.R\n\n\n\nA quick orientation to RStudio\nRStudio has four main panels:\n\nTop-left (Source/Script): Where you write and edit code\nBottom-left (Console): Where R actually runs your code and shows output\nTop-right (Environment): Shows what data and variables R currently has loaded\nBottom-right (Files/Plots/Help): File browser, visualizations, and documentation\n\n\n\nHow to run code\nTo run a single line: Put your cursor on the line and press Cmd+Return (Mac) or Ctrl+Enter (Windows).\nTo run multiple lines: Highlight the lines you want and use the same keyboard shortcut, or click the Run button in the top-right of the script panel.\nTo run the entire script: Click Source or press Ctrl+Shift+Enter (Windows) / Cmd+Shift+Enter (Mac)."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-5-quick-checkwhere-are-we",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-5-quick-checkwhere-are-we",
    "title": "Week 1: File Management Practice",
    "section": "Part 5: Quick Check—Where Are We?",
    "text": "Part 5: Quick Check—Where Are We?\nLet’s verify that R knows where we are. Run this command:\n\ngetwd()\n\nWhen you run this, your console (the pane below your script) should show something like:\n[1] \"/Users/yourname/Documents/Marketing_Research_R_Fall2025\"\nThis is your working directory—the folder R considers “home base” for finding and saving files.\n\n\n\n\n\n\nImportantWhat if it’s wrong?\n\n\n\nIf the path shown is NOT your class folder, you probably didn’t open the RStudio Project. Close RStudio, find the .Rproj file in your class folder, and double-click it to open.\nAlternatively, you can set the working directory manually with setwd(), but I recommend using Projects instead—it’s more reliable."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-6-create-a-dataset",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-6-create-a-dataset",
    "title": "Week 1: File Management Practice",
    "section": "Part 6: Create a Dataset",
    "text": "Part 6: Create a Dataset\nLet’s make a small practice dataset. Imagine we’re tracking pop-up tastings for a “Maine Blueberry Beverage” across several towns. We recorded the town name, local advertising spend, foot traffic, and average satisfaction scores.\nRun the following code:\n\n# Create variables\nTown         &lt;- c(\"Orono\", \"Bangor\", \"Portland\", \"Bar Harbor\", \"Lewiston\", \"Augusta\")\nAdSpend      &lt;- c(120, 180, 350, 220, 150, 160)       # local ad dollars\nFootTraffic  &lt;- c(85, 110, 240, 130, 95, 100)         # people who stopped by\nSatisfaction &lt;- c(78, 80, 88, 85, 76, 79)             # avg satisfaction (0-100)\n\n# Combine into a data frame\nmaine_marketing &lt;- data.frame(\n  Town, AdSpend, FootTraffic, Satisfaction,\n  stringsAsFactors = FALSE\n)\n\n# Display the data\nprint(maine_marketing)\n\nWhen you run this, you should see the data print in your console:\n       Town AdSpend FootTraffic Satisfaction\n1     Orono     120          85           78\n2    Bangor     180         110           80\n3  Portland     350         240           88\n4 Bar Harbor    220         130           85\n5  Lewiston     150          95           76\n6   Augusta     160         100           79\nYou should also see maine_marketing appear in your Environment panel (top-right) as a dataset with 6 observations and 4 variables."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-7-save-the-data-to-a-csv-file",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-7-save-the-data-to-a-csv-file",
    "title": "Week 1: File Management Practice",
    "section": "Part 7: Save the Data to a CSV File",
    "text": "Part 7: Save the Data to a CSV File\nNow we’ll save this dataset to your Datasets folder using a relative path.\nBecause your working directory is already set to your class folder, you don’t need to type the full path. You can just say “go into the Datasets folder and save this file there.”\n\nwrite.csv(maine_marketing, \"Datasets/maine_marketing_sample.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nWarningDid you get an error?\n\n\n\nIf you see something like cannot open the connection or No such file or directory, I’d bet a metaphorical $1,000,000 it’s because you don’t have a folder called Datasets in your working directory.\nFix: Either create a folder called Datasets in your class folder, or change the code to match whatever folder structure you actually have.\n\n\n\nVerify the file was created\nYou can check manually by opening your Datasets folder in Finder/File Explorer. Or you can ask R to check for you:\n\nfile.exists(\"Datasets/maine_marketing_sample.csv\")\n\nIf it returns TRUE, the file is there."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-8-load-the-csv-back-into-r",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-8-load-the-csv-back-into-r",
    "title": "Week 1: File Management Practice",
    "section": "Part 8: Load the CSV Back into R",
    "text": "Part 8: Load the CSV Back into R\nNow let’s practice loading data from a file—something you’ll do constantly in this class.\nRun this code:\n\nloaded &lt;- read.csv(\"Datasets/maine_marketing.csv\")\n\n\n\n\n\n\n\nCautionDid that work? (Click to expand)\n\n\n\n\n\nIf you’ve been following exactly as written above, that should NOT have worked. You should have received an error message in your console.\nTake a moment to figure out why before reading on.\n\n\n\n\n\n\n\n\n\nCautionHint (Click to expand)\n\n\n\n\n\nCheck the file name carefully. What did we name the file when we saved it?\n\n\n\n\n\n\n\n\n\nCautionThe answer (Click to expand)\n\n\n\n\n\nWe saved the file as maine_marketing_sample.csv, but we tried to load maine_marketing.csv. The names don’t match!\nI tricked you a little there—but only with the best of intentions. This is the kind of error you will make approximately 1,000 times in your coding career. Noticing small differences in file names is a skill you’ll develop.\nThe correct code is:\n\nloaded &lt;- read.csv(\"Datasets/maine_marketing_sample.csv\")\n\n\n\n\nNow run the corrected version and verify it worked by printing both datasets:\n\nprint(loaded)\nprint(maine_marketing)\n\nThey should look identical."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#part-9-verify-the-data-matches",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#part-9-verify-the-data-matches",
    "title": "Week 1: File Management Practice",
    "section": "Part 9: Verify the Data Matches",
    "text": "Part 9: Verify the Data Matches\nYou confirmed with your eyes that the datasets look the same. But eyeballing data is inefficient—especially when datasets have thousands of rows. Let’s ask R to compare them for us.\n\nsame &lt;- all.equal(maine_marketing, loaded)\n\nOK, the code ran without errors… but nothing happened. R just sits there. What did we miss?\n\n\n\n\n\n\nCautionThink about it… (Click to expand)\n\n\n\n\n\nWe told R to create something called same and store the result of all.equal() in it. But we never asked R to show us what same contains.\nR is very good at a literal, nerdy version of “Simon Says.” We have to explicitly ask it to display things.\n\n\n\nTry this:\n\nSame\n\n\n\n\n\n\n\nCautionStill not working? (Click to expand)\n\n\n\n\n\nYou should have gotten an error: object 'Same' not found.\nWell, this class is broken. The professor clearly doesn’t know what she’s doing. Here you are having to troubleshoot her broken code…\nActually, no. We defined same (lowercase), but we typed Same (capitalized). R is case-sensitive—same and Same are completely different things to R.\nTry:\n\nsame\n\nYou should see [1] TRUE, confirming the datasets are identical.\n\n\n\nIn this case, the verification saved us zero time—the data was tiny and obvious. But when your datasets have thousands of rows, being able to programmatically verify things is invaluable."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#troubleshooting-checklist",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#troubleshooting-checklist",
    "title": "Week 1: File Management Practice",
    "section": "Troubleshooting Checklist",
    "text": "Troubleshooting Checklist\nIf you see errors like cannot open the connection or no such file or directory, work through this checklist:\n\nWhat folder is R actually in? Run getwd() and verify it’s your class folder.\nDoes R see your file? Try:\n\nfile.exists(\"Datasets/maine_marketing_sample.csv\")\nlist.files(\"Datasets\")\n\nCheck for typos. File names must match exactly, including capitalization and underscores.\nAre you using forward slashes? In R, always use / in file paths, even on Windows. (I know this is hard for you PC folks. Hang in there.)\nDid you open the RStudio Project? If you just opened RStudio without opening the Project file, your working directory might be somewhere else entirely.\nIs the file open in another program? If you opened the CSV in Excel, close it and try again. Some programs lock files while they’re open."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#bonus-save-a-dated-backup",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#bonus-save-a-dated-backup",
    "title": "Week 1: File Management Practice",
    "section": "Bonus: Save a Dated Backup",
    "text": "Bonus: Save a Dated Backup\nHere’s a handy trick for keeping track of different versions of your data: automatically add today’s date to the filename.\n\n# Get today's date in YYYY-MM-DD format\ntoday &lt;- format(Sys.Date(), \"%Y-%m-%d\")\n\n# Create a filename with the date\ncsv_dated &lt;- file.path(\"Datasets\", paste0(\"maine_marketing_\", today, \".csv\"))\n\n# Save the file\nwrite.csv(maine_marketing, csv_dated, row.names = FALSE)\n\n# Check what's in your Datasets folder now\nlist.files(\"Datasets\")\n\nYou should see both your original file and a new dated version.\nTo load the dated file back:\n\ndated_maine_marketing &lt;- read.csv(csv_dated)\ndated_maine_marketing\n\nThis pattern is useful when you’re iterating on data cleaning or analysis and want to preserve earlier versions."
  },
  {
    "objectID": "teaching/marketing-research/assignments/hw1-file-management.html#submission",
    "href": "teaching/marketing-research/assignments/hw1-file-management.html#submission",
    "title": "Week 1: File Management Practice",
    "section": "Submission",
    "text": "Submission\nTo complete this assignment:\n\nMake sure you can successfully run all the code in this document\nTake a screenshot showing your RStudio with:\n\nYour script in the top-left panel\nThe maine_marketing data printed in the console\nThe Environment panel showing your loaded datasets\n\nSubmit the screenshot to Brightspace\n\n\n\n\n\n\n\nTipStuck?\n\n\n\nIf you’re having trouble, check the Resources page for links to helpful documentation, or come to office hours. File management issues are extremely common at the start—you’re not alone!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html",
    "href": "teaching/marketing-research/assignments/stout_exercise.html",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "",
    "text": "TipDownload Option\n\n\n\nIf you prefer to work directly in RStudio, you can download the R script version of this exercise."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#overview",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#overview",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Overview",
    "text": "Overview\nGoals for this exercise:\n\nLoad the stout_research.csv dataset (it’s in the folder for this week on Brightspace)\nBuild three different models of willingness to pay\nCalculate errors (DATA - MODEL)\nVisualize how model predictions compare to the actual data\n\nRemember: DATA = MODEL + ERROR\nThat means ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-1-load-the-dataset",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-1-load-the-dataset",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 1 — Load the Dataset",
    "text": "Part 1 — Load the Dataset\nFirst, let’s load our dataset directly from the .csv file. Make sure “stout_research.csv” is saved in your working directory. If you’re building from assignment 1 and working from your project file, this is likely in the class folder you established last time and you may or may not need to specify the Datasets subfolder.\nTip: use getwd() to check your current working directory and setwd() if you need to change where R is looking.\n\nstout_data &lt;- read.csv(\"Datasets/stout_research.csv\")\n\nLet’s take a quick peek at the dataset (this function shows us just the top rows of a dataset):\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-2-look-at-the-structure-of-the-data",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-2-look-at-the-structure-of-the-data",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 2 — Look at the Structure of the Data",
    "text": "Part 2 — Look at the Structure of the Data\nWe should see 4 columns:\n\nID — an ID number for each participant\nLikesStout — Yes/No\nLocalImportant — Yes/No\nWTP — willingness to pay in dollars\n\n\nstr(stout_data)\n\nYou can probably kind of tell what that function did, namely, produce a little summary of what each variable in the dataset is doing, but a nice tip in RStudio is that you always type “?” followed immediately by the function that you’re trying to run or better understand and it will pull up the documentation explaining what that particular command in R is doing in the bottom right of RStudio. Let’s give it a quick shot:\n\n?str\n\nAs you read that documentation, you’ll notice it says that str is a common alternative to summary. Maybe we should give that a try, too.\n\nsummary(stout_data)\n\nBut wait, there’s more. What if I want to ask R to summarize not the entire stout dataset, but just teh willingness to pay data? Well, that’s going to look something like this:\n\nsummary(stout_data$WTP)\n\nWhat we’re doing in that command is once again orienting R with exactly where we want it to look. The statement says “Hey, R, go ahead and do summary on this thing that exists in the dataset I have loaded named stout_data. Once you get in stout_data, you’re going to want to look for the variable named WTP. Use that and only that.”\nSo same info as before, but a way to be more specific in asking R to focus on what I specifically want more information about as opposed to just summarizing everything."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-3-model-0-the-brewmasters-intuition",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-3-model-0-the-brewmasters-intuition",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 3 — Model 0: The Brewmaster’s Intuition",
    "text": "Part 3 — Model 0: The Brewmaster’s Intuition\nOur brewmaster says: “Price it at $7 per pint.” That’s a model! It predicts 7 for everyone.\nSo here’s one way we can examine that prediction: we add a new column to the dataset literally predicting “7” for every single respondent. We’ll call that new column/variable Model_Brewmaster:\n\nstout_data$Model_Brewmaster &lt;- 7\n\nLet’s check to make sure that worked the way we expected…\n\nhead(stout_data)\n\nNow let’s calculate ERROR = DATA - MODEL\nRemember, the data in this case are the WTP data from our dataset, the model we’re examining is the Brewmaster’s suggested price of $7. In this case, we can just ask R to calculate the difference between each person’s reported amount in our dataset and that guess of 7. We’ll call that value in our dataset Error_Brewmaster:\n\nstout_data$Error_Brewmaster &lt;- stout_data$WTP - stout_data$Model_Brewmaster\n\nWhat this line of code is saying to R is: “R, I want you to go to that dataset I have loaded called stout_data and when you get there, create a NEW variable that you’re going to call Error_Brewmaster. The way that you will create Error_Brewmaster will be to subtract the value associated with the existing variable Model_Brewmaster from the value associated with the existing variable WTP for each observation in the dataset.”\n…And because I’m compulsive, I’m going to check again to make sure it did exactly that:\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-4-model-1-the-sample-mean",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-4-model-1-the-sample-mean",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 4 — Model 1: The Sample Mean",
    "text": "Part 4 — Model 1: The Sample Mean\nNext, we try a data-based model: predict the average WTP for everyone.\nHere we’re going to create a value in R that is called mean_wtp. We could call it FantasticBananas and it wouldn’t make a difference, R doesn’t care what we name things. R does care that we specify how to calculate or define that value, though, which is everything that is happening to the right of the &lt;-\nOn that side, we’re saying “R, that thing we asked you to create, here’s how you create it: run the function mean on the variable WTP that you will find in the dataset we have loaded titled stout_data”\n\nmean_wtp &lt;- mean(stout_data$WTP)\nmean_wtp   # this should come out to 7.425\n\nAdd predictions and errors:\n\nstout_data$Model_Mean &lt;- mean_wtp\nstout_data$Error_Mean &lt;- stout_data$WTP - stout_data$Model_Mean\n\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-5-model-2-split-by-stout-preference",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-5-model-2-split-by-stout-preference",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 5 — Model 2: Split by Stout Preference",
    "text": "Part 5 — Model 2: Split by Stout Preference\nNow let’s allow our model to use one variable: whether someone likes stout. We’ll calculate the average WTP separately for “Yes” and “No.”\n\nmean_yes &lt;- mean(stout_data$WTP[stout_data$LikesStout == \"Yes\"])\nmean_no  &lt;- mean(stout_data$WTP[stout_data$LikesStout == \"No\"])\n\nOK, and now let’s check to see what they were:\n\nmean_yes  # should be ~8.46\nmean_no   # should be ~5.88\n\nAdd predictions to the dataset with this line of code:\n\nstout_data$Model_Like_Stout &lt;- ifelse(stout_data$LikesStout == \"Yes\", mean_yes, mean_no)\n\nThe line above is slightly more complex than the other variables we’ve created but if you break it down piece by piece, you’ll note that it all makes sense/isn’t that complicated. We’re telling R to create another new variable in the existing dataset (stout_data) and to call that new variable Model_Like_Stout. We then have to tell R how to fill in that column and this time, there are two different options. The statement we make there is saying IF the response on that row to the existing LikesStout item is “Yes”, then R should put the value we calculate for the mean of the yes respondents for that new variable. In all other cases, we’re telling R to go ahead and enter the mean of the no respondents for that new variable.\nAnd calculate the errors again:\n\nstout_data$Error_Like_Stout &lt;- stout_data$WTP - stout_data$Model_Like_Stout\n\n\nhead(stout_data)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-6-compare-models-visually",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-6-compare-models-visually",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 6 — Compare Models Visually",
    "text": "Part 6 — Compare Models Visually\nFor thsi section, here in just a second I am going to ask you to run some code as a block all at once and not worry too much about exactly what’s happening — the point for now is to create the figure to look at what’s going on with the performance of our models (as captured by the differences in ERROR) and not so much to try to learn the ins and outs of the ggplot code creating the figure (plenty of time for that later).\nThat said, our goals for this plot:\n\nShow actual WTP for each customer (points).\nShow each model’s prediction for each customer.\nMake it crystal clear that Brewmaster and Sample Mean are static estimates (we make the same prediction for everyone), whereas the Likes Stout model changes by respondent (we have two possible estimates to choose from for any given observation).\n\n\nInstalling and Loading Packages\nIn order to create this figure, you’re going to need to use a couple of commands that don’t come with stock R…but don’t worry, it’s not a DLC situation where each additional command is going to cost you $20, it’s just a DLC in the sense that you will need to download (or install) a couple of packages in R so that it knows what you mean when you use these new commands.\nSo the first time you run this, if you haven’t installed these packages previously, you will need to run the following install code:\n\n##### AFTER YOU RUN THE FOLLOWING THREE LINES ONE TIME, COMMENT THEM OUT BY ADDING\n##### A \"#\" IN FRONT OF EACH OF THE THREE LINES BELOW. YOU DON'T NEED TO INSTALL THE\n##### PACKAGES AGAIN\n#\n##### TO COMMENT OUT A LINE, JUST PUT A \"#\" AT THE VERY START OF THE LINE.\n##### FOR EXAMPLE, THE LINE BELOW IS COMMENTED OUT:\n##### # install.packages(\"ggplot2\")\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\n\nOK, but then once the packages are installed you will need to make sure that they’re loaded up and ready to go from the things in your library, so ALWAYS run the following three:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\nCreating the Visualization\nNow for the rest of this section, you can just go ahead and highlight the whole chunk of code and run it.\nTo be frank, I had to have chatgpt help me troubleshoot and write the very first section with the ID vars to get the figure to look the way I want it to look. Visualizations are important but coding them is, frankly, not my strong point and sometimes I need help.\n\n# Create a numeric x position for each row and keep the ID labels for display\nstout_data &lt;- stout_data |&gt;\n  arrange(ID) |&gt;\n  mutate(x_i = row_number())\n\n# Gather predictions to long format for faceting\nviz_long &lt;- stout_data |&gt;\n  select(ID, x_i, WTP, LikesStout, Model_Brewmaster, Model_Mean, Model_Like_Stout) |&gt;\n  pivot_longer(\n    cols = c(Model_Brewmaster, Model_Mean, Model_Like_Stout),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) |&gt;\n  mutate(\n    Model = factor(\n      Model,\n      levels = c(\"Model_Brewmaster\", \"Model_Mean\", \"Model_Like_Stout\"),\n      labels = c(\"Brewmaster ($7 flat)\", \n                 \"Sample Mean (~$7.43 flat)\", \n                 \"Likes Stout (two-level)\")\n    )\n  )\n\n\n# Create figure visualizing the residuals (ERROR = DATA – MODEL)\nggplot(viz_long, aes(x = x_i)) + \n  # thin vertical line shows leftover (residual): from prediction up to actual\n  geom_segment(aes(y = Pred, yend = WTP, xend = x_i),\n               linewidth = 0.6, alpha = 0.7, color = \"red\") +\n  # prediction planks\n  geom_segment(aes(y = Pred, yend = Pred,\n                   x = x_i - 0.3, xend = x_i + 0.3),\n               linewidth = 2, color = \"blue\") +\n  # actual dots\n  geom_point(aes(y = WTP), size = 2, color = \"black\") +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = stout_data$x_i, labels = stout_data$ID) +\n  labs(title = \"ERROR = DATA – MODEL (visualizing the ERROR or the residuals)\",\n       subtitle = \"Red lines = ERROR • Blue bars = MODEL predictions • Dots = DATA on actual WTP\",\n       x = \"Respondent ID\",\n       y = \"WTP ($)\") +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-7-review",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-7-review",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 7 — Review",
    "text": "Part 7 — Review\nWhat did we see?\n\nModel 0 (Brewmaster’s $7): simple, but lots of leftover error.\nModel 1 (Sample Mean): closer, error shrinks.\nModel 2 (Likes Stout): explains much more, error shrinks further.\n\nThis is DATA = MODEL + ERROR (and conversely ERROR = DATA - MODEL) in action."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout_exercise.html#part-8-proportional-reduction-in-error-pre",
    "href": "teaching/marketing-research/assignments/stout_exercise.html#part-8-proportional-reduction-in-error-pre",
    "title": "MKT 378 — Stout Pricing Example",
    "section": "Part 8 — Proportional Reduction in Error (PRE)",
    "text": "Part 8 — Proportional Reduction in Error (PRE)\nPRE = (Error_baseline - Error_new) / Error_baseline\nWe’ll use SSE (sum of squared errors) as our measure of error size here. Note there are alternative ways to measure/conceptualize error but we’re going to just square it for now (we’re squaring it to ensure all positive numbers so that we know the extent to which our predictions “missed” when we sum the “misses” for each observation).\nThe code below will calculate SSE for each model:\n\nSSE_brewmaster &lt;- sum((stout_data$WTP - stout_data$Model_Brewmaster)^2)\nSSE_mean       &lt;- sum((stout_data$WTP - stout_data$Model_Mean)^2)\nSSE_likes_stout      &lt;- sum((stout_data$WTP - stout_data$Model_Like_Stout)^2)\n\nLet’s take a look at those values now:\n\nSSE_brewmaster\nSSE_mean\nSSE_likes_stout\n\n\nComparing Models with PRE\nOK, now we want to compare some of our models using the PRE formula.\nAs you look at these results, I want you to think about what we’re really doing in the stout model compared to the others in marketing terms… instead of just having the same strategy for understanding/predicting an entire market, we’re coming up with different strategies for understanding and predicting different segments of the market…which is a lot like… SEGMENTATION! That’s right! A very rudimentary version of it, but look how well it performs compared to the others…\nLet’s give it a go:\nPRE: Sample Mean vs. Brewmaster\n\nPRE_mean_vs_brew &lt;- (SSE_brewmaster - SSE_mean) / SSE_brewmaster\nPRE_mean_vs_brew   # proportion of error reduced\n\nPRE: Likes Stout vs. Sample Mean\n\nPRE_likesstout_vs_mean &lt;- (SSE_mean - SSE_likes_stout) / SSE_mean\nPRE_likesstout_vs_mean\n\nPRE: Likes Stout vs. Brewmaster (direct comparison)\n\nPRE_likesstout_vs_brew &lt;- (SSE_brewmaster - SSE_likes_stout) / SSE_brewmaster\nPRE_likesstout_vs_brew"
  },
  {
    "objectID": "teaching/marketing-research/index.html",
    "href": "teaching/marketing-research/index.html",
    "title": "Marketing Research & Analytics",
    "section": "",
    "text": "This course provides hands-on training in research design, data collection, and analysis using R. You’ll learn to translate quantitative findings into actionable marketing insights—and build skills that transfer to any data-driven role."
  },
  {
    "objectID": "teaching/marketing-research/index.html#interactive-tools",
    "href": "teaching/marketing-research/index.html#interactive-tools",
    "title": "Marketing Research & Analytics",
    "section": "Interactive Tools",
    "text": "Interactive Tools\nThese Shiny applications help you explore statistical concepts visually. Play with them to build intuition before diving into the math.\n\nWeek 2: Bagel Run Predictor — Our first attempts at some basic model comparisons…but make it about bagels…\n\n\n\n\n\n\n\nMore Coming Soon · Links to Shiny apps will be added as they’re migrated to the new site."
  },
  {
    "objectID": "teaching/marketing-research/index.html#coding-assignments",
    "href": "teaching/marketing-research/index.html#coding-assignments",
    "title": "Marketing Research & Analytics",
    "section": "Coding Assignments",
    "text": "Coding Assignments\nThese assignments build your R skills progressively. Each is designed to be accessible—you don’t need prior programming experience.\n\nGetting Started\n\nWeek 1: File Management Practice — Install R/RStudio, set up your folders, learn relative paths\nWeek 2: Stout Exercise — Practice loading data, intro to model comparison\nWeek 3: Coffee Errors Exercise — In this exercise you will calculate A LOT (sorry but it’s important for building understanding long term) of different types of error\nWeek 3: Stout Festival Exercise — New data from our week 2 client! Now we’ll play with different measures of central tendency and error\n\n\n\n\n\n\n\nMore coming · Additional assignments will be added throughout the semester."
  },
  {
    "objectID": "teaching/marketing-research/index.html#resources",
    "href": "teaching/marketing-research/index.html#resources",
    "title": "Marketing Research & Analytics",
    "section": "Resources",
    "text": "Resources\n\nSetup\nBefore the first assignment, you’ll need:\n\nR — Download from CRAN\nRStudio — Download from Posit\n\nI’ll walk through installation in class, but these links have everything you need.\n\n\nReferences\n\nR for Data Science — Free online textbook; excellent for beginners\nQuarto Documentation — For when you’re ready to create your own reports\nggplot2 Cheat Sheet — Keep this handy\n\n\n\nGetting Help\nStuck on an assignment? In order:\n\nRe-read the error message carefully\nGoogle the error message (seriously—this is what professionals do)\nCheck the course discussion board\nCome to office hours"
  },
  {
    "objectID": "teaching/time-management/index.html",
    "href": "teaching/time-management/index.html",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "",
    "text": "This course examines the psychological foundations of time management, moving beyond simple productivity hacks to understand why certain strategies work and when they’re most effective."
  },
  {
    "objectID": "teaching/time-management/index.html#course-format",
    "href": "teaching/time-management/index.html#course-format",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Course Format",
    "text": "Course Format\nAsynchronous online. All materials are designed to be accessible and self-paced."
  },
  {
    "objectID": "teaching/time-management/index.html#modules",
    "href": "teaching/time-management/index.html#modules",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse modules and materials are under development for the current semester."
  },
  {
    "objectID": "teaching/time-management/index.html#readings",
    "href": "teaching/time-management/index.html#readings",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nReadings will be added as the course develops."
  },
  {
    "objectID": "teaching/time-management/index.html#assignments",
    "href": "teaching/time-management/index.html#assignments",
    "title": "Time Management: The Psychology of Getting Things Done",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment materials will be added here."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "I teach courses that help students understand how consumers think, how to build relationships that drive sales, and how to use data to make better marketing decisions. I also develop professional skills courses on topics I wish someone had taught me earlier."
  },
  {
    "objectID": "teaching/index.html#core-courses",
    "href": "teaching/index.html#core-courses",
    "title": "Teaching",
    "section": "Core Courses",
    "text": "Core Courses\n\nConsumer Behavior\nMKT 382 · Undergraduate\nWhy do people buy what they buy? This course examines the psychological, social, and cultural factors that shape consumer decisions—from perception and attention to attitudes, emotions, and social influence. Students learn to apply these insights to marketing strategy.\nOffered in-person and asynchronously online.\nCourse Materials →\n\n\nSales Management & Personal Selling\nMKT 374 · Undergraduate\nSales is fundamentally about understanding people and building relationships. This course develops both the interpersonal skills of effective selling and the strategic thinking required to manage sales teams and processes.\nOffered in-person and asynchronously online.\nCourse Materials →\n\n\nMarketing Research & Analytics\nMKT 378 · Undergraduate\nHands-on training in research design, data collection, and analysis using R. Students work with real data and learn to translate quantitative findings into actionable marketing insights. Includes custom-built interactive tools for exploring statistical concepts.\nCourse Materials →"
  },
  {
    "objectID": "teaching/index.html#professional-skills-courses",
    "href": "teaching/index.html#professional-skills-courses",
    "title": "Teaching",
    "section": "Professional Skills Courses",
    "text": "Professional Skills Courses\nOne-credit courses focused on building specific capabilities. Both are offered asynchronously online.\n\nTime Management: The Psychology of Getting Things Done\n1 Credit · Professional Skills\nBeyond productivity hacks: this course examines the psychological foundations of time management—attention, motivation, habit formation, and sustainable productivity. Students learn why certain strategies work and when to apply them.\nCourse Materials →\n\n\nMoral Judgment & Decision Making in the Marketplace\n1 Credit · Professional Skills · Coming Spring 2026\nHow do consumers, managers, and organizations navigate ethical decisions? This course examines frameworks for moral reasoning and applies them to contemporary business challenges—from pricing fairness to data ethics to sustainability.\nCourse Materials →"
  },
  {
    "objectID": "teaching/index.html#special-topics",
    "href": "teaching/index.html#special-topics",
    "title": "Teaching",
    "section": "Special Topics",
    "text": "Special Topics\nI’ve also developed experiential courses connecting business students with sustainable agriculture in Maine, in partnership with the BARD Institute and organizations including Maine Farmland Trust, the Maine Cheese Guild, and Maine Fibershed. These courses—funded in part by a congressional earmark—give students hands-on experience applying business skills in contexts they rarely encounter in traditional coursework."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research uses behavioral economics and social psychology to understand how people make judgments and decisions as consumers, employees, and citizens. I’m drawn to questions where understanding the psychology can lead to better outcomes—for individuals, for organizations, and for society.\nFor a complete list of publications and working papers, see my CV."
  },
  {
    "objectID": "research.html#well-being",
    "href": "research.html#well-being",
    "title": "Research",
    "section": "Meaning, Pleasure, and Well-Being",
    "text": "Meaning, Pleasure, and Well-Being\nThe pursuit of happiness drives many consumption decisions. But what do people actually mean when they say they want to be happy?\nWith Lawrence Williams, I study the distinction between happiness as pleasure (hedonic well-being) and happiness as meaning (eudaimonic well-being). We find that consumers approach these differently: compared to pursuing pleasure, pursuing meaning involves expectations of lasting benefits—and requires greater minimum time investments to derive those benefits.\nThis has implications for how we design experiences, how we market them, and how consumers allocate their most precious resource: time.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nConsumers’ Minimum Time Investments in Meaningful Consumption Percival Carter, Williams, & Light • Marketing Letters, 2023 Read →"
  },
  {
    "objectID": "research.html#fairness",
    "href": "research.html#fairness",
    "title": "Research",
    "section": "Fairness, Harm, and Moral Judgment",
    "text": "Fairness, Harm, and Moral Judgment\nWhen is a price unfair? When does a company cross an ethical line? These aren’t just philosophical questions—they drive consumer behavior, shape reputations, and determine market outcomes.\nMy work in this area approaches price fairness as fundamentally a moral judgment, shaped by perceptions of harm to consumers. I’ve also examined how pregnancy is perceived in academic institutions, how harassment and discrimination policies fail, and how consumers make demands of entrepreneurs based on gender.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nPainful Prices: A Moral Harm Approach to Price Fairness Campbell, Pomerance, & Percival Carter • Journal of Consumer Research, 2025\nReframing and Restructuring Organizational Strategies for Addressing Workplace Harassment and Discrimination Percival Carter & Obenauer • Group & Organization Management, 2025\nPower and the Perception of Pregnancy in the Academy Percival Carter • Gender, Work & Organization, 2023 Read →"
  },
  {
    "objectID": "research.html#sustainability",
    "href": "research.html#sustainability",
    "title": "Research",
    "section": "Sustainability and Food Systems",
    "text": "Sustainability and Food Systems\nHow do consumers think about local, sustainable, and artisan food? And how can small-scale producers compete in markets dominated by industrial agriculture?\nThis research stream connects my academic interests to work I care about deeply. With partners including Maine Farmland Trust, the Maine Cheese Guild, and Maine Fibershed, I study how consumers perceive small-scale producers’ products differently from conventional alternatives—and how producers can design experiences that cultivate genuine connection with their customers.\nCurrent work examines how Maine can communicate about PFAS contamination in ways that inform consumers without unfairly damaging perceptions of all Maine food.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nDesigning and Distinguishing Meaningful Artisan Food Experiences Percival Carter & Welcomer • Sustainability, 2021 Read →"
  },
  {
    "objectID": "research.html#authenticity",
    "href": "research.html#authenticity",
    "title": "Research",
    "section": "Authenticity and Imperfection",
    "text": "Authenticity and Imperfection\nCan a product be too perfect?\nWith Pete McGraw, I study how flaws can actually increase product evaluations. The key insight: when consumers care about production processes that are difficult to verify (organic farming, handmade crafts), visible imperfections serve as credible signals. A blemished apple might be more appealing than a perfect one—because the blemish reduces uncertainty about how it was grown.\nThis work has implications for artisan producers, sustainable agriculture, and anyone competing on authenticity rather than polish.\n\n\n\n\n\n\nNoteWorking Paper\n\n\n\n\n\nIn Pursuit of Imperfection: Flawed Products Reduce Process Uncertainty Percival Carter & McGraw • Preparing for submission\nAs discussed on the Here We Are podcast and demonstrated memorably by Ron Swanson."
  },
  {
    "objectID": "research.html#hype",
    "href": "research.html#hype",
    "title": "Research",
    "section": "Hype and Its Discontents",
    "text": "Hype and Its Discontents\nChampionships. Series finales. Award shows. Millions of people tune in to hyped events even when they have no intrinsic interest in boxing, period dramas, or filmmaking.\nWith Lawrence Williams and Pete McGraw, I studied how “believing the hype” affects consumer well-being. Using data from nearly 7,000 people across 16 hyped television events, we found that giving in to hype is largely detrimental—with one exception: it can improve social well-being by helping solitary viewers feel connected through shared cultural experiences.\n\n\n\n\n\n\nNoteWorking Paper\n\n\n\n\n\nHow Hype Helps and Hinders Well-Being Percival Carter, Williams, & McGraw • Under revision"
  },
  {
    "objectID": "research.html#humor",
    "href": "research.html#humor",
    "title": "Research",
    "section": "Humor",
    "text": "Humor\nIn a previous life, I managed the Humor Research Lab at the University of Colorado Boulder. That work examined when humor helps and hurts brand attitudes—finding that the key isn’t whether an ad is funny, but whether it triggers negative emotional reactions independent of humor.\n\n\n\n\n\n\nNoteSelected Publications\n\n\n\n\n\nBeing Funny is Not Enough: Negative Feelings Predict When Humor is Persuasive Warren, Percival Carter, & McGraw • International Journal of Advertising, 2019"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Erin Percival Carter Associate Professor of Marketing Maine Business School, University of Maine\nEmail · erin.p.carter@maine.edu Office · 5723 Donald P. Corbett Business Building, Orono, ME Phone · (207) 481-4944"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Erin Percival Carter Associate Professor of Marketing Maine Business School, University of Maine\nEmail · erin.p.carter@maine.edu Office · 5723 Donald P. Corbett Business Building, Orono, ME Phone · (207) 481-4944"
  },
  {
    "objectID": "contact.html#for-students",
    "href": "contact.html#for-students",
    "title": "Contact",
    "section": "For Students",
    "text": "For Students\nCurrent students: reach out via email or Brightspace. Office hours are posted each semester and announced in class.\nI’m always happy to talk about research opportunities, career questions, or course material you’re wrestling with."
  },
  {
    "objectID": "contact.html#for-collaboration",
    "href": "contact.html#for-collaboration",
    "title": "Contact",
    "section": "For Collaboration",
    "text": "For Collaboration\nI occasionally take on consulting projects focused on behavioral research to inform marketing strategy and brand development. I’m most interested in projects that:\n\nHave potential for academic publication\nFocus on sustainable agriculture, food systems, or Maine-based organizations\nInvolve interesting behavioral questions\n\nIf your project isn’t a fit for me, I’m glad to refer you to someone who might be better suited."
  },
  {
    "objectID": "contact.html#media-speaking",
    "href": "contact.html#media-speaking",
    "title": "Contact",
    "section": "Media & Speaking",
    "text": "Media & Speaking\nFor media inquiries or speaking requests, email me with details about your project or event. I’m particularly happy to discuss consumer psychology, sustainable agriculture, or the intersection of the two."
  },
  {
    "objectID": "index.html#how-i-work",
    "href": "index.html#how-i-work",
    "title": "Erin Percival Carter",
    "section": "How I Work",
    "text": "How I Work\n\n\n\n◈\n\n\nResearch\nUsing behavioral experiments to understand how consumers think about meaning, morality, authenticity, and sustainability.\n\n\n\n\n◇\n\n\nTeaching\nI primarily teach consumer behavior, sales, marketing research. Building courses on the psychology of time and moral decision-making.\n\n\n\n\n▽\n\n\nPractice\nThrough BARD, connecting small-scale agricultural producers with the business insights they need to thrive."
  },
  {
    "objectID": "index.html#navigating-risk-in-food-systems",
    "href": "index.html#navigating-risk-in-food-systems",
    "title": "Erin Percival Carter",
    "section": "Navigating Risk in Food Systems",
    "text": "Navigating Risk in Food Systems\nI’m currently investigating how Maine can communicate about PFAS contamination in ways that inform consumers without unfairly damaging perceptions of all Maine-grown food. This work sits at the intersection of risk perception, regional branding, and the psychology of trust.\nI’m also developing new courses on the psychology of time management and moral judgment in business—topics I wish someone had taught me earlier in my career.\nExplore my research →"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download as PDF · Last updated January 2025"
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Curriculum Vitae",
    "section": "Contact",
    "text": "Contact\nErin Percival Carter · Associate Professor of Marketing Maine Business School, University of Maine\n5723 Donald P. Corbett Business Building, Orono, ME 04469-5723 erin.p.carter@maine.edu · erinlpc.com Office: (207) 481-4944"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nPh.D., Marketing Leeds School of Business, University of Colorado Boulder, 2017\nBachelor of Science, Marketing University of Wyoming, 2010"
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Curriculum Vitae",
    "section": "Research Interests",
    "text": "Research Interests\nJudgment and Decision Making · Well-Being · Authenticity · Morality · Sustainability and Food Systems"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nPainful Prices: A Moral Harm Approach to Price Fairness Campbell, Margaret C., Justin Pomerance, and Erin Percival Carter Journal of Consumer Research, 2025\n\n\nReframing and Restructuring Organizational Strategies for Addressing Workplace Harassment and Discrimination in the Workplace Percival Carter, Erin, and William G. Obenauer Group & Organization Management, 2025\n\n\nConsumers’ minimum time investments in meaningful consumption Percival Carter, Erin, Lawrence E. Williams, Nicholas Light Marketing Letters, 2023 DOI →\n\n\nWhat Was Yours is (For Now) Mine: Prior User Knowledge Reduces Product Satisfaction but Can Improve Experiential Satisfaction in Access-Based Consumption Stough, Rusty and Erin Percival Carter Journal of Consumer Behavior, 2023\n\n\nPower and the Perception of Pregnancy in the Academy: Reflection, Review, and Recommendations for Institutional Change Percival Carter, Erin Gender, Work & Organization, 2023 DOI →\n\n\nDesigning and Distinguishing Meaningful Artisan Food Experiences Percival Carter, Erin, and Stephanie Welcomer Sustainability, 2021 DOI →\n\n\nBeing Funny is Not Enough: Negative Feelings Predict When Humor is Persuasive Warren, Caleb, Erin Percival Carter, and A. Peter McGraw International Journal of Advertising, 2019"
  },
  {
    "objectID": "cv.html#working-papers",
    "href": "cv.html#working-papers",
    "title": "Curriculum Vitae",
    "section": "Working Papers",
    "text": "Working Papers\n\nControl Without Consensus: Innovation Pressures in Tension with Consumer Preferences in Controlled Environment Agriculture Entsminger, Jason, Lucy McGowan, and Erin Percival Carter Preparing for Submission · Target: Science, Technology, and Human Values\n\n\nMessages that Help Consumers of Maine Food Navigate PFAS Information Percival Carter, Erin, Caroline Noblet, and Qiujie (Angie) Zheng Discussing outlets with co-authors\n\n\nGive a Little Bit: Consumers Ask More of Women Entrepreneurs Percival Carter, Erin, Jennifer Dinger, and Molly Rapert Submitting · Target: Journal of Research in Marketing and Entrepreneurship\n\n\nHow Hype Helps and Hinders Well-Being Percival Carter, Erin, Lawrence Williams, and A. Peter McGraw Preparing for submission Based on 3rd essay of dissertation research\n\n\nIn Pursuit of Imperfection: Flawed Products Reduce Process Uncertainty Percival Carter, Erin, and A. Peter McGraw Preparing for submission · Target: Food Quality and Preference\n\n\nAdapting Hackathons for Online Marketing Education Percival Carter, Erin Preparing for submission\n\n\nPrice Gouging at the Pumpkin Patch? Expense Neglect in Agritourism Leads to Perceptions of Price Unfairness Percival Carter, Erin Data collected, writing in progress\n\n\nIs Food Art? Looking at the Role of Food Entrepreneurs as Artists Percival Carter, Erin, Jason Entsminger, and Rusty Stough Writing in progress\n\n\nAgricultural Work-Based Learning Fosters Business Students’ Interest in Pursuing Agricultural Careers Welcomer, Stephanie, and Erin Percival Carter Writing in progress\n\n\nHumor Production and Perceptions of Psychological Health McGraw, A. Peter, Erin Percival Carter, and Jennifer Harman Preparing for submission Available at https://ssrn.com/abstract=2727829"
  },
  {
    "objectID": "cv.html#grants",
    "href": "cv.html#grants",
    "title": "Curriculum Vitae",
    "section": "Grants",
    "text": "Grants\n\nAwarded\nThe Agritourism Premium: Culinary Trails as an Experiential Marketing Strategy for State-Branded Farm, Fish, and Fiber Products $14,091 · 2024 · Northeast SARE · PI\nBARD (Business, Agriculture, and Rural Development) Technical Assistance Pipeline $292,000 · 2022 · US Congressionally Directed Spending (Senators Collins and King) · Co-Developer and Key Personnel\n\n\nSubmitted & Pending\nBurnout Contagion – Network Effects of Employee Burnout Spillover on Interdependent Organizations $288,127 · 2025 · NSF · Co-PI\nPFAS Risk Perception and Communication: Hypersensitivity to Language, Insensitivity to Dose $50,233 · 2025 · UMaine Research Funding Opportunity · Co-PI · Notice expected October 2025\nExpanding solar projects at the University of Maine, grazing dairy heifers at Witter Farm $49,875 · 2025 · US DOE LASSO · Co-PI · Final decisions pending\nPARTNERSHIP: Sustainable Agrofood, Consumer Response, and Venturing: Uncertainty Reduction within the context of Controlled Environment Agriculture $807,822 · 2024 · USDA, NIFA · Co-PI · Final decision pending\nOffshoreWind4Maine: 2.0 Offshore Wind Workforce Development Academy $496,918 · 2024 · State of Maine Governor’s Office · Co-PI"
  },
  {
    "objectID": "cv.html#awards-honors",
    "href": "cv.html#awards-honors",
    "title": "Curriculum Vitae",
    "section": "Awards & Honors",
    "text": "Awards & Honors\n\nMaine Business School Excellence in Research Award (2024)\nNominated for MBS Teaching Award (2022, 2023)\nSelected by Beta Gamma Sigma students for induction (2021)\nUniversity of Maine nominee for Donald Harward Award for Excellence in Service Learning (2021)\nUniversity of Maine CUGR Faculty Fellow (2020-2021)\nMaine Business School Dean’s Research Award (2019)\nQualtrics Behavioral Research Grant (2015)\nAMA Sheth Doctoral Consortium Fellow (2015)"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Curriculum Vitae",
    "section": "Teaching",
    "text": "Teaching\n\nCurrent Courses\n\nMKT 382 · Consumer Behavior\nMKT 374 · Personal Selling and Sales Management\nMKT 378 · Marketing Research"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Curriculum Vitae",
    "section": "Service",
    "text": "Service\n\nJournal Reviewing\nJournal of Marketing · Journal of Consumer Research (Trainee) · International Journal of Research in Marketing · International Journal of Advertising · British Food Journal · Humor: International Journal of Humor Research · Association for Consumer Research Conference\n\n\nProfessional Affiliations\nSociety for Agriculture and Human Values · American Marketing Association · Association for Consumer Research · International Positive Psychology Association · Society for Consumer Psychology · Society for Personality and Social Psychology\n\n\nPublic Service\n\nMaine Fibershed, Steering Committee Member\nThe Maine Food Strategy, Steering Committee Member\nOrono Town Council Ad-Hoc Committee on Diversity Equity and Inclusion, Associate Member and Data Consultant"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\nCo-Founder & Director · BARD Institute Orono, ME · Present\nDirector and Principal Behavioral Science Consultant · ELPC, LLC Orono, ME · Present\nDigital and Interactive Marketing Coordinator · University of Wyoming Laramie, WY · 2011-2012\nMarketing Manager · The Blue Sky Group Inc. Laramie, WY · 2010-2011"
  },
  {
    "objectID": "bard.html",
    "href": "bard.html",
    "title": "BARD Institute",
    "section": "",
    "text": "The BARD Institute is an independent organization I co-founded and co-direct. We work to strengthen sustainable agriculture by bringing contemporary business thinking and research to small-scale producers.\nThis work was inspired by Dr. Ayana Elizabeth Johnson’s Climate Venn Diagram—finding the intersection of what you’re good at, what the world needs, and what brings you joy. For me, that intersection sits squarely at the junction of behavioral science, business strategy, and sustainable food systems."
  },
  {
    "objectID": "bard.html#what-we-do",
    "href": "bard.html#what-we-do",
    "title": "BARD Institute",
    "section": "What We Do",
    "text": "What We Do\n\nBusiness Advising\nTailored guidance for small-scale agricultural producers navigating markets, pricing, branding, and growth—grounded in research on how consumers actually perceive and value local and artisan products.\n\n\nResearch & Analytics\nUnderstanding consumer perceptions and market opportunities through rigorous research methods. Our work helps producers make evidence-based decisions rather than relying on intuition alone.\n\n\nStudent Engagement\nDeep-dive courses and research assistantships that give business students hands-on experience in sustainable agriculture—filling a critical gap in both business education and agricultural service provision."
  },
  {
    "objectID": "bard.html#partners",
    "href": "bard.html#partners",
    "title": "BARD Institute",
    "section": "Partners",
    "text": "Partners\nWe’ve been fortunate to work with organizations committed to Maine’s agricultural future:\n\nMaine Farmland Trust\nMaine Cheese Guild\nMaine Fibershed\nMaine Fiber Frolic"
  },
  {
    "objectID": "bard.html#learn-more",
    "href": "bard.html#learn-more",
    "title": "BARD Institute",
    "section": "Learn More",
    "text": "Learn More\nVisit bardinstitute.com for current projects and ways to get involved.\nInterested in partnering with BARD or having us work with your organization? Get in touch."
  },
  {
    "objectID": "teaching/moral-judgment/index.html",
    "href": "teaching/moral-judgment/index.html",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "",
    "text": "WarningComing Spring 2026\n\n\n\nThis course is currently under development.\nThis course explores how consumers, managers, and organizations navigate ethical decisions in commercial contexts. We examine frameworks for moral reasoning and apply them to contemporary business challenges."
  },
  {
    "objectID": "teaching/moral-judgment/index.html#course-format",
    "href": "teaching/moral-judgment/index.html#course-format",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Course Format",
    "text": "Course Format\nAsynchronous online. All materials will be designed to be accessible and self-paced."
  },
  {
    "objectID": "teaching/moral-judgment/index.html#topics",
    "href": "teaching/moral-judgment/index.html#topics",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Topics",
    "text": "Topics\nPlanned topics include:\n\nFoundations of moral psychology\nConsumer responses to corporate ethics and CSR\nPricing and fairness\nPrivacy and data ethics\nEnvironmental and sustainability ethics\nWhistleblowing and organizational moral courage"
  },
  {
    "objectID": "teaching/moral-judgment/index.html#contact",
    "href": "teaching/moral-judgment/index.html#contact",
    "title": "Moral Judgment & Decision Making in the Marketplace",
    "section": "Contact",
    "text": "Contact\nInterested in this course? Get in touch with questions."
  },
  {
    "objectID": "teaching/sales-management/index.html",
    "href": "teaching/sales-management/index.html",
    "title": "Sales Management & Personal Selling",
    "section": "",
    "text": "This course develops skills in personal selling, sales force management, and relationship building. You’ll learn both the interpersonal dynamics of effective selling and the strategic considerations of managing a sales organization."
  },
  {
    "objectID": "teaching/sales-management/index.html#course-format",
    "href": "teaching/sales-management/index.html#course-format",
    "title": "Sales Management & Personal Selling",
    "section": "Course Format",
    "text": "Course Format\nThis course alternates between in-person and asynchronous online formats depending on the semester."
  },
  {
    "objectID": "teaching/sales-management/index.html#lecture-videos",
    "href": "teaching/sales-management/index.html#lecture-videos",
    "title": "Sales Management & Personal Selling",
    "section": "Lecture Videos",
    "text": "Lecture Videos\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nLecture video links will be added here."
  },
  {
    "objectID": "teaching/sales-management/index.html#readings",
    "href": "teaching/sales-management/index.html#readings",
    "title": "Sales Management & Personal Selling",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nCourse readings will be added here."
  },
  {
    "objectID": "teaching/sales-management/index.html#assignments",
    "href": "teaching/sales-management/index.html#assignments",
    "title": "Sales Management & Personal Selling",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nAssignment materials will be added here."
  },
  {
    "objectID": "teaching/marketing-research/shinyapps/BagelRunPredictor.html",
    "href": "teaching/marketing-research/shinyapps/BagelRunPredictor.html",
    "title": "Bagel Run Predictor App",
    "section": "",
    "text": "TipAlternative Access\n\n\n\nIf the embedded app below doesn’t display correctly in your browser, you can open it directly in a new tab."
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "",
    "text": "This exercise walks through:\n\nBuilding a tiny dataset of ticket totals + item descriptions\nPicking a one-number model (b₀) to predict EVERY ticket\nCalculating, by hand, the error columns:\n\nERROR = DATA - MODEL\n\nComputing four summary measures of error:\n\nCE = count of mismatches (who isn’t exactly b₀?)\nSE = sum of residuals (signed) — can cancel!\nSAE = sum of absolute errors\nSSE = sum of squared errors\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#overview-and-goals",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#overview-and-goals",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "",
    "text": "This exercise walks through:\n\nBuilding a tiny dataset of ticket totals + item descriptions\nPicking a one-number model (b₀) to predict EVERY ticket\nCalculating, by hand, the error columns:\n\nERROR = DATA - MODEL\n\nComputing four summary measures of error:\n\nCE = count of mismatches (who isn’t exactly b₀?)\nSE = sum of residuals (signed) — can cancel!\nSAE = sum of absolute errors\nSSE = sum of squared errors\n\n\n\n\n\n\n\n\nNoteRemember the fundamental equation\n\n\n\nDATA = MODEL + ERROR → ERROR = DATA - MODEL"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-0-build-the-dataset",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-0-build-the-dataset",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 0 — Build the Dataset",
    "text": "Part 0 — Build the Dataset\nWe’ll hard-code a small sample dataset with six tickets. The first column will be a respondent ID, the second the ticket total, and the third column will be an order summary.\n\nID    &lt;- 1:6\nTotal &lt;- c(6, 7, 7, 8, 30, 300)  # dollars per ticket (continuous outcome)\nItem  &lt;- c(\"8 oz drip\",\n           \"12 oz drip\",\n           \"8 oz drip plus flavor\",\n           \"Pumpkin spice latte\",\n           \"Matcha latte + salad to go, breakfast bowl\",\n           \"Catering order - full coffee bar\")\n\ncoffee &lt;- data.frame(ID, Total, Item, stringsAsFactors = FALSE)\n\n# Peek:\ncoffee"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-1-mode",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-1-mode",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 1 — Mode",
    "text": "Part 1 — Mode\nNow we need to go through and create a new variable to capture what kind of drink (if any) appears on the ticket. There’s no right or wrong answer as far as EXACTLY what the catergories could be here (for example, depending on what all is included in the “coffee bar” catering order, that might be just drip coffee but a lot of it though I’m not treating it that way). This type of recoding of data is something it’s important to make sure is well-documented and that whoever is going to use and rely on your analysis either agrees with beforehand (ideal) or at a minimum is able to decipher easily on their own.\nNote I could just use my silly human eyes and brain to go through and code each one by one but instead, I’m going to use a function in R from the grep family of commands. This is basically a series of commands that tells R how to do some pattern matching. You do need to be careful about confirming results when you use something like this but it’s pretty handy especially if we’re attempting to recode more than 7 observations. This is not a section of code I want you to get particularly bogged down in (I won’t be testing you on it) but for those of you looking to stretch some skills, you might have fun playing around with it a bit. For what it’s worth, typically my advice would be to do your best to design your survey or other data collection instrument so that all of this coding happens at the time of data collection so that you can avoid needing to recode as much as possible.\nThat said, we’ll classify each Item into a drink category using base R grepl().\nI’m doing this to provide a simple categorical lens for us to estimate “mode”.\n\nDrinkCategory &lt;- ifelse(grepl(\"drip\",      Item, ignore.case = TRUE), \"Drip coffee\",\n                        ifelse(grepl(\"latte\",     Item, ignore.case = TRUE), \"Latte\",\n                               ifelse(grepl(\"catering\",  Item, ignore.case = TRUE), \"Catering\",\n                                      \"Other\")))\ncoffee$DrinkCategory &lt;- DrinkCategory\n\nNow we’re going to use the table() command to give us a count of items in each of the categories in that new variable we created (a categorical “mode” demo). Expect “Drip coffee” to be the modal category here (3 of 6).\n\ntable(coffee$DrinkCategory)\n\nNote we can also generate a similar table to try to calculate the modal ticket total.\n\ntable(coffee$Total) #You should see that the mode here is 7\n\nThe problem is that for continuous data like income or ticket total or wtp, mode isn’t necessarily THAT informative…it tells us what the most frequent amount people paid but we have no idea based on this value if that most frequent value is near the center of our distribution, if those tickets were similar in composition of the order or just happened by chance, etc.\nThat said, we’re going to go ahead and record it as our mode with a new column:\n\nb0_mode &lt;- 7\ncoffee$b0_mode &lt;- b0_mode\n\nNow let’s try some alternative measures of central tendency."
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-2-alternative-single-b₀-models",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-2-alternative-single-b₀-models",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 2 — Alternative Single b₀ Models",
    "text": "Part 2 — Alternative Single b₀ Models\nLet’s try each of the following:\n\nmedian → a robust “typical” ticket, insensitive to extremes\nmean → our average value, sensitive to extremes\na number you choose (e.g., 7, 8, 12) to see how the error changes\n\nNote that for median and mean, we can just use the command median() and mean():\n\nb0_median &lt;- median(coffee$Total)  # with even n, it's the average of the two middle values\nb0_mean   &lt;- mean(coffee$Total)\n\nNow we make new columns in the dataset for each measure of central tendency (remember, this is the full extent of our MODEL for these simple models):\n\nb0_median &lt;- b0_median\nb0_mean &lt;- b0_mean\n\nAnd for fun, you pick a number for yourself here. I have it set at 12 but go ahead and change it to something else.\n\nb0_your_guess &lt;- 12       # try any number you like OTHER than 7, 12, 7.5, or 59.67\n\nOK, let’s take a look at our dataset really quick and make sure it looks how we expect it to look… remember, we’re expecting to see each one of our b₀’s in a new column on the right.\n\nhead(coffee)\n\nWell that’s not correct. Why do we only see one column for mode? There should be one for median, one for mean, and one for your guess…\nCan you figure out what went wrong? Give it a shot before revealing the hint below.\n\n\n\n\n\n\nTipClick to reveal hint\n\n\n\n\n\nUgh, we didn’t specify that we were creating new variables in the coffee dataset for the median, mean, and your guess values. We calculated the values and stored them in objects, but we never added them as columns to our dataframe!\n\n\n\nOK, fixed below:\n\ncoffee$b0_median &lt;- b0_median\ncoffee$b0_mean &lt;- b0_mean\ncoffee$b0_your_guess &lt;- b0_your_guess\n\n# Now let's check again:\nhead(coffee)\n\nNICE!"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-3-calculate-error-columns-by-hand",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#part-3-calculate-error-columns-by-hand",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Part 3 — Calculate Error Columns BY HAND",
    "text": "Part 3 — Calculate Error Columns BY HAND\n(Some of you probably know that we could use commands to calculate these error columns instead of coding the math by hand…me too. We’re going to do it by hand anyway because: 1. it’s not that tough, 2. it’s good practice, 3. I think it does a better job of helping you intuit what’s going on with error and that is like priority 1 in this chapter.)\n\nResiduals (signed): ERROR = DATA - MODEL\n\ncoffee$ei_mode &lt;- coffee$Total - coffee$b0_mode\ncoffee$ei_median &lt;- coffee$Total - coffee$b0_median\ncoffee$ei_mean &lt;- coffee$Total - coffee$b0_mean\ncoffee$ei_your_guess &lt;- coffee$Total - coffee$b0_your_guess\n\n# Let's take a peek at the dataset now (it's going to be looking a bit wider)\nhead(coffee)\n\nOK, so at this point we have calculated an error for EACH PREDICTION. The question is, how do we turn that into a single measure of error for the ENTIRE MODEL…?\nWe’re going to try 4 different strategies.\n\n\n\nStrategy 1: Count of Errors (CE)\nWe define ERROR as any time our MODEL prediction for any given particular DATA is not EXACTLY correct. This is the “Count of Errors” style from the book.\nSo one way that we could check this in our code is to tell R the following: For a given b₀, mark each row TRUE if the model nails it exactly (in other words, if Total == b₀), FALSE otherwise. Then count the TRUEs and FALSEs.\nRemember for continuous data like ticket totals, exact matches are rare — that’s the whole point here - we’re going to see CE kind of fall apart in terms of its usefulness for continuous data.\n\n# Row-by-row TRUE/FALSE hits for each constant model\ncoffee$hit_mode        &lt;- coffee$Total == coffee$b0_mode\ncoffee$hit_median      &lt;- coffee$Total == coffee$b0_median\ncoffee$hit_mean        &lt;- coffee$Total == coffee$b0_mean\ncoffee$hit_your_guess  &lt;- coffee$Total == coffee$b0_your_guess\n\n# Peek at what we just made (Note in the code below, I'm asking R to just show \n# me a peek of only specific columns in the dataset because we've now added a lot \n# of columns; everything we need to evaluate what's happening here and nothing \n# else will appear using this code with left side = data, right = our results for\n# each measure of central tendency and when DATA == MODEL exactly)\nhead(coffee[, c(\"ID\", \"Total\",\n                \"b0_mode\", \"b0_median\", \"b0_mean\", \"b0_your_guess\",\n                \"hit_mode\", \"hit_median\", \"hit_mean\", \"hit_your_guess\")])\n\nThis is a small dataset (and for all predictors other than MODE, we’re going to have a count of errors == our n since not a single observation in data == our other predictions) but still, what we want here is not a column of TRUEs and FALSEs, but instead a count of how many FALSEs we generate for any given measure of central tendency. So let’s make that using code below:\n\n# Summarize the TRUE/FALSE counts for each b0\ncat(\"\\n--- TRUE/FALSE counts (does Total equal b0?) ---\\n\")\ncat(\"Mode b0:\\n\");       print(table(coffee$hit_mode))\ncat(\"Median b0:\\n\");     print(table(coffee$hit_median))\ncat(\"Mean b0:\\n\");       print(table(coffee$hit_mean))\ncat(\"Your guess b0:\\n\"); print(table(coffee$hit_your_guess))\n\nIf you want one compact little table of counts, we can actually create OUR OWN FUNCTION to do that for us. We’re going to call that function count_tf:\n\ncount_tf &lt;- function(log_vec) c(\"FALSE\" = sum(!log_vec), \"TRUE\" = sum(log_vec))\nce_counts &lt;- rbind(\n  mode.      = count_tf(coffee$hit_mode),\n  median     = count_tf(coffee$hit_median),\n  mean       = count_tf(coffee$hit_mean),\n  your_guess = count_tf(coffee$hit_your_guess)\n)\n\ncat(\"\\nCompact CE-style counts (rows = model, cols = FALSE/TRUE):\\n\")\nprint(ce_counts)\n\n(Don’t get terribly bogged down in the function of it all if that’s overwhelming right now - that’s more of a fun little easter egg for anyone interested and also so that you can say that you’ve coded a new function in R by week 3 of the class which sounds pretty badass to be honest, so enjoy that)\nSo to be clear, our count of errors == 6 for all measures of central tendency except for mode, in which case the count of errors = 4.\nBut there’s another way that we could get this same count using 1 column in our dataset instead of two… any ideas…?\n\n\n\n\n\n\nTipClick to reveal hint\n\n\n\n\n\nThat’s right! We could also just count the number of eᵢ’s == 0 in any of our calculated ERROR columns! Because if our MODEL == DATA, our ERROR == 0 (because ERROR == DATA - MODEL, so if our model predicts 7 and a given observation is 7, error for that value is ERROR == 7-7 which == 0)\n\n\n\nSo let’s check that out:\n\n# Count exact hits by checking residuals equal zero\nei_equal_0_count_mode       &lt;- coffee$ei_mode == 0\nei_equal_0_count_median     &lt;- coffee$ei_median == 0\nei_equal_0_count_mean       &lt;- coffee$ei_mean == 0\nei_equal_0_count_your_guess &lt;- coffee$ei_your_guess == 0\n\n# we can just look at each of the objects that we just made (they're going to be\n# just a series of TRUEs and FALSEs for whether each value == 0)\nei_equal_0_count_mode\nei_equal_0_count_median\nei_equal_0_count_mean\nei_equal_0_count_your_guess\n\nBut what we would really rather do is get a table of values, so as before let’s summarize the TRUE/FALSE counts for each eᵢ:\n\ncat(\"\\n--- TRUE/FALSE counts (does ei == 0?) ---\\n\")\ncat(\"Mode ei:\\n\");         print(table(ei_equal_0_count_mode))\ncat(\"Median ei:\\n\");       print(table(ei_equal_0_count_median))\ncat(\"Mean ei:\\n\");         print(table(ei_equal_0_count_mean))\ncat(\"Your guess ei:\\n\");   print(table(ei_equal_0_count_your_guess))\n\nOK, and finally, the entire point of doing this two ways (using counts of times in which DATA=MODEL and ERROR==0 as hits and every other instance in our sample as misses or a count of errors) is that the two are equivalent, so let’s just really quick check to make sure that both of the strategies we tried gave us the same answers:\n\ncat(\"\\n--- Check the equivalence (hits via DATA==b0 vs ei==0) ---\\n\")\ncat(\"Mode:       \", all(coffee$hit_mode     == ei_equal_0_count_mode),     \"\\n\")\ncat(\"Median:     \", all(coffee$hit_median   == ei_equal_0_count_median),   \"\\n\")\ncat(\"Mean:       \", all(coffee$hit_mean     == ei_equal_0_count_mean),     \"\\n\")\ncat(\"Your guess: \", all(coffee$hit_your_guess == ei_equal_0_count_your_guess), \"\\n\")\n\nThat was honestly a lot of time spent on count of errors as a summary measure of total error in our model for me to now say that we will rarely use that as a summary measure of error in our model again…mode just isn’t that useful for continuous data which will be the majority of our focus for at least the next while. So, if we want to have a measure of error that gets at the idea of how close our model predictions are to our actual data (without requiring that those predictions be EXACT), what might we try…\nOne thing that might make sense, given that we have these handy error columns calculated and ready to go, is that we could just take a sum of all of our errors. This frankly sounds pretty reasonable. Those values are the difference between DATA and our MODEL (we literally calculated them by subtracting MODEL predictions from individual observations in data), so let’s just add up those differences and call it a day, yeah?\n\n\n\nStrategy 2: Sum of Errors (SE)\n\nSoE_mode &lt;- sum(coffee$ei_mode)\nSoE_median &lt;- sum(coffee$ei_median)\nSoE_mean &lt;- sum(coffee$ei_mean)\nSoE_your_guess &lt;- sum(coffee$ei_your_guess)\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSoE_mode\nSoE_median\nSoE_mean\nSoE_your_guess\n\nOK, so what the heck is this telling us…one thing should really stand out immediately, which is that we are getting a VERY, VERY small number for the SoE for our mean. Like really small. That’s a really impressive model, hardly any error at all!\nAs a reminder, this value is showing in scientific notation because it is so so small - if it wasn’t in scientific notation with that e-14 at the end it would be extremely obnoxious trying to count the zeroes between the decimal and the first non-zero number…see for yourself when I tell it to give me a version not in scientific notation below:\n\nSoE_mean_nonsci &lt;- format(SoE_mean, scientific=F)\nSoE_mean_nonsci\n\nBut wait a second, that doesn’t seem right…when we looked at the dataset earlier, I don’t remember that column having a bunch of exceptionally small error values…let’s check again to confirm:\n\ncoffee$ei_mean\n\nThose are not small errors. Well wtf.\nLet’s think about this for a second. What’s wrong with this measure…?!?!\n\n\n\n\n\n\nTipClick to reveal the answer\n\n\n\n\n\nDid you figure it out?\nThe problem is that we are summing the eᵢ’s but when our eᵢ’s are pretty evenly distributed around 0, summing is going to allow the MODEL predictions that UNDERPREDICT the data to be positive values and the MODEL predictions that OVERPREDICT the data to be negative values. We then sum those positive and negative values and can end up with something that looks like zero error because the positive and negative errors cancel out. But we don’t have 0 error. We have a lot of error.\nWe can do better than this. Suggestions…?\n\n\n\n\n\n\nStrategy 3: Sum of Absolute Errors (SAE)\nWell, one thing we can do is to just take the absolute value of each eᵢ (making each a positive value or akin to a measure of distance). Let’s try that.\nAbsolute errors (remove the ability for some errors to be negative - let’s make it more like a measure of the distance from MODEL to DATA regardless of whether we under- or overpredict with MODEL):\n\nSAE_mode &lt;- sum(abs(coffee$ei_mode))\nSAE_median &lt;- sum(abs(coffee$ei_median))\nSAE_mean &lt;- sum(abs(coffee$ei_mean))\nSAE_your_guess &lt;- sum(abs(coffee$ei_your_guess))\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSAE_mode\nSAE_median\nSAE_mean\nSAE_your_guess\n\nOK, that looks more reasonable, yeah? When we move from a sum of errors to a sum of ABSOLUTE errors, our model error estimate for the simple model using mean as our b₀ significantly changes:\n\nSoE_mean\nSAE_mean\n\nSo now that’s behaving in a much more logical way (and understandably we won’t really ever use SoE again). But let’s think about what’s going on with SAE. What is it really doing for us? It’s giving us a simple sum of the distance measures of our predictions and we get a total measure of the extent to which our predictions “miss.” It doesn’t really take into account the severity of the extent to which our MODEL misses, though.\n\n\n\nStrategy 4: Sum of Squared Errors (SSE)\nOK, so SAE gives us the linear, additive “miss.” What if we want to have much harsher penalties (greater errors, a non-linear effect) for our MODEL predictions that miss DATA to a greater extent? Well, we could make our error non-linear, then. We could inflate those errors more significantly the greater the difference between the MODEL and DATA.\nHere’s what that’s going to look like:\n\nSSE_mode &lt;- sum(coffee$ei_mode^2)\nSSE_median &lt;- sum(coffee$ei_median^2)\nSSE_mean &lt;- sum(coffee$ei_mean^2)\nSSE_your_guess &lt;- sum(coffee$ei_your_guess^2)\n\n# Alright, let's see what we get for each one of those errors now, shall we?\nSSE_mode\nSSE_median\nSSE_mean\nSSE_your_guess\n\nThose are some big old whopping errors. But honestly, you know what? As long as we are consistent in how we calculate our errors between any given Model C and Model A that we might want to compare, it doesn’t much matter what the ABSOLUTE value of the errors is; we don’t calculate ARE (absolute reduction in error) in the approach to building models we use in this class, we calculate the PRE (proportional reduction in error). So it’s all relative.\nThat said, the difference between these two numbers does tell us something:\n\nSAE_mean\nSSE_mean\n\nIt tells us that because of those non-linear penalties for MODEL mispredicting DATA by more than a little bit, we must have some predictions that are off the mark by a pretty significant amount."
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#summary-table",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#summary-table",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Summary Table",
    "text": "Summary Table\nOK, let’s try putting this all together:\n\n# Table of summary values\nerror_summary &lt;- data.frame(\n  Model = c(\"Mode (7)\", \"Median (7.5)\", \"Mean (~59.67)\", sprintf(\"Your guess (%.2f)\", b0_your_guess)),\n  CE    = c(sum(!coffee$hit_mode), sum(!coffee$hit_median), sum(!coffee$hit_mean), sum(!coffee$hit_your_guess)),\n  SoE   = c(SoE_mode, SoE_median, SoE_mean, SoE_your_guess),\n  SAE   = c(SAE_mode, SAE_median, SAE_mean, SAE_your_guess),\n  SSE   = c(SSE_mode, SSE_median, SSE_mean, SSE_your_guess)\n)\n\nerror_summary\n\nIf the scientific notation in the SoE column throws you, we can create a version without scientific notation here:\n\nerror_summary_nsdisplay &lt;- error_summary\nerror_summary_nsdisplay$SoE &lt;- format(error_summary$SoE, scientific = FALSE, trim = TRUE)\n\nerror_summary_nsdisplay"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#visualizations",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#visualizations",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Visualizations",
    "text": "Visualizations\n\n\n\n\n\n\nWarningNote on “Your guess” labels\n\n\n\nThe visualization code below uses hardcoded labels like \"Your guess (b0 = 12)\". If you changed your guess to a different value, you’ll want to update these labels in the visualization code to match your actual value!\n\n\nYou can just run all the code from here to the end (unless you’ve added lines of code as you’ve worked through the assignment).\n\nOPTIONAL - Visualization Code\nThe code below creates the visualizations from the lecture video. If you worked through the above code EXACTLY as written (with the exception of changing the value for “your_guess”, which would be good/fine), you should be able to recreate these.\n\n# Load required packages\n# If any of these library calls throws an error, you probably haven't \n# installed that package. Run install.packages(\"packagename\")\n# and then call it from library with the code below again.\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)\n\n\n\n\nCount of Errors (CE) Visualization\n\n# Long format with one row per ticket x model (add numeric x_i)\nviz_ce &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    hit = (Total == Pred),\n    x_i = as.numeric(ID),           \n    col = ifelse(hit, \"hit\", \"miss\")\n  )\n\n# Count of errors (misses) per model for annotation\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_ce$x_i)\nce_counts &lt;- viz_ce %&gt;%\n  group_by(Model) %&gt;%\n  summarize(CountErrors = sum(!hit), .groups = \"drop\") %&gt;%\n  mutate(x_pos = x_last, y_pos = y_top)\n\n# Plot (facet 4 panels)\nCEplot &lt;- ggplot(viz_ce, aes(x = x_i)) +\n  geom_segment(\n    aes(y = Pred, yend = Pred,\n        x = x_i - 0.35, xend = x_i + 0.35),\n    color = \"blue\", linewidth = 2\n  ) +\n  geom_point(aes(y = Total, color = col), size = 2.6) +\n  scale_color_manual(\n    values = c(hit = \"forestgreen\", miss = \"red3\"),\n    labels = c(hit = \"Hit (MODEL == DATA)\", miss = \"Miss\"),\n    name = NULL\n  ) +\n  geom_text(\n    data = ce_counts,\n    aes(x = x_pos, y = y_pos,\n        label = paste0(\"Count of errors: \", CountErrors)),\n    inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5\n  ) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_ce$x_i[!duplicated(viz_ce$x_i)],\n                     labels = viz_ce$ID[!duplicated(viz_ce$ID)]) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Count of Errors (CE)\",\n    subtitle = \"Dots == DATA;\\n\n      • Green dots = exact hits (ei == 0 or MODEL==DATA) \\n\n      • Red dots = misses.\\n\\nBlue planks = model predictions\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\nCEplot\n\nBut that catering order outlier is a bit obnoxious, making it hard to see. Let’s take a log of our Y axis and try again:\n\n# CE (log y)\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\nce_counts_log &lt;- ce_counts %&gt;%\n  mutate(x_pos = max(viz_ce$x_i), y_pos = ymax_log * 0.95)\n\nCElogplot &lt;- ggplot(viz_ce, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total, color = col), size = 2.6) +\n  scale_color_manual(values = c(hit = \"forestgreen\", miss = \"red3\"),\n                     labels = c(hit = \"Hit (MODEL == DATA)\", miss = \"Miss\"),\n                     name = NULL) +\n  geom_text(data = ce_counts_log,\n            aes(x = x_pos, y = y_pos, label = paste0(\"Count of errors: \", CountErrors)),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_ce$x_i[!duplicated(viz_ce$x_i)],\n                     labels = viz_ce$ID[!duplicated(viz_ce$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(title = \"Count of Errors (CE) — log scale\",\n       subtitle = \"Dots == DATA;\\n\n      • Green dots = exact hits (ei == 0 or MODEL==DATA) \\n\n      • Red dots = misses.\\n\\nBlue planks = model predictions\",\n       x = \"Ticket ID\", y = \"Total ($, log10)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\nCElogplot\n\n\n\n\nSum of Errors (SE) Visualization\n\n# Long format with residuals and numeric xi\nviz_se &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    x_i   = as.numeric(ID),\n    resid = Total - Pred,\n    # color for the residual segment: black if positive (Pred &lt; Actual), \n    # red if negative (Pred &gt; Actual)\n    seg_col = ifelse(resid &gt;= 0, \"pos\", \"neg\")\n  )\n\n# Per-panel SE annotation (sum of residuals)\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_se$x_i)\nse_annot &lt;- viz_se %&gt;%\n  group_by(Model) %&gt;%\n  summarize(SE = sum(resid), .groups = \"drop\") %&gt;%\n  mutate(\n    label = sprintf(\"SE = %.2f\", SE),\n    x_pos = x_last, y_pos = y_top\n  )\n\n# Plot (facet 4 panels)\nSoEplot &lt;- ggplot(viz_se, aes(x = x_i)) +\n  geom_segment(\n    aes(y = Pred, yend = Total, xend = x_i, color = seg_col),\n    linewidth = 0.7, alpha = 0.9\n  ) +\n  scale_color_manual(values = c(pos = \"black\", neg = \"red3\"), guide = \"none\") +\n  geom_segment(\n    aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n    color = \"blue\", linewidth = 2\n  ) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(\n    data = se_annot,\n    aes(x = x_pos, y = y_pos, label = label),\n    inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5\n  ) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(\n    breaks = viz_se$x_i[!duplicated(viz_se$x_i)],\n    labels = viz_se$ID[!duplicated(viz_se$ID)]\n  ) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Sum of Error (SE)\",\n    subtitle = \"Line lengths == ERROR\\n\n    • Black line = underpredict (MODEL &lt; DATA, +ei) \\n\n    • Red line = overpredict (MODEL &gt; DATA, -ei)\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSoEplot\n\nBut let’s take the log of our y axis again:\n\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\nse_annot_log &lt;- se_annot %&gt;%\n  mutate(x_pos = max(viz_se$x_i), y_pos = ymax_log * 0.95)\n\nSoElogplot &lt;- ggplot(viz_se, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i, color = seg_col),\n               linewidth = 0.7, alpha = 0.9) +\n  scale_color_manual(values = c(pos = \"black\", neg = \"red3\"), guide = \"none\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = se_annot_log,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_se$x_i[!duplicated(viz_se$x_i)],\n                     labels = viz_se$ID[!duplicated(viz_se$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(title = \"Sum of Error (SE) log\",\n       subtitle = \"Line lengths == ERROR\\n\n            • Black line = underpredict (MODEL &lt; DATA, +ei) \\n\n            • Red line = overpredict (MODEL &gt; DATA, -ei)\",\n                   x = \"Ticket ID\", y = \"Total ($, log10)\") +\n  theme_minimal(base_size = 12)\n\nSoElogplot\n\n\n\n\nSum of Absolute Error (SAE) Visualization\nBut that’s not great (as we well know) because it suggests we have 0 error for mean because it’s treating some errors as negative and others as positive. Let’s represent making all line lengths positive by simply making the lines all black:\n\n# Long format with abs residuals\nviz_sae &lt;- coffee %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"Pred\"\n  ) %&gt;%\n  mutate(\n    x_i       = as.numeric(ID),\n    resid     = Total - Pred,\n    abs_resid = abs(resid)\n  )\n\n# Per-panel SAE annotation\ny_top  &lt;- max(coffee$Total) * 1.10\nx_last &lt;- max(viz_sae$x_i)\nsae_annot &lt;- viz_sae %&gt;%\n  group_by(Model) %&gt;%\n  summarize(SAE = sum(abs_resid), .groups = \"drop\") %&gt;%\n  mutate(\n    label = sprintf(\"SAE = %.2f\", SAE),\n    x_pos = x_last, y_pos = y_top\n  )\n\n# Plot (facet 4 panels)\nSAEplot &lt;- ggplot(viz_sae, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i),\n               linewidth = 0.7, alpha = 0.9, color = \"black\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = sae_annot,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(\n    breaks = viz_sae$x_i[!duplicated(viz_sae$x_i)],\n    labels = viz_sae$ID[!duplicated(viz_sae$ID)]\n  ) +\n  coord_cartesian(ylim = c(0, max(coffee$Total) * 1.12)) +\n  labs(\n    title = \"Sum of Absolute Error (SAE): Sum of absolute value of line lengths\",\n    subtitle = \"\\nBlack lines ==  ERROR = DATA - MODEL \\n\n    • Blue planks = MODEL\\n\n    • Dots = DATA\",\n    x = \"Ticket ID\",\n    y = \"Total ($)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSAEplot\n\nAnd now we use a log Y axis again for visualization given our outlier:\n\nymin_log &lt;- min(coffee$Total) * 0.8\nymax_log &lt;- max(coffee$Total) * 1.30\n\nsae_annot_log &lt;- sae_annot %&gt;%\n  mutate(x_pos = x_last, y_pos = ymax_log * 0.95)\n\nSAElogplot &lt;- ggplot(viz_sae, aes(x = x_i)) +\n  geom_segment(aes(y = Pred, yend = Total, xend = x_i),\n               linewidth = 0.7, alpha = 0.9, color = \"black\") +\n  geom_segment(aes(y = Pred, yend = Pred, x = x_i - 0.35, xend = x_i + 0.35),\n               color = \"blue\", linewidth = 2) +\n  geom_point(aes(y = Total), size = 2.6, color = \"black\") +\n  geom_text(data = sae_annot_log,\n            aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1, scales = \"fixed\") +\n  scale_x_continuous(breaks = viz_sae$x_i[!duplicated(viz_sae$x_i)],\n                     labels = viz_sae$ID[!duplicated(viz_sae$ID)]) +\n  scale_y_log10(breaks = c(5,6,7,8,10,20,30,50,100,200,300),\n                labels = number_format(accuracy = 1)) +\n  coord_cartesian(ylim = c(ymin_log, ymax_log)) +\n  labs(\n    title = \"Sum of Absolute Error (SAE) LOG: Sum of absolute value of line lengths\",\n    subtitle = \"\\nBlack lines ==  ERROR = DATA - MODEL \\n\n    • Blue planks = MODEL\\n\n    • Dots = DATA\",\n    x = \"Ticket ID\",\n    y = \"Total ($, log10)\"\n  ) +\n  theme_minimal(base_size = 12)\n\nSAElogplot\n\n\n\n\nSum of Squared Errors (SSE) Visualization\nFinally, we’re going to plot the sum of squared errors but it’s going to look a bit different because we need to represent that the eᵢ are being SQUARED to produce a nonlinear effect on our total error estimate. The best way to do that? Turn the error lines into SQUARES…literally.\n\n# Set to 1:6 to include all tickets. \n# Could change to 1:5 if you want to remove the outlier catering order and\n# see what things look like near the b0.\nids_to_use &lt;- 1:6\n\n# Long data with a b0 per panel\nviz_sse_base &lt;- coffee %&gt;%\n  filter(ID %in% ids_to_use) %&gt;%\n  transmute(\n    ID, Total,\n    `Mode (b0 = 7)`        = b0_mode,\n    `Median (b0 = 7.5)`    = b0_median,\n    `Mean (b0 = 59.67…)`   = b0_mean,\n    `Your guess (b0 = 12)` = b0_your_guess\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(ID, Total),\n    names_to = \"Model\",\n    values_to = \"b0\"\n  )\n\n# Compute residual, square side, and per-panel square placement\ngap &lt;- 0.6 \nviz_sse &lt;- viz_sse_base %&gt;%\n  mutate(\n    resid = Total - b0,\n    side  = abs(resid)\n  ) %&gt;%\n  group_by(Model) %&gt;%\n  arrange(Model, ID) %&gt;%\n  mutate(\n    xmin = cumsum(dplyr::lag(side + gap, default = 0)),\n    xmax = xmin + side,\n    ymin = ifelse(resid &gt;= 0, b0, b0 - side),\n    ymax = ifelse(resid &gt;= 0, b0 + side, b0),\n    xmid = (xmin + xmax)/2\n  ) %&gt;%\n  ungroup()\n\n# Baseline (b0) per panel\nbaseline_df &lt;- viz_sse %&gt;%\n  group_by(Model) %&gt;%\n  summarise(b0 = dplyr::first(b0), .groups = \"drop\")\n\n# compute global limits across all panels\nx_max &lt;- max(viz_sse$xmax)\ny_min &lt;- min(viz_sse$ymin)\ny_max &lt;- max(viz_sse$ymax)\npad_x &lt;- 0.10 * x_max\npad_y &lt;- 0.10 * (y_max - y_min)\n\n# panel SSE labels \nsse_annot &lt;- viz_sse %&gt;%\n  dplyr::group_by(Model) %&gt;%\n  dplyr::summarise(SSE = sum(side^2), .groups = \"drop\") %&gt;%\n  dplyr::mutate(\n    label = sprintf(\"SSE = %.2f\", SSE),\n    x_pos = x_max,                 # place at common right edge\n    y_pos = y_max + pad_y * 0.5    # a bit above the tallest square\n  )\n\n# Plot: Now with squares!\nSSEplot &lt;- ggplot(viz_sse) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n            fill = \"grey60\", color = \"grey35\") +\n  geom_hline(data = baseline_df, aes(yintercept = b0),\n             linetype = \"dashed\", color = \"grey30\") +\n  geom_point(aes(x = (xmin + xmax)/2, y = Total), size = 2.4, color = \"black\") +\n  geom_text(data = sse_annot, aes(x = x_pos, y = y_pos, label = label),\n            inherit.aes = FALSE, hjust = 1, vjust = 1, size = 3.5) +\n  facet_wrap(~ Model, ncol = 1) +\n  coord_fixed(ratio = 1,\n              xlim = c(0, x_max + pad_x),\n              ylim = c(y_min - pad_y, y_max + pad_y)) +\n  scale_x_continuous(breaks=NULL, labels=NULL) +\n  labs(\n    title = \"Sum of Squared Errors (SSE) \\n(they're literal squared line lengths)\\n\",\n    subtitle = \"Each side of each square = |DATA-MODEL|; \\n\n    The summed area = our measure of ERROR which is now |ERROR|^2 \\n\n    Dashed line is b0 (the model estimate)\",\n    x = \"\",\n    y = \"Total ($)\",\n  ) +\n  theme_minimal(base_size = 12)\n\nSSEplot"
  },
  {
    "objectID": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#review-all-plots",
    "href": "teaching/marketing-research/assignments/ch2-coffee-errors-exercise.html#review-all-plots",
    "title": "Coffee Shop Micro-Demo for Chapter 2",
    "section": "Review All Plots",
    "text": "Review All Plots\nLet’s take a look at all the plots we just made:\n\nCEplot\n\n\nCElogplot\n\n\nSoEplot\n\n\nSoElogplot\n\n\nSAEplot\n\n\nSAElogplot\n\n\nSSEplot"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html",
    "title": "Stout Festival Exercise",
    "section": "",
    "text": "We’re going to continue working with our brewery client brief but there’s been an exciting development - NEW DATA!\nActually two new sets of data but we’re going to focus on one for now. One of the brewery managers traveled to a beer festival and collected additional data there. That data is available in the class folder for this week for you to download. Your job is to make a first pass at analyzing the data to start to figure out what’s going on here.\n\n\n\n\n\n\nImportantComplete the Coffee Shop Exercise First\n\n\n\nI recommend you complete the readings, lecture, and associated coffee shop exercise before attempting to complete this exercise. I do a lot more walking you through the logic and providing every step of the code in the coffee shop exercise and I assume you need less of that by the time you start working on this one."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#introduction",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#introduction",
    "title": "Stout Festival Exercise",
    "section": "",
    "text": "We’re going to continue working with our brewery client brief but there’s been an exciting development - NEW DATA!\nActually two new sets of data but we’re going to focus on one for now. One of the brewery managers traveled to a beer festival and collected additional data there. That data is available in the class folder for this week for you to download. Your job is to make a first pass at analyzing the data to start to figure out what’s going on here.\n\n\n\n\n\n\nImportantComplete the Coffee Shop Exercise First\n\n\n\nI recommend you complete the readings, lecture, and associated coffee shop exercise before attempting to complete this exercise. I do a lot more walking you through the logic and providing every step of the code in the coffee shop exercise and I assume you need less of that by the time you start working on this one."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#load-the-data",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#load-the-data",
    "title": "Stout Festival Exercise",
    "section": "Load the Data",
    "text": "Load the Data\nLoad the dataset (make sure the .csv lives where R can see it).\nTip: getwd() shows where R is looking. setwd(\"path/…\") changes it.\n\nfestival &lt;- read.csv(\"Datasets/stout_festival.csv\", stringsAsFactors = FALSE)\n\n# Quick peeks (these should *not* error)\nhead(festival)\nstr(festival)\nsummary(festival$WTP)   # &lt;- the star of the show for most of this exercise"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-1-calculate-measures-of-central-tendency",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-1-calculate-measures-of-central-tendency",
    "title": "Stout Festival Exercise",
    "section": "Part 1: Calculate Measures of Central Tendency",
    "text": "Part 1: Calculate Measures of Central Tendency\nFirst, you need to calculate 2 measures of central tendency for WTP.\n\nCalculate the median\n\nfestival_median &lt;- median(festival$WTP)\n\n\n\nCalculate the mean\n(You’re going to have to do this one on your own. You can do it. Promise.)\n\n# YOUR CODE HERE\n\n\n\nAdd b₀ predictions to the dataset\nNow add those values as b₀ predictions to the dataset. In other words, create new columns in the dataset for each model prediction that list that prediction for each observation in the dataset.\n\nCreate b0_median variable in the festival dataset\n(You’re going to have to do this one on your own. I believe in you.)\n\n# YOUR CODE HERE\n\n\n\nCreate b0_mean variable in teh festival dataset\n(I’m going to take a stab at this one but I can’t know for sure because it will depend on what you did when you calculated the mean above. Trying my best but you will need to check and possibly troubleshoot my work)\n\nfestival$b0_mean &lt;- festival_mean"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-2-calculate-error-measures",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#part-2-calculate-error-measures",
    "title": "Stout Festival Exercise",
    "section": "Part 2: Calculate Error Measures",
    "text": "Part 2: Calculate Error Measures\nSecond, you’re going to calculate three different types of error:\n\nSum of Errors\nSum of Absolute Errors\nSum of Squared Error\n\nYou’ll need to calculate each type of error for b0_median and b0_mean.\n\nError calculations for Median\nI’ll help you get started with median.\n\nIndividual eᵢ (DATA - MODEL)\nWe’ll begin by calculating our individual eᵢ - the straight DATA-MODEL version:\n\nfestival$ei_median &lt;- festival$WTP - festival$b0_median\n\n\n\nSum of Errors\nThen you’re going to need to calculate the Sum of Errors…pretty simple:\n\nSoE_median &lt;- sum(festival$ei_median)\nSoE_median\n\n\n\nSum of Absolute Errors\nNow you’re going to need to calculate the sum of absolute errors (you’re on your own here):\n\n# YOUR CODE HERE\n\n\n\nSum of Squared Errors\nAnd finally, let’s calculate the sum of squared error:\n\nSSE_median &lt;- sum((festival$ei_median)^2)\nSSE_median\n\n\n\n\n\nError calculations for Mean\nOK, now everything we just did for median, you’re going to need to do again for mean. This time, I’ll just watch. You got this.\n\nIndividual eᵢ (DATA - MODEL)\nBegin by calculating the individual eᵢ - the straight DATA-MODEL for b0_mean:\n\n# YOUR CODE HERE\n\n\n\nSum of Errors\nNow calculate the Sum of Errors:\n\n# YOUR CODE HERE\n\n\n\nSum of Absolute Errors\n…and the Sum of Absolute Errors:\n\n# YOUR CODE HERE\n\n\n\nSum of Squared Errors\n… and finally the Sum of Squared Errors:\n\n# YOUR CODE HERE"
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#summary-table",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#summary-table",
    "title": "Stout Festival Exercise",
    "section": "Summary Table",
    "text": "Summary Table\nOK, we’re getting to the end.\nYou don’t have to, but something like this might be handy (though whether or not it will work as is will depend on the variable names you specified above):\n\nfestivalresults &lt;- data.frame((\n  Model = c(\"Mean\", \"Median\")),\n  SoE    = c(SoE_mean, SoE_median),\n  SAE   = c(SAE_mean, SAE_median),\n  SSE   = c(SSE_mean, SSE_median)\n)\n\nfestivalresults\n\nAnd if you got the above to work but want it without the scientific notation, you should be able to make light work of the following:\n\nfestival_results_pretty &lt;- festivalresults\nfestival_results_pretty$SoE &lt;- format(festival_results_pretty$SoE, scientific = FALSE, trim = TRUE)\nfestival_results_pretty\n\n\nNice job! You made it to the end! I have a little (OPTIONAL) treat for you that has been waiting here."
  },
  {
    "objectID": "teaching/marketing-research/assignments/stout-festival-exercise.html#optional-word-cloud",
    "href": "teaching/marketing-research/assignments/stout-festival-exercise.html#optional-word-cloud",
    "title": "Stout Festival Exercise",
    "section": "OPTIONAL: Word Cloud",
    "text": "OPTIONAL: Word Cloud\nWord cloud from the open-text “Notes” responses in our festival dataset.\n\n# If you haven't installed these packages before, uncomment them, run the\n# install lines ONCE (then # them out).\n\n# install.packages(\"tidytext\")\n# install.packages(\"wordcloud\")     \n# install.packages(\"wordcloud2\")    \n# install.packages(\"stringr\")\n# install.packages(\"dplyr\")\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(wordcloud)    \nlibrary(wordcloud2)   \n\n# 1) Grab the text column \ntxt &lt;- festival$OpenText\n\n# 2) Replace NAs with blanks; build a small tibble\ntexts &lt;- tibble(text = ifelse(is.na(txt), \"\", txt))\n\n# 3) Tokenize into words and remove common stop words, general cleanup\ntokens &lt;- texts %&gt;%\n  mutate(text = tolower(text)) %&gt;%\n  mutate(text = str_replace_all(text, \"[^a-z\\\\s]\", \" \")) %&gt;%  # keep letters + space\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"^[a-z]+$\"), nchar(word) &gt; 2) %&gt;%\n  anti_join(stop_words, by = \"word\")\n\n# 4) Count word frequencies\nword_counts &lt;- tokens %&gt;% count(word, sort = TRUE)\n\n# Quick peek at top 20 terms:\nhead(word_counts, 20)\n\n# OK, so this was a new wordcloud function to me and actually generates HTML that\n# is interactive which is wild and crazy and cool. If you hover over the different\n# words, it gives you the counts which is kind of awesome if you ask me but I \n# don't get out much.\nwordcloud2(word_counts, size = 1, minSize = 2, rotateRatio = 0.1)"
  }
]